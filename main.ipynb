{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/07/15 10:55:56] ppocr INFO: Architecture : \n",
      "[2024/07/15 10:55:56] ppocr INFO:     Backbone : \n",
      "[2024/07/15 10:55:56] ppocr INFO:         last_conv_stride : [1, 2]\n",
      "[2024/07/15 10:55:56] ppocr INFO:         last_pool_kernel_size : [2, 2]\n",
      "[2024/07/15 10:55:56] ppocr INFO:         last_pool_type : avg\n",
      "[2024/07/15 10:55:56] ppocr INFO:         name : MobileNetV1Enhance\n",
      "[2024/07/15 10:55:56] ppocr INFO:         scale : 0.5\n",
      "[2024/07/15 10:55:56] ppocr INFO:     Head : \n",
      "[2024/07/15 10:55:56] ppocr INFO:         head_list : \n",
      "[2024/07/15 10:55:56] ppocr INFO:             CTCHead : \n",
      "[2024/07/15 10:55:56] ppocr INFO:                 Head : \n",
      "[2024/07/15 10:55:56] ppocr INFO:                     fc_decay : 1e-05\n",
      "[2024/07/15 10:55:56] ppocr INFO:                 Neck : \n",
      "[2024/07/15 10:55:56] ppocr INFO:                     depth : 2\n",
      "[2024/07/15 10:55:56] ppocr INFO:                     dims : 64\n",
      "[2024/07/15 10:55:56] ppocr INFO:                     hidden_dims : 120\n",
      "[2024/07/15 10:55:56] ppocr INFO:                     name : svtr\n",
      "[2024/07/15 10:55:56] ppocr INFO:                     use_guide : True\n",
      "[2024/07/15 10:55:56] ppocr INFO:             SARHead : \n",
      "[2024/07/15 10:55:56] ppocr INFO:                 enc_dim : 512\n",
      "[2024/07/15 10:55:56] ppocr INFO:                 max_text_length : 25\n",
      "[2024/07/15 10:55:56] ppocr INFO:         name : MultiHead\n",
      "[2024/07/15 10:55:56] ppocr INFO:     Transform : None\n",
      "[2024/07/15 10:55:56] ppocr INFO:     algorithm : SVTR_LCNet\n",
      "[2024/07/15 10:55:56] ppocr INFO:     model_type : rec\n",
      "[2024/07/15 10:55:56] ppocr INFO: Eval : \n",
      "[2024/07/15 10:55:56] ppocr INFO:     dataset : \n",
      "[2024/07/15 10:55:56] ppocr INFO:         data_dir : /home/nguyen/workplace/signboard_ocr/PaddleOCR/ocr_datasets/test\n",
      "[2024/07/15 10:55:56] ppocr INFO:         label_file_list : ['/home/nguyen/workplace/signboard_ocr/PaddleOCR/ocr_datasets/test_labels.txt']\n",
      "[2024/07/15 10:55:56] ppocr INFO:         name : SimpleDataSet\n",
      "[2024/07/15 10:55:56] ppocr INFO:         transforms : \n",
      "[2024/07/15 10:55:56] ppocr INFO:             DecodeImage : \n",
      "[2024/07/15 10:55:56] ppocr INFO:                 channel_first : False\n",
      "[2024/07/15 10:55:56] ppocr INFO:                 img_mode : BGR\n",
      "[2024/07/15 10:55:56] ppocr INFO:             MultiLabelEncode : None\n",
      "[2024/07/15 10:55:56] ppocr INFO:             RecResizeImg : \n",
      "[2024/07/15 10:55:56] ppocr INFO:                 image_shape : [3, 48, 320]\n",
      "[2024/07/15 10:55:56] ppocr INFO:             KeepKeys : \n",
      "[2024/07/15 10:55:56] ppocr INFO:                 keep_keys : ['image', 'label_ctc', 'label_sar', 'length', 'valid_ratio']\n",
      "[2024/07/15 10:55:56] ppocr INFO:     loader : \n",
      "[2024/07/15 10:55:56] ppocr INFO:         batch_size_per_card : 256\n",
      "[2024/07/15 10:55:56] ppocr INFO:         drop_last : False\n",
      "[2024/07/15 10:55:56] ppocr INFO:         num_workers : 4\n",
      "[2024/07/15 10:55:56] ppocr INFO:         shuffle : False\n",
      "[2024/07/15 10:55:56] ppocr INFO: Global : \n",
      "[2024/07/15 10:55:56] ppocr INFO:     cal_metric_during_train : True\n",
      "[2024/07/15 10:55:56] ppocr INFO:     character_dict_path : /home/nguyen/workplace/signboard_ocr/PaddleOCR/ppocr/utils/dict/vietnamese_dict.txt\n",
      "[2024/07/15 10:55:56] ppocr INFO:     checkpoints : PaddleOCR/output/v3_latin_mobile/best_model/model\n",
      "[2024/07/15 10:55:56] ppocr INFO:     debug : False\n",
      "[2024/07/15 10:55:56] ppocr INFO:     distributed : False\n",
      "[2024/07/15 10:55:56] ppocr INFO:     epoch_num : 500\n",
      "[2024/07/15 10:55:56] ppocr INFO:     eval_batch_step : [0, 575]\n",
      "[2024/07/15 10:55:56] ppocr INFO:     infer_img : PaddleOCR/ocr_datasets/test/20230119__SFC__Bao_cao_tai_chinh_quy_1_nien_do_20222023_Signed_page_2_crop_60.jpg\n",
      "[2024/07/15 10:55:56] ppocr INFO:     infer_mode : False\n",
      "[2024/07/15 10:55:56] ppocr INFO:     log_smooth_window : 20\n",
      "[2024/07/15 10:55:56] ppocr INFO:     max_text_length : 25\n",
      "[2024/07/15 10:55:56] ppocr INFO:     pretrained_model : None\n",
      "[2024/07/15 10:55:56] ppocr INFO:     print_batch_step : 115\n",
      "[2024/07/15 10:55:56] ppocr INFO:     save_epoch_step : 2\n",
      "[2024/07/15 10:55:56] ppocr INFO:     save_inference_dir : None\n",
      "[2024/07/15 10:55:56] ppocr INFO:     save_model_dir : ./output/v3_latin_mobile\n",
      "[2024/07/15 10:55:56] ppocr INFO:     save_res_path : /home/nguyen/workplace/signboard_ocr/PaddleOCR/output/rec/predicts_ppocrv3_latin.txt\n",
      "[2024/07/15 10:55:56] ppocr INFO:     use_gpu : False\n",
      "[2024/07/15 10:55:56] ppocr INFO:     use_space_char : True\n",
      "[2024/07/15 10:55:56] ppocr INFO:     use_visualdl : False\n",
      "[2024/07/15 10:55:56] ppocr INFO: Loss : \n",
      "[2024/07/15 10:55:56] ppocr INFO:     loss_config_list : \n",
      "[2024/07/15 10:55:56] ppocr INFO:         CTCLoss : None\n",
      "[2024/07/15 10:55:56] ppocr INFO:         SARLoss : None\n",
      "[2024/07/15 10:55:56] ppocr INFO:     name : MultiLoss\n",
      "[2024/07/15 10:55:56] ppocr INFO: Metric : \n",
      "[2024/07/15 10:55:56] ppocr INFO:     ignore_space : False\n",
      "[2024/07/15 10:55:56] ppocr INFO:     main_indicator : acc\n",
      "[2024/07/15 10:55:56] ppocr INFO:     name : RecMetric\n",
      "[2024/07/15 10:55:56] ppocr INFO: Optimizer : \n",
      "[2024/07/15 10:55:56] ppocr INFO:     beta1 : 0.9\n",
      "[2024/07/15 10:55:56] ppocr INFO:     beta2 : 0.999\n",
      "[2024/07/15 10:55:56] ppocr INFO:     lr : \n",
      "[2024/07/15 10:55:56] ppocr INFO:         learning_rate : 0.0005\n",
      "[2024/07/15 10:55:56] ppocr INFO:         name : Cosine\n",
      "[2024/07/15 10:55:56] ppocr INFO:         warmup_epoch : 5\n",
      "[2024/07/15 10:55:56] ppocr INFO:     name : Adam\n",
      "[2024/07/15 10:55:56] ppocr INFO:     regularizer : \n",
      "[2024/07/15 10:55:56] ppocr INFO:         factor : 3e-05\n",
      "[2024/07/15 10:55:56] ppocr INFO:         name : L2\n",
      "[2024/07/15 10:55:56] ppocr INFO: PostProcess : \n",
      "[2024/07/15 10:55:56] ppocr INFO:     name : CTCLabelDecode\n",
      "[2024/07/15 10:55:56] ppocr INFO: Train : \n",
      "[2024/07/15 10:55:56] ppocr INFO:     dataset : \n",
      "[2024/07/15 10:55:56] ppocr INFO:         data_dir : /home/nguyen/workplace/signboard_ocr/PaddleOCR/ocr_datasets/train\n",
      "[2024/07/15 10:55:56] ppocr INFO:         ext_op_transform_idx : 1\n",
      "[2024/07/15 10:55:56] ppocr INFO:         label_file_list : ['/home/nguyen/workplace/signboard_ocr/PaddleOCR/ocr_datasets/train_labels.txt']\n",
      "[2024/07/15 10:55:56] ppocr INFO:         name : SimpleDataSet\n",
      "[2024/07/15 10:55:56] ppocr INFO:         transforms : \n",
      "[2024/07/15 10:55:56] ppocr INFO:             DecodeImage : \n",
      "[2024/07/15 10:55:56] ppocr INFO:                 channel_first : False\n",
      "[2024/07/15 10:55:56] ppocr INFO:                 img_mode : BGR\n",
      "[2024/07/15 10:55:56] ppocr INFO:             RecConAug : \n",
      "[2024/07/15 10:55:56] ppocr INFO:                 ext_data_num : 2\n",
      "[2024/07/15 10:55:56] ppocr INFO:                 image_shape : [48, 320, 3]\n",
      "[2024/07/15 10:55:56] ppocr INFO:                 prob : 0.5\n",
      "[2024/07/15 10:55:56] ppocr INFO:             RecAug : None\n",
      "[2024/07/15 10:55:56] ppocr INFO:             MultiLabelEncode : None\n",
      "[2024/07/15 10:55:56] ppocr INFO:             RecResizeImg : \n",
      "[2024/07/15 10:55:56] ppocr INFO:                 image_shape : [3, 48, 320]\n",
      "[2024/07/15 10:55:56] ppocr INFO:             KeepKeys : \n",
      "[2024/07/15 10:55:56] ppocr INFO:                 keep_keys : ['image', 'label_ctc', 'label_sar', 'length', 'valid_ratio']\n",
      "[2024/07/15 10:55:56] ppocr INFO:     loader : \n",
      "[2024/07/15 10:55:56] ppocr INFO:         batch_size_per_card : 128\n",
      "[2024/07/15 10:55:56] ppocr INFO:         drop_last : False\n",
      "[2024/07/15 10:55:56] ppocr INFO:         num_workers : 4\n",
      "[2024/07/15 10:55:56] ppocr INFO:         shuffle : True\n",
      "[2024/07/15 10:55:56] ppocr INFO: profiler_options : None\n",
      "[2024/07/15 10:55:56] ppocr INFO: train with paddle 2.6.1 and device Place(cpu)\n",
      "[2024/07/15 10:55:57] ppocr INFO: resume from PaddleOCR/output/v3_latin_mobile/best_model/model\n",
      "[2024/07/15 10:55:57] ppocr INFO: infer_img: PaddleOCR/ocr_datasets/test/20230119__SFC__Bao_cao_tai_chinh_quy_1_nien_do_20222023_Signed_page_2_crop_60.jpg\n",
      "[2024/07/15 10:55:57] ppocr INFO: \t result: 1.Chi phí trả trước dài hạn\t0.9987502098083496\n",
      "[2024/07/15 10:55:57] ppocr INFO: success!\n"
     ]
    }
   ],
   "source": [
    "# %cd PaddleOCR/\n",
    "!python PaddleOCR/infer_rec.py -c PaddleOCR/configs/rec/PP-OCRv3/multi_language/latin_PP-OCRv3_rec.yml \\\n",
    "    -o Global.checkpoints=PaddleOCR/output/v3_latin_mobile/best_model/model \\\n",
    "    Global.infer_img=PaddleOCR/ocr_datasets/test/20230119__SFC__Bao_cao_tai_chinh_quy_1_nien_do_20222023_Signed_page_2_crop_60.jpg\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/07/29 09:15:30] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/nguyen/.paddleocr/whl/det/ch/ch_PP-OCRv4_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/nguyen/workplace/signboard_ocr/inference/latin_PP-OCRv3_rec', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='paddleocr_data/vietnamese_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/home/nguyen/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='ch', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "\n",
    "paddle_ocr = PaddleOCR(\n",
    "    use_angle_cls=True,\n",
    "    # lang=\"vi\",\n",
    "    rec_model_dir=\"/home/nguyen/workplace/signboard_ocr/inference/latin_PP-OCRv3_rec\",\n",
    "    rec_char_dict_path=\"paddleocr_data/vietnamese_dict.txt\",\n",
    ")  # need to run only once to download and load model into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17105 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17105/17105 [21:42<00:00, 13.13it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = \"generated_text_recognition_dataset/undistorted_images\"\n",
    "image_names = np.array(sorted(os.listdir(path)))\n",
    "# n_images = len(image_names)\n",
    "# batch_size = 1\n",
    "# n_batches = int(np.ceil(n_images / batch_size))\n",
    "\n",
    "\n",
    "def load_images(batch_names):\n",
    "    images = []\n",
    "    for name in batch_names:\n",
    "        img = np.array(Image.open(os.path.join(path, name)))\n",
    "        images.append(img)\n",
    "    if len(batch_names) == 1:\n",
    "        return img\n",
    "    return images\n",
    "\n",
    "\n",
    "count = 0\n",
    "for name in tqdm(image_names):\n",
    "    # batch_names = image_names[batch_size * i : min(batch_size * (i + 1), n_images)]\n",
    "    # images = load_images(batch_names)\n",
    "    image = np.array(Image.open(os.path.join(path, name)))\n",
    "    result = paddle_ocr.ocr(image, cls=True, rec=False)[0]\n",
    "    if result is None:\n",
    "        with open(\"cant_detect.txt\", \"a\") as f:\n",
    "            f.write(f\"{name}\\n\")\n",
    "    # else:\n",
    "    #     bbox = result[0]\n",
    "    # if count == 10:\n",
    "    #     break\n",
    "    # count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "with open(\"generated_text_recognition_dataset/undetectable_labels.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    shutil.move(\n",
    "        os.path.join(\n",
    "            \"generated_text_recognition_dataset/undetectable_images\", line[:-1]\n",
    "        ),\n",
    "        \"generated_text_recognition_dataset/undistorted_images\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated_text_recognition_dataset/undetectable_labels.txt', 'r') as f:\n",
    "    undetectable = f.readlines()\n",
    "undetectable = [line[:-1] for line in undetectable]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nguyen/workplace/signboard_ocr/PaddleOCR\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "C++ Traceback (most recent call last):\n",
      "--------------------------------------\n",
      "0   paddle_infer::Predictor::Predictor(paddle::AnalysisConfig const&)\n",
      "1   std::unique_ptr<paddle::PaddlePredictor, std::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig, (paddle::PaddleEngineKind)2>(paddle::AnalysisConfig const&)\n",
      "2   paddle::AnalysisPredictor::Init(std::shared_ptr<paddle::framework::Scope> const&, std::shared_ptr<paddle::framework::ProgramDesc> const&)\n",
      "3   paddle::AnalysisPredictor::PrepareProgram(std::shared_ptr<paddle::framework::ProgramDesc> const&)\n",
      "4   paddle::AnalysisPredictor::OptimizeInferenceProgram()\n",
      "5   paddle::inference::analysis::Analyzer::RunAnalysis(paddle::inference::analysis::Argument*)\n",
      "6   paddle::inference::analysis::IrAnalysisPass::RunImpl(paddle::inference::analysis::Argument*)\n",
      "7   paddle::inference::analysis::IRPassManager::Apply(std::unique_ptr<paddle::framework::ir::Graph, std::default_delete<paddle::framework::ir::Graph> >)\n",
      "8   paddle::framework::ir::Pass::Apply(paddle::framework::ir::Graph*) const\n",
      "9   paddle::framework::ir::SelfAttentionFusePass::ApplyImpl(paddle::framework::ir::Graph*) const\n",
      "10  paddle::framework::ir::GraphPatternDetector::operator()(paddle::framework::ir::Graph*, std::function<void (std::map<paddle::framework::ir::PDNode*, paddle::framework::ir::Node*, paddle::framework::ir::GraphPatternDetector::PDNodeCompare, std::allocator<std::pair<paddle::framework::ir::PDNode* const, paddle::framework::ir::Node*> > > const&, paddle::framework::ir::Graph*)>)\n",
      "\n",
      "----------------------\n",
      "Error Message Summary:\n",
      "----------------------\n",
      "FatalError: `Illegal instruction` is detected by the operating system.\n",
      "  [TimeInfo: *** Aborted at 1721028684 (unix time) try \"date -d @1721028684\" if you are using GNU date ***]\n",
      "  [SignalInfo: *** SIGILL (@0x7f5ded21e86a) received by PID 15778 (TID 0x7f5e03487000) from PID 18446744073393006698 ***]\n",
      "\n",
      "/home/nguyen/workplace/signboard_ocr\n"
     ]
    }
   ],
   "source": [
    "%cd PaddleOCR/\n",
    "!python3 tools/infer/predict_rec.py \\\n",
    "    --image_dir=\"ocr_datasets/test/20230119__SFC__Bao_cao_tai_chinh_quy_1_nien_do_20222023_Signed_page_2_crop_60.jpg\" \\\n",
    "    --rec_model_dir=\"inference/latin_PP-OCRv3_rec\" \\\n",
    "    --rec_char_dict_path=\"ppocr/utils/dict/vietnamese_dict.txt\" \\\n",
    "    --vis_font_path=\"doc/fonts/korean.ttf\" \\\n",
    "    --use_gpu=False\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image orientation + layout analysis + table recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/07/17 08:31:44] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/nguyen/.paddleocr/whl/det/ch/ch_PP-OCRv4_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/nguyen/.paddleocr/whl/rec/ch/ch_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/home/nguyen/workplace/signboard_ocr/PaddleOCR/ppocr/utils/ppocr_keys_v1.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir=None, cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir='/home/nguyen/.paddleocr/whl/table/ch_ppstructure_mobile_v2.0_SLANet_infer', merge_no_span_structure=True, table_char_dict_path='/home/nguyen/workplace/signboard_ocr/PaddleOCR/ppocr/utils/dict/table_structure_dict_ch.txt', layout_model_dir='/home/nguyen/.paddleocr/whl/layout/picodet_lcnet_x1_0_fgd_layout_cdla_infer', layout_dict_path='/home/nguyen/workplace/signboard_ocr/PaddleOCR/ppocr/utils/dict/layout_dict/layout_cdla_dict.txt', layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=True, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='ch', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "[2024/07/17 08:31:44] ppcls INFO: download https://paddleclas.bj.bcebos.com/models/PULC/inference/text_image_orientation_infer.tar to /home/nguyen/.paddleclas/inference_model/PULC/text_image_orientation/text_image_orientation_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7.40M/7.40M [00:12<00:00, 583kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/07/17 08:31:58] ppcls WARNING: The current running environment does not support the use of GPU. CPU has been used instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/07/17 08:32:02] ppocr DEBUG: dt_boxes num : 230, elapsed : 1.0707526206970215\n",
      "[2024/07/17 08:33:25] ppocr DEBUG: rec_res num  : 230, elapsed : 82.87537503242493\n",
      "[2024/07/17 08:33:27] ppocr DEBUG: dt_boxes num : 108, elapse : 0.38725900650024414\n",
      "[2024/07/17 08:34:07] ppocr DEBUG: rec_res num  : 108, elapse : 39.62550210952759\n",
      "[2024/07/17 08:34:09] ppocr DEBUG: dt_boxes num : 77, elapse : 0.36330699920654297\n",
      "[2024/07/17 08:34:36] ppocr DEBUG: rec_res num  : 77, elapse : 26.43921661376953\n",
      "{'type': 'text', 'bbox': [460, 0, 856, 117], 'res': [], 'img_idx': 0, 'score': 0.8404665589332581}\n",
      "{'type': 'text', 'bbox': [30, 0, 425, 102], 'res': [], 'img_idx': 0, 'score': 0.7354930639266968}\n",
      "{'type': 'title', 'bbox': [308, 128, 424, 143], 'res': [], 'img_idx': 0, 'score': 0.9530608057975769}\n",
      "{'type': 'figure', 'bbox': [21, 547, 860, 847], 'res': [{'text': 'M (P)', 'confidence': 0.6822103261947632, 'text_region': [[94.0, 540.0], [173.0, 540.0], [173.0, 554.0], [94.0, 554.0]]}, {'text': '0M ()', 'confidence': 0.6392375230789185, 'text_region': [[299.0, 538.0], [380.0, 538.0], [380.0, 555.0], [299.0, 555.0]]}, {'text': '()', 'confidence': 0.6114261150360107, 'text_region': [[507.0, 539.0], [585.0, 539.0], [585.0, 553.0], [507.0, 553.0]]}, {'text': ' ())', 'confidence': 0.6127791404724121, 'text_region': [[713.0, 538.0], [792.0, 538.0], [792.0, 555.0], [713.0, 555.0]]}, {'text': '10.', 'confidence': 0.8478307723999023, 'text_region': [[521.0, 609.0], [547.0, 606.0], [548.0, 618.0], [522.0, 621.0]]}, {'text': 'Ceien mmmuae ap elia?', 'confidence': 0.5531696677207947, 'text_region': [[270.0, 633.0], [399.0, 611.0], [402.0, 628.0], [273.0, 650.0]]}, {'text': 'OL!', 'confidence': 0.6489985585212708, 'text_region': [[106.0, 639.0], [144.0, 648.0], [140.0, 663.0], [102.0, 653.0]]}, {'text': 'COHS', 'confidence': 0.6930473446846008, 'text_region': [[70.0, 712.0], [187.0, 712.0], [187.0, 752.0], [70.0, 752.0]]}, {'text': 'N', 'confidence': 0.673729658126831, 'text_region': [[319.0, 714.0], [358.0, 714.0], [358.0, 726.0], [319.0, 726.0]]}, {'text': '32', 'confidence': 0.7054972052574158, 'text_region': [[734.0, 739.0], [757.0, 739.0], [757.0, 751.0], [734.0, 751.0]]}, {'text': 'FOH.LIS', 'confidence': 0.6280502080917358, 'text_region': [[303.0, 785.0], [374.0, 785.0], [374.0, 796.0], [303.0, 796.0]]}], 'img_idx': 0, 'score': 0.9715456962585449}\n",
      "{'type': 'figure_caption', 'bbox': [33, 489, 855, 553], 'res': [{'text': 'M (P)', 'confidence': 0.6822103261947632, 'text_region': [[94.0, 540.0], [173.0, 540.0], [173.0, 554.0], [94.0, 554.0]]}, {'text': '0M ()', 'confidence': 0.6392375230789185, 'text_region': [[299.0, 538.0], [380.0, 538.0], [380.0, 555.0], [299.0, 555.0]]}, {'text': '()', 'confidence': 0.6114261150360107, 'text_region': [[507.0, 539.0], [585.0, 539.0], [585.0, 553.0], [507.0, 553.0]]}, {'text': ' ())', 'confidence': 0.6127791404724121, 'text_region': [[713.0, 538.0], [792.0, 538.0], [792.0, 555.0], [713.0, 555.0]]}], 'img_idx': 0, 'score': 0.5233359932899475}\n",
      "{'type': 'table', 'bbox': [460, 133, 849, 489], 'res': {'cell_bbox': [[1.2592211961746216, 4.053653717041016, 49.962791442871094, 4.1710333824157715, 49.85721969604492, 34.0887565612793, 1.2180579900741577, 33.80595397949219], [42.29520034790039, 4.785712718963623, 90.10179901123047, 4.76415491104126, 91.7356948852539, 29.654529571533203, 42.63793182373047, 30.05779457092285], [92.60689544677734, 3.9019038677215576, 144.70941162109375, 3.9456167221069336, 146.22171020507812, 29.6195125579834, 92.84158325195312, 30.14270782470703], [139.21444702148438, 3.240405797958374, 199.61599731445312, 3.292264223098755, 201.57742309570312, 30.351655960083008, 140.50949096679688, 30.977792739868164], [193.33926391601562, 2.394209623336792, 253.3385772705078, 2.4825708866119385, 253.15557861328125, 40.973533630371094, 192.7772216796875, 41.65976333618164], [257.2176818847656, 2.298844814300537, 379.8241271972656, 2.4063668251037598, 379.6540222167969, 50.76458740234375, 256.8560791015625, 51.171085357666016], [2.0394275188446045, 33.07381820678711, 51.133033752441406, 32.82038116455078, 51.2781867980957, 61.271888732910156, 1.9982542991638184, 61.541175842285156], [43.03388595581055, 33.1199836730957, 90.13289642333984, 32.75864028930664, 91.02730560302734, 56.78913116455078, 43.250545501708984, 57.239410400390625], [94.41384887695312, 32.88405227661133, 145.2935791015625, 32.73659896850586, 145.42710876464844, 56.82417297363281, 94.01335906982422, 57.12733840942383], [138.06338500976562, 32.6615104675293, 195.06724548339844, 32.40514373779297, 195.68539428710938, 56.4609260559082, 138.20797729492188, 57.04862594604492], [197.84388732910156, 35.61860275268555, 248.70213317871094, 35.9279899597168, 247.7650909423828, 67.43819427490234, 196.79428100585938, 67.17313385009766], [257.0162353515625, 50.3392219543457, 383.665283203125, 52.08212661743164, 383.5406799316406, 123.23297882080078, 256.1691589355469, 119.75308227539062], [7.585635662078857, 58.30669021606445, 50.1480827331543, 58.32700729370117, 49.37510299682617, 86.2371826171875, 7.41593599319458, 86.38512420654297], [47.98552322387695, 59.39202117919922, 96.73937225341797, 59.217567443847656, 96.7677993774414, 80.81827545166016, 47.57318115234375, 80.96379852294922], [97.67770385742188, 58.758602142333984, 148.89898681640625, 58.749916076660156, 148.9440460205078, 81.13833618164062, 97.54491424560547, 81.22563934326172], [143.90086364746094, 59.86922073364258, 201.3826446533203, 59.91246032714844, 201.42459106445312, 82.62455749511719, 143.55665588378906, 82.8549575805664], [198.9267578125, 65.29434204101562, 249.64170837402344, 66.18927764892578, 248.8644256591797, 95.30186462402344, 198.14967346191406, 94.41404724121094], [259.9951477050781, 71.73934936523438, 386.151611328125, 73.98149108886719, 386.0476989746094, 141.1497344970703, 258.1636962890625, 138.082763671875], [8.070173263549805, 77.60405731201172, 45.843238830566406, 76.98918151855469, 45.478668212890625, 100.45233154296875, 7.993414402008057, 101.37998962402344], [47.65119552612305, 77.44270324707031, 95.17955017089844, 77.07272338867188, 95.11454772949219, 98.69830322265625, 47.49363708496094, 99.24100494384766], [99.12368774414062, 78.01322937011719, 149.6744384765625, 77.7912826538086, 149.486328125, 98.26962280273438, 98.84375, 98.72774505615234], [144.93406677246094, 78.30847930908203, 201.1304473876953, 78.32703399658203, 200.81451416015625, 99.52588653564453, 144.55870056152344, 99.81443786621094], [199.34835815429688, 79.9984130859375, 249.22998046875, 80.7164306640625, 248.1958465576172, 108.8095703125, 198.61671447753906, 108.29972839355469], [257.71661376953125, 79.32818603515625, 386.8194274902344, 80.58464813232422, 386.7527160644531, 113.70059204101562, 256.6734619140625, 112.45158386230469], [6.924548149108887, 94.67450714111328, 45.723968505859375, 94.08309173583984, 45.40249252319336, 115.17345428466797, 6.868479251861572, 115.98420715332031], [47.711448669433594, 94.14068603515625, 95.08560943603516, 93.83600616455078, 95.10142517089844, 114.59961700439453, 47.703372955322266, 115.03897094726562], [99.3836669921875, 94.91001892089844, 149.5374298095703, 94.63224792480469, 149.34693908691406, 114.1837387084961, 99.20957946777344, 114.65979766845703], [144.9535369873047, 95.54470825195312, 199.33351135253906, 95.43962860107422, 199.06951904296875, 115.83853912353516, 144.66583251953125, 116.26181030273438], [198.79107666015625, 97.30486297607422, 248.88526916503906, 97.8610610961914, 248.00274658203125, 123.62619018554688, 198.2255859375, 123.33675384521484], [257.2696533203125, 95.72840118408203, 386.3328552246094, 96.79454040527344, 386.2723083496094, 124.74872589111328, 256.8189697265625, 123.88932037353516], [5.559781551361084, 112.17729187011719, 46.554405212402344, 111.684326171875, 46.2977409362793, 133.55514526367188, 5.496028900146484, 134.33993530273438], [47.61689758300781, 112.06963348388672, 95.35111999511719, 111.92825317382812, 95.26424407958984, 132.59556579589844, 47.481746673583984, 132.8818817138672], [99.35671997070312, 112.73773193359375, 148.62266540527344, 112.65699768066406, 148.3833465576172, 132.46066284179688, 99.11668395996094, 132.72137451171875], [144.89950561523438, 113.48635864257812, 198.9303436279297, 113.47946166992188, 198.58763122558594, 134.11410522460938, 144.4988250732422, 134.43304443359375], [197.8345184326172, 115.4885025024414, 248.24615478515625, 116.12137603759766, 247.44094848632812, 140.3296356201172, 197.29783630371094, 139.97695922851562], [256.4417724609375, 113.6935043334961, 385.77764892578125, 114.96711730957031, 385.71417236328125, 141.16131591796875, 256.1313171386719, 140.1895751953125], [5.7992377281188965, 130.62814331054688, 45.897953033447266, 130.25086975097656, 45.72495651245117, 151.52291870117188, 5.73899507522583, 152.18020629882812], [47.273773193359375, 130.49664306640625, 95.05531311035156, 130.43087768554688, 94.97208404541016, 150.66575622558594, 47.10564041137695, 150.8284149169922], [99.04025268554688, 131.04957580566406, 147.98692321777344, 131.04708862304688, 147.77162170410156, 150.34933471679688, 98.787353515625, 150.4551544189453], [144.65615844726562, 131.875732421875, 198.4141845703125, 131.89622497558594, 198.08518981933594, 152.09503173828125, 144.17311096191406, 152.3100128173828], [197.4891357421875, 133.68983459472656, 248.08717346191406, 134.2039337158203, 247.3877410888672, 156.48558044433594, 196.95057678222656, 156.19329833984375], [255.50103759765625, 132.7433319091797, 385.1571044921875, 134.1596221923828, 385.08782958984375, 158.59829711914062, 255.2096710205078, 157.45510864257812], [6.201096534729004, 149.3602752685547, 45.342533111572266, 149.2286376953125, 45.045169830322266, 170.6593017578125, 6.102739334106445, 171.0605926513672], [47.41236877441406, 148.84268188476562, 95.3248519897461, 148.8645477294922, 95.08087921142578, 169.10516357421875, 47.067543029785156, 169.19020080566406], [99.02719116210938, 149.20895385742188, 148.00003051757812, 149.27676391601562, 147.5803985595703, 168.40695190429688, 98.51423645019531, 168.45603942871094], [144.6698455810547, 150.1327362060547, 198.93576049804688, 150.14906311035156, 198.4253387451172, 170.11476135253906, 143.9075927734375, 170.31028747558594], [198.10853576660156, 151.64791870117188, 248.4963836669922, 152.0576629638672, 247.681396484375, 173.92398071289062, 197.31381225585938, 173.72158813476562], [255.68504333496094, 151.0294189453125, 384.67230224609375, 152.49522399902344, 384.59246826171875, 176.50912475585938, 255.286865234375, 175.30624389648438], [4.980301380157471, 168.67454528808594, 47.4528923034668, 168.906982421875, 47.06491470336914, 191.7210693359375, 4.883254051208496, 191.68569946289062], [47.704036712646484, 167.89700317382812, 96.27542877197266, 168.21542358398438, 95.85282897949219, 188.26248168945312, 47.2416877746582, 188.11219787597656], [98.96839904785156, 168.31646728515625, 148.04087829589844, 168.56040954589844, 147.41259765625, 187.63523864746094, 98.30855560302734, 187.55331420898438], [144.78977966308594, 169.2987060546875, 198.57662963867188, 169.42295837402344, 197.8319091796875, 189.2391815185547, 143.84417724609375, 189.3445587158203], [198.18795776367188, 170.56710815429688, 248.18612670898438, 170.9569091796875, 247.3079071044922, 192.20458984375, 197.30233764648438, 192.04054260253906], [254.97938537597656, 169.19711303710938, 385.5139465332031, 170.7529296875, 385.4502868652344, 194.17898559570312, 254.6555938720703, 192.92640686035156], [5.088443756103516, 188.17514038085938, 45.870784759521484, 188.419677734375, 45.47620391845703, 209.93629455566406, 5.004318714141846, 209.8332061767578], [47.65279006958008, 187.42239379882812, 95.77129364013672, 187.81976318359375, 95.28515625, 206.9912872314453, 47.211734771728516, 206.73971557617188], [98.62325286865234, 187.78201293945312, 148.18905639648438, 188.02325439453125, 147.55609130859375, 206.26962280273438, 98.02721405029297, 206.14031982421875], [144.75286865234375, 188.55642700195312, 198.023681640625, 188.7063446044922, 197.22305297851562, 207.7453155517578, 143.81907653808594, 207.77369689941406], [197.94277954101562, 189.5665283203125, 247.66368103027344, 189.92808532714844, 246.86795043945312, 209.96029663085938, 197.15115356445312, 209.78921508789062], [254.5196990966797, 188.15992736816406, 385.9932556152344, 189.81716918945312, 385.9369201660156, 212.13461303710938, 254.1315155029297, 210.797607421875], [6.256642818450928, 206.22653198242188, 44.929222106933594, 206.34811401367188, 44.4707145690918, 226.0496826171875, 6.168324947357178, 226.00921630859375], [48.018978118896484, 205.38636779785156, 95.47521209716797, 205.64871215820312, 95.03462219238281, 224.17803955078125, 47.620723724365234, 224.0037384033203], [98.80050659179688, 205.88742065429688, 148.49844360351562, 205.9180908203125, 147.9076690673828, 222.99703979492188, 98.2500991821289, 223.03807067871094], [144.769287109375, 206.58642578125, 198.25344848632812, 206.51254272460938, 197.53150939941406, 224.36888122558594, 143.91600036621094, 224.5535430908203], [198.3164520263672, 207.404541015625, 247.81019592285156, 207.52474975585938, 247.13150024414062, 226.49278259277344, 197.6468963623047, 226.5028839111328], [256.458984375, 206.2435302734375, 385.6068420410156, 207.6223602294922, 385.5487365722656, 228.5505828857422, 256.20220947265625, 227.4770965576172], [6.69804573059082, 223.56719970703125, 44.43480682373047, 223.54981994628906, 44.00398254394531, 242.72735595703125, 6.600855827331543, 242.81883239746094], [48.61127853393555, 222.82701110839844, 95.64373016357422, 222.91876220703125, 95.2874755859375, 241.31483459472656, 48.22602081298828, 241.26158142089844], [98.92437744140625, 223.52981567382812, 148.61361694335938, 223.3712158203125, 148.15484619140625, 240.52743530273438, 98.47237396240234, 240.72201538085938], [144.82911682128906, 224.3206329345703, 198.77247619628906, 224.04934692382812, 198.18653869628906, 241.87179565429688, 144.1211395263672, 242.2217254638672], [198.8903045654297, 225.2105255126953, 247.75672912597656, 225.17262268066406, 247.2262420654297, 244.14907836914062, 198.4219512939453, 244.29078674316406], [257.8575134277344, 223.4814910888672, 385.49945068359375, 224.50634765625, 385.4512023925781, 245.1262969970703, 257.901611328125, 244.38233947753906], [7.708397388458252, 240.88319396972656, 44.391902923583984, 240.811279296875, 43.97441864013672, 259.6352844238281, 7.5916829109191895, 259.741455078125], [49.419132232666016, 240.0527801513672, 95.8278579711914, 240.1400604248047, 95.56439971923828, 258.71832275390625, 49.03594207763672, 258.6319580078125], [99.15030670166016, 240.89328002929688, 148.5012969970703, 240.70822143554688, 148.16441345214844, 258.24169921875, 98.7503433227539, 258.4297790527344], [145.08766174316406, 241.4913330078125, 198.84619140625, 241.17398071289062, 198.3957977294922, 259.341552734375, 144.46441650390625, 259.6936950683594], [199.2991943359375, 242.46469116210938, 246.89505004882812, 242.35670471191406, 246.50946044921875, 261.66339111328125, 198.9495086669922, 261.8329772949219], [257.439453125, 240.45494079589844, 386.01666259765625, 241.2540283203125, 385.9867858886719, 262.2432556152344, 257.7940979003906, 261.66412353515625], [8.368246078491211, 258.829345703125, 45.3315315246582, 258.69482421875, 44.94047164916992, 277.8257141113281, 8.24941635131836, 277.91485595703125], [50.07677459716797, 257.7015380859375, 95.84977722167969, 257.7994689941406, 95.65479278564453, 276.4972839355469, 49.67189025878906, 276.37335205078125], [99.88445281982422, 258.5032653808594, 148.87379455566406, 258.33465576171875, 148.59820556640625, 276.3328857421875, 99.4753189086914, 276.4921875], [145.22952270507812, 259.0953369140625, 198.72177124023438, 258.7611999511719, 198.33358764648438, 277.38690185546875, 144.59986877441406, 277.69561767578125], [199.18075561523438, 260.0802001953125, 246.44369506835938, 259.92010498046875, 246.12269592285156, 279.7135925292969, 198.8317413330078, 279.86639404296875], [259.31573486328125, 258.5128173828125, 385.80035400390625, 259.2279968261719, 385.7686462402344, 281.8525085449219, 259.6140441894531, 281.37066650390625], [7.053747653961182, 277.4830322265625, 47.17146301269531, 277.43743896484375, 46.7781867980957, 297.6248474121094, 6.933199882507324, 297.5997619628906], [49.996829986572266, 276.1900939941406, 95.42008972167969, 276.3401794433594, 95.23717498779297, 294.813232421875, 49.56623077392578, 294.67840576171875], [100.02559661865234, 276.9038391113281, 148.73863220214844, 276.7842102050781, 148.5434112548828, 295.00994873046875, 99.61161804199219, 295.15972900390625], [145.0167236328125, 277.3389892578125, 198.8035430908203, 277.07135009765625, 198.43606567382812, 295.9683532714844, 144.42706298828125, 296.2366027832031], [198.16159057617188, 278.46533203125, 247.2723846435547, 278.3531188964844, 247.0516357421875, 298.2852478027344, 197.93055725097656, 298.41656494140625], [260.2014465332031, 277.3979797363281, 385.8855895996094, 278.0159606933594, 385.8530578613281, 301.30328369140625, 260.4292907714844, 300.9646911621094], [5.718391418457031, 295.5182800292969, 47.069725036621094, 295.58392333984375, 46.77336120605469, 316.46575927734375, 5.632713794708252, 316.3634033203125], [49.249637603759766, 294.7139587402344, 94.9756851196289, 294.8432922363281, 94.91826629638672, 313.14599609375, 48.875492095947266, 313.07904052734375], [99.6086654663086, 295.33123779296875, 148.85595703125, 295.2077331542969, 148.84439086914062, 313.0758361816406, 99.23050689697266, 313.25177001953125], [144.57693481445312, 295.68310546875, 197.7957763671875, 295.3996887207031, 197.6465301513672, 313.9676208496094, 144.12608337402344, 314.2457275390625], [197.41824340820312, 296.73822021484375, 246.79287719726562, 296.539306640625, 246.84938049316406, 316.0309143066406, 197.42578125, 316.2270202636719], [257.1811828613281, 296.1919250488281, 385.50054931640625, 296.4997863769531, 385.4707946777344, 319.6209411621094, 257.5022888183594, 319.5321350097656], [6.131834506988525, 313.09344482421875, 46.24421310424805, 313.0830078125, 46.099266052246094, 333.15081787109375, 6.061145305633545, 333.1058654785156], [49.366912841796875, 312.779296875, 95.64144897460938, 312.84881591796875, 95.89900207519531, 330.25872802734375, 49.0869255065918, 330.2413024902344], [98.71564483642578, 313.365234375, 149.8473663330078, 313.21685791015625, 150.2302703857422, 330.15533447265625, 98.589111328125, 330.31427001953125], [144.58139038085938, 313.668212890625, 197.60305786132812, 313.3630676269531, 197.81068420410156, 331.31646728515625, 144.41986083984375, 331.555419921875], [198.8922119140625, 314.5211486816406, 246.64529418945312, 314.21514892578125, 247.05145263671875, 332.8597106933594, 199.33258056640625, 333.0728759765625], [253.40695190429688, 314.2367248535156, 385.50341796875, 314.1546936035156, 385.4986877441406, 336.3634338378906, 254.43209838867188, 336.4692077636719], [4.860623359680176, 330.578857421875, 44.83226776123047, 330.503662109375, 44.54313278198242, 351.0794372558594, 4.762022018432617, 351.042724609375], [48.681358337402344, 330.86187744140625, 94.4823226928711, 330.8749084472656, 95.09730529785156, 348.04522705078125, 48.505943298339844, 348.0244140625], [97.8244400024414, 331.4648742675781, 149.40650939941406, 331.2873229980469, 150.29791259765625, 347.2442321777344, 97.9947509765625, 347.3067626953125], [144.9009552001953, 331.80499267578125, 196.6747283935547, 331.50067138671875, 197.445556640625, 347.64324951171875, 145.138427734375, 347.7593688964844], [196.74087524414062, 332.2068786621094, 249.1555633544922, 331.8326110839844, 249.737548828125, 348.521728515625, 197.10568237304688, 348.65277099609375], [251.0416717529297, 331.883056640625, 385.6057434082031, 331.4382019042969, 385.6177673339844, 349.4634094238281, 252.2918243408203, 349.6053161621094]], 'html': '<html><body><table><tbody><tr><td>1771</td><td></td><td>0058 54980958</td><td></td><td>IIN u&lt;s</td><td>so sno no</td></tr><tr><td>S171 80</td><td>18 4688</td><td>188 99.48</td><td>18 LS:08</td><td></td><td>r</td></tr><tr><td>·</td><td>98</td><td>S8</td><td>618</td><td>us</td><td></td></tr><tr><td>-</td><td></td><td>698</td><td>08</td><td>ITW</td><td> </td></tr><tr><td>S&#x27;t</td><td>68</td><td>L8</td><td>18</td><td>-</td><td></td></tr><tr><td>86</td><td>228</td><td>98</td><td>718</td><td>u&lt;s</td><td>[ NA</td></tr><tr><td></td><td>98</td><td>58</td><td>878</td><td>+ITN</td><td>[ </td></tr><tr><td>·</td><td>S58</td><td>0&#x27;98</td><td>I&#x27;18</td><td>ufs</td><td>A</td></tr><tr><td>07</td><td>58</td><td>6&#x27;98</td><td>7&#x27;08</td><td>us</td><td>[] a</td></tr><tr><td>6</td><td>7.78</td><td>88</td><td>L6L</td><td>IIN</td><td></td></tr><tr><td>&#x27;</td><td>S18</td><td>18</td><td>0.6L</td><td>ufs</td><td></td></tr><tr><td>09</td><td>t18</td><td>08</td><td>86L</td><td>uss</td><td>)</td></tr><tr><td></td><td>18</td><td>8:78</td><td>86L</td><td>uss</td><td> +i</td></tr><tr><td></td><td>1&#x27;08</td><td>1&#x27;08</td><td>7&#x27;08</td><td>-s</td><td></td></tr><tr><td></td><td>8&#x27;08</td><td>L&#x27;S8</td><td>S&#x27;94</td><td>ufs</td><td>[0NT</td></tr><tr><td>8&#x27;0</td><td>LL</td><td>L84</td><td>I&#x27;9L</td><td>ITN</td><td></td></tr><tr><td></td><td>9:SL</td><td>69</td><td>&#x27;s8</td><td>u&lt;s</td><td>l</td></tr><tr><td></td><td>山</td><td>d</td><td>Y</td><td>X</td><td>s</td></tr></tbody></table></body></html>'}, 'img_idx': 0, 'score': 0.9332485198974609}\n",
      "{'type': 'table', 'bbox': [45, 187, 412, 492], 'res': {'cell_bbox': [[12.901508331298828, 7.848019123077393, 52.85872268676758, 7.903256416320801, 52.56401824951172, 35.726497650146484, 12.526966094970703, 35.959320068359375], [69.87850952148438, 7.667753219604492, 110.05594635009766, 7.6126790046691895, 110.37566375732422, 33.50537872314453, 69.94047546386719, 33.94277572631836], [125.54615020751953, 7.197216987609863, 165.89236450195312, 7.142050266265869, 166.4116668701172, 34.83531951904297, 126.07075500488281, 35.22854995727539], [184.00631713867188, 8.149191856384277, 221.77272033691406, 8.13487720489502, 222.532470703125, 37.355281829833984, 184.60128784179688, 37.600372314453125], [231.5029296875, 6.474116802215576, 363.7131652832031, 6.587099552154541, 363.6778259277344, 39.31928253173828, 231.7703094482422, 39.128150939941406], [12.44773006439209, 44.09424591064453, 51.34782409667969, 43.966094970703125, 50.945674896240234, 66.95693969726562, 12.057060241699219, 66.78367614746094], [65.28872680664062, 43.82023239135742, 108.84465026855469, 43.60909652709961, 108.88847351074219, 63.659149169921875, 64.93766784667969, 63.69636154174805], [125.18325805664062, 43.29342269897461, 166.67233276367188, 43.220279693603516, 167.3951873779297, 63.47002410888672, 125.39089965820312, 63.52898025512695], [184.00808715820312, 44.122474670410156, 221.44488525390625, 44.063148498535156, 221.9029541015625, 63.084537506103516, 184.0935516357422, 63.23978805541992], [253.81295776367188, 46.724708557128906, 358.02117919921875, 47.35388946533203, 357.8733215332031, 68.1607437133789, 253.07582092285156, 67.5031509399414], [14.184444427490234, 65.73799896240234, 56.20328140258789, 65.15303039550781, 55.768558502197266, 85.61465454101562, 14.009966850280762, 86.25552368164062], [68.01412963867188, 64.5475845336914, 108.25912475585938, 64.1317367553711, 108.4157485961914, 82.4952621459961, 67.98074340820312, 82.90502166748047], [126.0622329711914, 64.6037368774414, 169.6817169189453, 64.44560241699219, 169.68650817871094, 82.45858764648438, 125.97407531738281, 82.72590637207031], [185.2562713623047, 65.07293701171875, 222.66436767578125, 65.02631378173828, 222.79640197753906, 81.81034088134766, 185.3233642578125, 82.07549285888672], [253.861328125, 63.89533996582031, 358.43133544921875, 64.51895141601562, 358.2789001464844, 83.66265869140625, 253.24407958984375, 83.23905181884766], [14.316886901855469, 83.98834228515625, 56.23070526123047, 83.22840881347656, 55.796180725097656, 100.96623229980469, 14.11927318572998, 101.73653411865234], [70.6474838256836, 82.98717498779297, 109.79422760009766, 82.51025390625, 109.79409790039062, 99.30972290039062, 70.49874114990234, 99.8113784790039], [125.77882385253906, 83.21842193603516, 169.94017028808594, 83.01220703125, 170.11866760253906, 99.73060607910156, 125.71646881103516, 100.08586883544922], [186.04750061035156, 83.78102111816406, 221.94796752929688, 83.60819244384766, 222.23724365234375, 98.5049819946289, 186.2746124267578, 98.9027328491211], [252.5636444091797, 81.6259536743164, 355.72509765625, 82.0847396850586, 355.5782470703125, 99.27333068847656, 252.19715881347656, 99.11093139648438], [16.379608154296875, 100.30460357666016, 52.27752685546875, 99.6704330444336, 51.8746337890625, 116.38458251953125, 16.130002975463867, 117.0943832397461], [70.78809356689453, 99.01055145263672, 109.65828704833984, 98.6222152709961, 109.50228118896484, 115.157470703125, 70.54335021972656, 115.57736206054688], [124.70552062988281, 99.46434783935547, 170.73133850097656, 99.2525405883789, 170.73475646972656, 115.72706604003906, 124.49983978271484, 116.11148834228516], [185.61508178710938, 100.43983459472656, 222.2082061767578, 100.2146987915039, 222.45327758789062, 114.65592956542969, 185.76434326171875, 115.12425231933594], [252.7020721435547, 98.211181640625, 348.6587829589844, 98.50218200683594, 348.466064453125, 115.22088623046875, 252.32086181640625, 115.23316192626953], [17.13271141052246, 119.56756591796875, 49.79526138305664, 119.1727294921875, 49.40721893310547, 136.57058715820312, 16.818269729614258, 137.00311279296875], [71.60751342773438, 116.46029663085938, 108.83665466308594, 116.13236999511719, 108.46864318847656, 132.95407104492188, 71.24408721923828, 133.27838134765625], [123.53237915039062, 117.060546875, 172.51243591308594, 116.8091812133789, 172.37046813964844, 133.20361328125, 123.22395324707031, 133.64724731445312], [186.0996856689453, 118.27769470214844, 221.74020385742188, 118.01932525634766, 221.84703063964844, 132.457763671875, 186.1137237548828, 132.979248046875], [251.89552307128906, 115.78543090820312, 349.0467529296875, 116.02857971191406, 348.8709411621094, 132.82763671875, 251.5709991455078, 132.9002227783203], [16.338722229003906, 137.3868865966797, 48.22001647949219, 137.15554809570312, 47.91045379638672, 153.45042419433594, 16.063796997070312, 153.66616821289062], [70.76536560058594, 134.49917602539062, 111.3177490234375, 134.4073944091797, 110.93562316894531, 150.3692626953125, 70.3985366821289, 150.483154296875], [122.36702728271484, 135.02200317382812, 174.17529296875, 134.94659423828125, 174.05210876464844, 150.45791625976562, 122.0632553100586, 150.7373809814453], [186.45132446289062, 136.04318237304688, 221.89874267578125, 135.9397735595703, 221.95892333984375, 149.9581756591797, 186.4322509765625, 150.340087890625], [250.34071350097656, 133.40533447265625, 354.5014343261719, 133.87139892578125, 354.4012145996094, 150.66932678222656, 250.17410278320312, 150.51051330566406], [18.87412452697754, 155.00717163085938, 45.177547454833984, 154.72584533691406, 45.02162170410156, 169.7573699951172, 18.682214736938477, 170.05223083496094], [69.70697021484375, 152.7261505126953, 111.62396240234375, 152.5623779296875, 111.44218444824219, 167.87229919433594, 69.53282165527344, 168.00169372558594], [121.62877655029297, 153.21897888183594, 174.8743133544922, 153.09866333007812, 174.98184204101562, 168.0441436767578, 121.53060150146484, 168.33389282226562], [188.77557373046875, 153.98548889160156, 222.404296875, 153.85594177246094, 222.6387481689453, 167.5586700439453, 188.96420288085938, 167.9335174560547], [255.1415557861328, 152.1344451904297, 350.3160400390625, 152.56185913085938, 350.2160949707031, 169.03863525390625, 254.97509765625, 168.9277801513672], [19.429306030273438, 172.44668579101562, 46.894432067871094, 172.13699340820312, 46.714256286621094, 187.709716796875, 19.26335334777832, 187.97958374023438], [70.20411682128906, 170.06295776367188, 111.75276184082031, 169.78343200683594, 111.68437957763672, 185.435791015625, 70.11253356933594, 185.660888671875], [123.21051025390625, 170.78988647460938, 177.96087646484375, 170.53871154785156, 178.02120971679688, 186.033447265625, 123.12703704833984, 186.4270477294922], [188.92420959472656, 171.68685913085938, 222.8909912109375, 171.3921661376953, 223.2003631591797, 185.16537475585938, 189.2228240966797, 185.65945434570312], [256.2185363769531, 169.98243713378906, 351.22027587890625, 170.33860778808594, 351.1379699707031, 187.9612274169922, 256.1820983886719, 187.86834716796875], [19.30258560180664, 190.08204650878906, 48.46520233154297, 189.6929168701172, 48.302528381347656, 206.10853576660156, 19.175344467163086, 206.3916473388672], [71.2640151977539, 187.9445343017578, 112.2392578125, 187.5958709716797, 112.21952056884766, 203.14700317382812, 71.21642303466797, 203.45266723632812], [123.30780792236328, 188.82704162597656, 181.00729370117188, 188.50035095214844, 181.08164978027344, 204.5990447998047, 123.25593566894531, 205.02066040039062], [188.41087341308594, 190.00706481933594, 223.39759826660156, 189.63475036621094, 223.69651794433594, 204.0068817138672, 188.75091552734375, 204.51690673828125], [256.75018310546875, 187.75538635253906, 347.371337890625, 188.01678466796875, 347.27313232421875, 205.6984405517578, 256.7555847167969, 205.7029571533203], [16.798959732055664, 207.28387451171875, 51.41717529296875, 206.74913024902344, 51.26625442504883, 222.90257263183594, 16.715612411499023, 223.30101013183594], [70.7524185180664, 206.08837890625, 112.87797546386719, 205.70079040527344, 112.964111328125, 220.7628936767578, 70.7864990234375, 221.1238555908203], [122.99986267089844, 206.7423858642578, 180.90008544921875, 206.3975830078125, 181.05979919433594, 222.3648681640625, 123.01873016357422, 222.76438903808594], [187.7908477783203, 207.73265075683594, 223.32212829589844, 207.3404083251953, 223.65733337402344, 221.9276580810547, 188.19871520996094, 222.4203643798828], [251.19993591308594, 205.2566375732422, 354.7731628417969, 205.62515258789062, 354.7079772949219, 224.70465087890625, 251.31317138671875, 224.61199951171875], [14.72948169708252, 224.38844299316406, 50.88092803955078, 223.7728729248047, 50.928714752197266, 239.8613739013672, 14.734149932861328, 240.3294677734375], [69.55723571777344, 224.27085876464844, 113.14997863769531, 223.83746337890625, 113.56038665771484, 239.0618438720703, 69.86308288574219, 239.43748474121094], [122.59346008300781, 224.72195434570312, 179.9653778076172, 224.31272888183594, 180.4770050048828, 240.1540985107422, 122.95702362060547, 240.58612060546875], [186.70167541503906, 225.8192901611328, 222.48622131347656, 225.31890869140625, 223.06971740722656, 239.73007202148438, 187.40611267089844, 240.2898406982422], [248.97561645507812, 223.67919921875, 355.39508056640625, 223.81576538085938, 355.37579345703125, 242.29685974121094, 249.43653869628906, 242.38107299804688], [13.485503196716309, 241.67575073242188, 48.320045471191406, 241.0825653076172, 48.367820739746094, 256.76220703125, 13.450210571289062, 257.180419921875], [68.01048278808594, 241.37005615234375, 113.868896484375, 241.01918029785156, 114.3763427734375, 256.5140380859375, 68.33228302001953, 256.829833984375], [123.45751190185547, 241.9761199951172, 177.7648468017578, 241.6061248779297, 178.2268829345703, 257.1756591796875, 123.70021057128906, 257.5759582519531], [186.29298400878906, 242.7563018798828, 222.6064910888672, 242.28175354003906, 223.11239624023438, 256.904052734375, 186.86215209960938, 257.4180908203125], [250.57965087890625, 242.1801300048828, 355.2619323730469, 242.14654541015625, 355.25048828125, 259.15142822265625, 251.09010314941406, 259.3343505859375], [11.677717208862305, 260.2576599121094, 44.21044158935547, 259.8990783691406, 44.24321365356445, 275.3767395019531, 11.619529724121094, 275.5721130371094], [65.93032836914062, 259.94866943359375, 114.05735778808594, 259.7777099609375, 114.74542236328125, 274.7247314453125, 66.2830581665039, 274.88775634765625], [122.2440414428711, 260.4075622558594, 181.3369598388672, 260.1610412597656, 182.0800323486328, 275.47509765625, 122.603515625, 275.7288513183594], [186.204833984375, 260.7380065917969, 224.12852478027344, 260.3567199707031, 224.75567626953125, 275.0412292480469, 186.81964111328125, 275.40771484375], [249.97579956054688, 261.2591247558594, 357.62603759765625, 261.16357421875, 357.6766662597656, 277.2168273925781, 251.14027404785156, 277.3719482421875], [10.301788330078125, 279.0636901855469, 49.4797477722168, 278.9552307128906, 50.064701080322266, 292.915283203125, 10.341358184814453, 292.8901062011719], [68.71109008789062, 278.774658203125, 104.99653625488281, 278.8114013671875, 106.2743911743164, 291.9443359375, 69.46281433105469, 291.9306945800781], [129.6405029296875, 279.0943908691406, 169.18643188476562, 278.9921569824219, 170.65478515625, 292.9345397949219, 130.85418701171875, 293.013671875], [188.4547576904297, 279.3053894042969, 224.25767517089844, 279.03863525390625, 225.54466247558594, 292.4107666015625, 189.71185302734375, 292.5948181152344], [254.70286560058594, 279.740234375, 348.700927734375, 279.4842224121094, 348.9270324707031, 294.23846435546875, 256.29571533203125, 294.4034423828125]], 'html': '<html><body><table><thead><tr><td>17 89:71</td><td>LS&#x27;S8 4678</td><td>79.98 058</td><td>8 8908</td><td>(LI-ITW) sunO ) s</td></tr></thead><tbody><tr><td></td><td>8058</td><td>$0:88</td><td>0078</td><td></td></tr><tr><td>07</td><td>68</td><td>S&#x27;16</td><td>7&#x27;6L</td><td>[]a</td></tr><tr><td>700</td><td>18</td><td>8</td><td>858</td><td>[ NA</td></tr><tr><td></td><td>98</td><td>7&#x27;S8</td><td>1&#x27;78</td><td></td></tr><tr><td></td><td>8</td><td>88</td><td>6L</td><td>[91  NN</td></tr><tr><td>98</td><td>678</td><td>7&#x27;88</td><td>7.8L</td><td>A</td></tr><tr><td></td><td>678</td><td>78</td><td>L18</td><td></td></tr><tr><td>·</td><td>078</td><td>98</td><td>T&#x27;LL</td><td>[E] NSI</td></tr><tr><td>·</td><td>L18</td><td>8</td><td>L&#x27;9L</td><td></td></tr><tr><td>7&#x27;S</td><td>18</td><td>28</td><td>6SL</td><td>)</td></tr><tr><td>II</td><td>8</td><td>758</td><td>6</td><td></td></tr><tr><td>·</td><td>8LL</td><td>08</td><td>7&#x27;</td><td> t</td></tr><tr><td>68</td><td>O&#x27;LL</td><td>0:98</td><td>0:00</td><td>[ </td></tr><tr><td></td><td>山</td><td>d</td><td>Y</td><td>Ss</td></tr></tbody></table></body></html>'}, 'img_idx': 0, 'score': 0.9211517572402954}\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "cannot open resource",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m font_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaldleOCR/doc/fonts/simfang.ttf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# PaddleOCR下提供字体包\u001b[39;00m\n\u001b[1;32m     20\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m im_show \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_structure_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfont_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m im_show \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(im_show)\n\u001b[1;32m     23\u001b[0m im_show\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workplace/signboard_ocr/PaddleOCR/ppstructure/utility.py:149\u001b[0m, in \u001b[0;36mdraw_structure_result\u001b[0;34m(image, result, font_path)\u001b[0m\n\u001b[1;32m    147\u001b[0m catid2color \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    148\u001b[0m font_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m--> 149\u001b[0m font \u001b[38;5;241m=\u001b[39m \u001b[43mImageFont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruetype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m region \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m region[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m catid2color:\n",
      "File \u001b[0;32m~/workplace/signboard_ocr/venv/lib/python3.10/site-packages/PIL/ImageFont.py:819\u001b[0m, in \u001b[0;36mtruetype\u001b[0;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FreeTypeFont(font, size, index, encoding, layout_engine)\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfreetype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_path(font):\n",
      "File \u001b[0;32m~/workplace/signboard_ocr/venv/lib/python3.10/site-packages/PIL/ImageFont.py:816\u001b[0m, in \u001b[0;36mtruetype.<locals>.freetype\u001b[0;34m(font)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfreetype\u001b[39m(font):\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFreeTypeFont\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_engine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/signboard_ocr/venv/lib/python3.10/site-packages/PIL/ImageFont.py:245\u001b[0m, in \u001b[0;36mFreeTypeFont.__init__\u001b[0;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 load_from_bytes(f)\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfont \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetfont\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_engine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayout_engine\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     load_from_bytes(font)\n",
      "\u001b[0;31mOSError\u001b[0m: cannot open resource"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from paddleocr import PPStructure, draw_structure_result, save_structure_res\n",
    "\n",
    "table_engine = PPStructure(show_log=True, image_orientation=True)\n",
    "\n",
    "save_folder = \"./output\"\n",
    "img_path = \"PaddleOCR/ppstructure/docs/table/1.png\"\n",
    "img = cv2.imread(img_path)\n",
    "result = table_engine(img)\n",
    "save_structure_res(result, save_folder, os.path.basename(img_path).split(\".\")[0])\n",
    "\n",
    "for line in result:\n",
    "    line.pop(\"img\")\n",
    "    print(line)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "font_path = \"PaddleOCR/doc/fonts/simfang.ttf\"  # PaddleOCR下提供字体包\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "im_show = draw_structure_result(image, result, font_path=font_path)\n",
    "im_show = Image.fromarray(im_show)\n",
    "im_show.save(\"result.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layout analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/07/17 08:45:07] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/nguyen/.paddleocr/whl/det/ch/ch_PP-OCRv4_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/nguyen/.paddleocr/whl/rec/ch/ch_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/home/nguyen/workplace/signboard_ocr/PaddleOCR/ppocr/utils/ppocr_keys_v1.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir=None, cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir='/home/nguyen/.paddleocr/whl/table/ch_ppstructure_mobile_v2.0_SLANet_infer', merge_no_span_structure=True, table_char_dict_path='/home/nguyen/workplace/signboard_ocr/PaddleOCR/ppocr/utils/dict/table_structure_dict_ch.txt', layout_model_dir='/home/nguyen/.paddleocr/whl/layout/picodet_lcnet_x1_0_fgd_layout_cdla_infer', layout_dict_path='/home/nguyen/workplace/signboard_ocr/PaddleOCR/ppocr/utils/dict/layout_dict/layout_cdla_dict.txt', layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=False, ocr=False, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='ch', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "{'type': 'text', 'bbox': [11, 729, 407, 847], 'res': '', 'img_idx': 0, 'score': 0.8465436697006226}\n",
      "{'type': 'text', 'bbox': [442, 754, 837, 847], 'res': '', 'img_idx': 0, 'score': 0.7548182010650635}\n",
      "{'type': 'title', 'bbox': [443, 705, 559, 719], 'res': '', 'img_idx': 0, 'score': 0.952214777469635}\n",
      "{'type': 'figure', 'bbox': [10, 1, 841, 294], 'res': '', 'img_idx': 0, 'score': 0.9753302931785583}\n",
      "{'type': 'figure_caption', 'bbox': [70, 317, 707, 357], 'res': '', 'img_idx': 0, 'score': 0.7537910342216492}\n",
      "{'type': 'table', 'bbox': [453, 359, 822, 664], 'res': '', 'img_idx': 0, 'score': 0.9647144675254822}\n",
      "{'type': 'table', 'bbox': [12, 360, 410, 716], 'res': '', 'img_idx': 0, 'score': 0.9603001475334167}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from paddleocr import PPStructure, save_structure_res\n",
    "\n",
    "table_engine = PPStructure(table=False, ocr=False, show_log=True)\n",
    "\n",
    "save_folder = \"./output\"\n",
    "img_path = \"PaddleOCR/ppstructure/docs/table/1.png\"\n",
    "img = cv2.imread(img_path)\n",
    "result = table_engine(img)\n",
    "save_structure_res(result, save_folder, os.path.basename(img_path).split(\".\")[0])\n",
    "\n",
    "for line in result:\n",
    "    line.pop(\"img\")\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/07/17 08:47:57] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/nguyen/.paddleocr/whl/det/ch/ch_PP-OCRv4_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/nguyen/.paddleocr/whl/rec/ch/ch_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/home/nguyen/workplace/signboard_ocr/PaddleOCR/ppocr/utils/ppocr_keys_v1.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir=None, cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir='/home/nguyen/.paddleocr/whl/table/ch_ppstructure_mobile_v2.0_SLANet_infer', merge_no_span_structure=True, table_char_dict_path='/home/nguyen/workplace/signboard_ocr/PaddleOCR/ppocr/utils/dict/table_structure_dict_ch.txt', layout_model_dir='/home/nguyen/.paddleocr/whl/layout/picodet_lcnet_x1_0_fgd_layout_cdla_infer', layout_dict_path='/home/nguyen/workplace/signboard_ocr/PaddleOCR/ppocr/utils/dict/layout_dict/layout_cdla_dict.txt', layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=False, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='ch', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "[2024/07/17 08:47:58] ppocr INFO: processing 1/13 page:\n",
      "[2024/07/17 08:48:00] ppocr DEBUG: dt_boxes num : 95, elapsed : 1.0815579891204834\n",
      "[2024/07/17 08:49:18] ppocr DEBUG: rec_res num  : 95, elapsed : 78.66613483428955\n",
      "[2024/07/17 08:49:18] ppocr INFO: processing 2/13 page:\n",
      "[2024/07/17 08:49:20] ppocr DEBUG: dt_boxes num : 106, elapsed : 1.207949161529541\n",
      "[2024/07/17 08:50:54] ppocr DEBUG: rec_res num  : 106, elapsed : 94.39673566818237\n",
      "[2024/07/17 08:50:54] ppocr INFO: processing 3/13 page:\n",
      "[2024/07/17 08:50:56] ppocr DEBUG: dt_boxes num : 100, elapsed : 1.0173192024230957\n",
      "[2024/07/17 08:52:24] ppocr DEBUG: rec_res num  : 100, elapsed : 88.21319270133972\n",
      "[2024/07/17 08:52:24] ppocr INFO: processing 4/13 page:\n",
      "[2024/07/17 08:52:25] ppocr DEBUG: dt_boxes num : 66, elapsed : 0.9723024368286133\n",
      "[2024/07/17 08:53:20] ppocr DEBUG: rec_res num  : 66, elapsed : 55.30825734138489\n",
      "[2024/07/17 08:53:20] ppocr INFO: processing 5/13 page:\n",
      "[2024/07/17 08:53:22] ppocr DEBUG: dt_boxes num : 94, elapsed : 0.9336309432983398\n",
      "[2024/07/17 08:54:41] ppocr DEBUG: rec_res num  : 94, elapsed : 79.18132066726685\n",
      "[2024/07/17 08:54:41] ppocr INFO: processing 6/13 page:\n",
      "[2024/07/17 08:54:42] ppocr DEBUG: dt_boxes num : 130, elapsed : 1.0408267974853516\n",
      "[2024/07/17 08:56:17] ppocr DEBUG: rec_res num  : 130, elapsed : 94.93563055992126\n",
      "[2024/07/17 08:56:17] ppocr INFO: processing 7/13 page:\n",
      "[2024/07/17 08:56:19] ppocr DEBUG: dt_boxes num : 192, elapsed : 1.2646465301513672\n",
      "[2024/07/17 08:58:10] ppocr DEBUG: rec_res num  : 192, elapsed : 111.06477570533752\n",
      "[2024/07/17 08:58:10] ppocr INFO: processing 8/13 page:\n",
      "[2024/07/17 08:58:11] ppocr DEBUG: dt_boxes num : 89, elapsed : 1.277724266052246\n",
      "[2024/07/17 08:59:34] ppocr DEBUG: rec_res num  : 89, elapsed : 82.5168731212616\n",
      "[2024/07/17 08:59:34] ppocr INFO: processing 9/13 page:\n",
      "[2024/07/17 08:59:35] ppocr DEBUG: dt_boxes num : 196, elapsed : 0.9988131523132324\n",
      "[2024/07/17 09:01:34] ppocr DEBUG: rec_res num  : 196, elapsed : 118.78380036354065\n",
      "[2024/07/17 09:01:34] ppocr INFO: processing 10/13 page:\n",
      "[2024/07/17 09:01:36] ppocr DEBUG: dt_boxes num : 116, elapsed : 1.3379185199737549\n",
      "[2024/07/17 09:03:20] ppocr DEBUG: rec_res num  : 116, elapsed : 104.07683324813843\n",
      "[2024/07/17 09:03:20] ppocr INFO: processing 11/13 page:\n",
      "[2024/07/17 09:03:21] ppocr DEBUG: dt_boxes num : 67, elapsed : 1.0794241428375244\n",
      "[2024/07/17 09:04:19] ppocr DEBUG: rec_res num  : 67, elapsed : 58.26328134536743\n",
      "[2024/07/17 09:04:19] ppocr INFO: processing 12/13 page:\n",
      "[2024/07/17 09:04:21] ppocr DEBUG: dt_boxes num : 73, elapsed : 1.0784435272216797\n",
      "[2024/07/17 09:05:13] ppocr DEBUG: rec_res num  : 73, elapsed : 52.70537257194519\n",
      "[2024/07/17 09:05:13] ppocr INFO: processing 13/13 page:\n",
      "[2024/07/17 09:05:15] ppocr DEBUG: dt_boxes num : 108, elapsed : 1.2338454723358154\n",
      "[2024/07/17 09:06:34] ppocr DEBUG: rec_res num  : 108, elapsed : 78.74963116645813\n",
      "{'type': 'text', 'bbox': [97, 789, 572, 1286], 'res': [{'text': 'Syntheticdatahasbeenacriticaltoolfortrainingscene', 'confidence': 0.9984138607978821, 'text_region': [[121.0, 785.0], [574.0, 787.0], [574.0, 810.0], [121.0, 808.0]]}, {'text': 'textdetectionandrecognitionmodels.Ontheonehand,', 'confidence': 0.9921590089797974, 'text_region': [[98.0, 812.0], [574.0, 812.0], [574.0, 833.0], [98.0, 833.0]]}, {'text': 'synthetic word images have proven tobe a successful sub-', 'confidence': 0.9864365458488464, 'text_region': [[96.0, 833.0], [575.0, 833.0], [575.0, 861.0], [96.0, 861.0]]}, {'text': 'stituteforrealimagesintrainingscenetextrecognizers.On', 'confidence': 0.9984900951385498, 'text_region': [[98.0, 860.0], [577.0, 860.0], [577.0, 883.0], [98.0, 883.0]]}, {'text': 'theotherhand,however,scenetextdetectors stillheavily', 'confidence': 0.9824748635292053, 'text_region': [[97.0, 881.0], [575.0, 883.0], [575.0, 906.0], [96.0, 904.0]]}, {'text': 'relyonalargeamountofmanuallyannotatedreal-world', 'confidence': 0.9988702535629272, 'text_region': [[100.0, 908.0], [577.0, 908.0], [577.0, 931.0], [100.0, 931.0]]}, {'text': 'images,which are expensive.In this paper,we introduce', 'confidence': 0.9575586318969727, 'text_region': [[100.0, 932.0], [575.0, 932.0], [575.0, 955.0], [100.0, 955.0]]}, {'text': 'UnrealText,anefficientimagesynthesis methodthatren-', 'confidence': 0.9878118634223938, 'text_region': [[100.0, 955.0], [574.0, 955.0], [574.0, 978.0], [100.0, 978.0]]}, {'text': 'ders realisticimagesvia a 3Dgraphics engine.3Dsyn-', 'confidence': 0.9644061326980591, 'text_region': [[98.0, 980.0], [574.0, 980.0], [574.0, 1003.0], [98.0, 1003.0]]}, {'text': 'thetic engine provides realistic appearance by rendering', 'confidence': 0.9842774271965027, 'text_region': [[95.0, 1000.0], [577.0, 1002.0], [577.0, 1030.0], [95.0, 1028.0]]}, {'text': 'scene andtext as awhole,and allowsforbettertextre-', 'confidence': 0.9560262560844421, 'text_region': [[100.0, 1028.0], [572.0, 1028.0], [572.0, 1051.0], [100.0, 1051.0]]}, {'text': 'gionproposalswith accesstoprecisesceneinformation,', 'confidence': 0.9935409426689148, 'text_region': [[98.0, 1051.0], [574.0, 1051.0], [574.0, 1074.0], [98.0, 1074.0]]}, {'text': 'e.g.normal and even object meshes.The comprehensive', 'confidence': 0.9944592714309692, 'text_region': [[93.0, 1073.0], [575.0, 1071.0], [575.0, 1099.0], [93.0, 1101.0]]}, {'text': 'experimentsverifyits effectiveness onbothscenetext de-', 'confidence': 0.9801914691925049, 'text_region': [[98.0, 1099.0], [574.0, 1099.0], [574.0, 1122.0], [98.0, 1122.0]]}, {'text': 'tection and recognition.We also generate a multilingual', 'confidence': 0.9503967761993408, 'text_region': [[98.0, 1119.0], [577.0, 1119.0], [577.0, 1147.0], [98.0, 1147.0]]}, {'text': 'versionforfutureresearchintomultilingualscenetextde-', 'confidence': 0.9976160526275635, 'text_region': [[100.0, 1147.0], [574.0, 1147.0], [574.0, 1170.0], [100.0, 1170.0]]}, {'text': 'tection andrecognition.Additionally,were-annotatescene', 'confidence': 0.9803580045700073, 'text_region': [[98.0, 1170.0], [575.0, 1170.0], [575.0, 1193.0], [98.0, 1193.0]]}, {'text': 'textrecognitiondatasetsin a case-sensitivewayandin-', 'confidence': 0.9819284081459045, 'text_region': [[98.0, 1195.0], [574.0, 1195.0], [574.0, 1218.0], [98.0, 1218.0]]}, {'text': 'clude punctuation marks for more comprehensive evalua-', 'confidence': 0.9768952131271362, 'text_region': [[97.0, 1214.0], [575.0, 1216.0], [575.0, 1244.0], [96.0, 1242.0]]}, {'text': 'tions.The codeand thegenerated datasets arereleased at:', 'confidence': 0.9596123695373535, 'text_region': [[98.0, 1242.0], [575.0, 1242.0], [575.0, 1266.0], [98.0, 1266.0]]}, {'text': 'https://jyouhou.github.io/UnrealText/', 'confidence': 0.9850108027458191, 'text_region': [[98.0, 1267.0], [399.0, 1267.0], [399.0, 1290.0], [98.0, 1290.0]]}], 'img_idx': 0, 'score': 0.9943260550498962}\n",
      "{'type': 'text', 'bbox': [615, 743, 1091, 1419], 'res': [{'text': 'els are data-thirsty,and it is expensive and sometimes dif-', 'confidence': 0.9581262469291687, 'text_region': [[617.0, 741.0], [1091.0, 741.0], [1091.0, 762.0], [617.0, 762.0]]}, {'text': 'ficult, if not impossible, to collect enough data. More-', 'confidence': 0.9818054437637329, 'text_region': [[614.0, 761.0], [1093.0, 762.0], [1093.0, 790.0], [614.0, 789.0]]}, {'text': 'over, the various applications, from traffic sign reading in', 'confidence': 0.9898492097854614, 'text_region': [[615.0, 789.0], [1093.0, 789.0], [1093.0, 812.0], [615.0, 812.0]]}, {'text': 'autonomous vehicles to instant translation, require a large', 'confidence': 0.981514573097229, 'text_region': [[617.0, 812.0], [1093.0, 812.0], [1093.0, 835.0], [617.0, 835.0]]}, {'text': 'amount of data specifically for each domain, further es-', 'confidence': 0.9906908273696899, 'text_region': [[614.0, 833.0], [1093.0, 833.0], [1093.0, 861.0], [614.0, 861.0]]}, {'text': 'calating this issue. Therefore, synthetic data and synthe-', 'confidence': 0.9836729764938354, 'text_region': [[614.0, 858.0], [1089.0, 858.0], [1089.0, 881.0], [614.0, 881.0]]}, {'text': 'sis algorithms are important for scene text tasks. Further-', 'confidence': 0.9945923686027527, 'text_region': [[615.0, 884.0], [1091.0, 884.0], [1091.0, 908.0], [615.0, 908.0]]}, {'text': 'more, synthetic data can provide detailed annotations, such', 'confidence': 0.9902983903884888, 'text_region': [[615.0, 908.0], [1093.0, 908.0], [1093.0, 931.0], [615.0, 931.0]]}, {'text': 'as character-level or even pixel-level ground truths that are', 'confidence': 0.993338942527771, 'text_region': [[615.0, 932.0], [1094.0, 932.0], [1094.0, 955.0], [615.0, 955.0]]}, {'text': 'rare for real images due to high cost.', 'confidence': 0.9884403347969055, 'text_region': [[615.0, 955.0], [911.0, 955.0], [911.0, 978.0], [615.0, 978.0]]}, {'text': 'Currently, there exist several synthesis algorithms [46,', 'confidence': 0.9953688383102417, 'text_region': [[639.0, 985.0], [1089.0, 985.0], [1089.0, 1008.0], [639.0, 1008.0]]}, {'text': '10,6, 50] that have proven beneficial. Especially, in scene', 'confidence': 0.9881605505943298, 'text_region': [[620.0, 1010.0], [1093.0, 1010.0], [1093.0, 1033.0], [620.0, 1033.0]]}, {'text': 'text recognition, training on synthetic data [10, 6] alone', 'confidence': 0.9911515712738037, 'text_region': [[614.0, 1031.0], [1094.0, 1030.0], [1094.0, 1058.0], [614.0, 1059.0]]}, {'text': 'has become a widely accepted standard practice. Some re-', 'confidence': 0.9844554662704468, 'text_region': [[615.0, 1058.0], [1091.0, 1058.0], [1091.0, 1081.0], [615.0, 1081.0]]}, {'text': 'searchers that attempt training on both synthetic and real', 'confidence': 0.9810479879379272, 'text_region': [[615.0, 1081.0], [1091.0, 1081.0], [1091.0, 1104.0], [615.0, 1104.0]]}, {'text': 'data only report marginal improvements [15, 20] on most', 'confidence': 0.9832892417907715, 'text_region': [[615.0, 1104.0], [1091.0, 1104.0], [1091.0, 1127.0], [615.0, 1127.0]]}, {'text': 'datasets. Mixing synthetic and real data is only improving', 'confidence': 0.9848539233207703, 'text_region': [[617.0, 1130.0], [1091.0, 1130.0], [1091.0, 1153.0], [617.0, 1153.0]]}, {'text': 'performance on a few difficult cases that are not yet well', 'confidence': 0.9848877191543579, 'text_region': [[615.0, 1153.0], [1093.0, 1153.0], [1093.0, 1176.0], [615.0, 1176.0]]}, {'text': 'covered by existing synthetic datasets, such as seriously', 'confidence': 0.9890692234039307, 'text_region': [[615.0, 1178.0], [1091.0, 1178.0], [1091.0, 1201.0], [615.0, 1201.0]]}, {'text': 'blurred or curved text. This is reasonable, since cropped', 'confidence': 0.9922404885292053, 'text_region': [[614.0, 1200.0], [1094.0, 1200.0], [1094.0, 1228.0], [614.0, 1228.0]]}, {'text': 'text images have much simpler background, and synthetic', 'confidence': 0.9897663593292236, 'text_region': [[615.0, 1226.0], [1093.0, 1226.0], [1093.0, 1249.0], [615.0, 1249.0]]}, {'text': 'data enjoys advantages in larger vocabulary size and diver-', 'confidence': 0.9911699295043945, 'text_region': [[614.0, 1247.0], [1093.0, 1247.0], [1093.0, 1275.0], [614.0, 1275.0]]}, {'text': 'sity of backgrounds, fonts, and lighting conditions, as well', 'confidence': 0.9813413023948669, 'text_region': [[615.0, 1274.0], [1094.0, 1274.0], [1094.0, 1297.0], [615.0, 1297.0]]}, {'text': 'as thousands of times more data samples.', 'confidence': 0.9794555902481079, 'text_region': [[615.0, 1297.0], [948.0, 1297.0], [948.0, 1320.0], [615.0, 1320.0]]}, {'text': ' On the contrary, however, scene text detection is still', 'confidence': 0.9709135890007019, 'text_region': [[635.0, 1323.0], [1094.0, 1325.0], [1094.0, 1353.0], [635.0, 1351.0]]}, {'text': 'heavily dependent on real-world data. Synthetic data [6, 50]', 'confidence': 0.9856473803520203, 'text_region': [[615.0, 1351.0], [1091.0, 1351.0], [1091.0, 1374.0], [615.0, 1374.0]]}, {'text': 'plays a less significant role, and only brings marginal im-', 'confidence': 0.975541889667511, 'text_region': [[614.0, 1376.0], [1089.0, 1376.0], [1089.0, 1399.0], [614.0, 1399.0]]}, {'text': 'provements. Existing synthesizers for scene text detec-', 'confidence': 0.9788095355033875, 'text_region': [[612.0, 1398.0], [1094.0, 1396.0], [1094.0, 1424.0], [612.0, 1426.0]]}], 'img_idx': 0, 'score': 0.9922813773155212}\n",
      "{'type': 'text', 'bbox': [99, 1354, 573, 1418], 'res': [{'text': 'With the resurgence of neural networks, the past few', 'confidence': 0.965769350528717, 'text_region': [[121.0, 1351.0], [572.0, 1351.0], [572.0, 1374.0], [121.0, 1374.0]]}, {'text': 'years have witnessed significant progress in the field of', 'confidence': 0.9765847325325012, 'text_region': [[98.0, 1376.0], [577.0, 1376.0], [577.0, 1399.0], [98.0, 1399.0]]}, {'text': 'scene text detection and recognition. However, these mod-', 'confidence': 0.9830071926116943, 'text_region': [[100.0, 1399.0], [574.0, 1399.0], [574.0, 1422.0], [100.0, 1422.0]]}], 'img_idx': 0, 'score': 0.9795377254486084}\n",
      "{'type': 'text', 'bbox': [263, 293, 925, 370], 'res': [{'text': 'Shangbang Long', 'confidence': 0.9992292523384094, 'text_region': [[311.0, 287.0], [481.0, 291.0], [480.0, 319.0], [311.0, 315.0]]}, {'text': 'Cong Yao', 'confidence': 0.9942947626113892, 'text_region': [[715.0, 289.0], [817.0, 289.0], [817.0, 317.0], [715.0, 317.0]]}, {'text': 'Carnegie Mellon University', 'confidence': 0.9893225431442261, 'text_region': [[261.0, 317.0], [531.0, 317.0], [531.0, 345.0], [261.0, 345.0]]}, {'text': 'Megvi (Face++) Technology Inc.', 'confidence': 0.9947629570960999, 'text_region': [[600.0, 317.0], [926.0, 317.0], [926.0, 345.0], [600.0, 345.0]]}, {'text': 'shangbal@cs.cmu.edu', 'confidence': 0.9976130127906799, 'text_region': [[294.0, 350.0], [501.0, 350.0], [501.0, 371.0], [294.0, 371.0]]}, {'text': 'yaocong2010@gmail.com', 'confidence': 0.9880284070968628, 'text_region': [[650.0, 350.0], [880.0, 348.0], [880.0, 371.0], [650.0, 373.0]]}], 'img_idx': 0, 'score': 0.9653185606002808}\n",
      "{'type': 'text', 'bbox': [34, 414, 75, 624], 'res': [{'text': 'O', 'confidence': 0.5061145424842834, 'text_region': [[38.0, 414.0], [63.0, 414.0], [63.0, 436.0], [38.0, 436.0]]}, {'text': '2', 'confidence': 0.9986444115638733, 'text_region': [[35.0, 429.0], [67.0, 429.0], [67.0, 457.0], [35.0, 457.0]]}, {'text': '2', 'confidence': 0.9995100498199463, 'text_region': [[34.0, 473.0], [64.0, 468.0], [67.0, 490.0], [37.0, 494.0]]}, {'text': 'A', 'confidence': 0.9159933924674988, 'text_region': [[37.0, 541.0], [68.0, 541.0], [68.0, 571.0], [37.0, 571.0]]}, {'text': '8', 'confidence': 0.9981964230537415, 'text_region': [[36.0, 576.0], [70.0, 582.0], [65.0, 608.0], [32.0, 603.0]]}], 'img_idx': 0, 'score': 0.6913373470306396}\n",
      "{'type': 'title', 'bbox': [101, 1313, 254, 1331], 'res': [{'text': '1. Introduction', 'confidence': 0.9978073835372925, 'text_region': [[98.0, 1312.0], [256.0, 1312.0], [256.0, 1335.0], [98.0, 1335.0]]}], 'img_idx': 0, 'score': 0.9531297087669373}\n",
      "{'type': 'title', 'bbox': [163, 214, 1060, 239], 'res': [{'text': 'UnrealText:SynthesizingRealisticSceneTextImagesfromtheUnrealWorld', 'confidence': 0.9962701797485352, 'text_region': [[125.0, 215.0], [1066.0, 215.0], [1066.0, 238.0], [125.0, 238.0]]}], 'img_idx': 0, 'score': 0.8778466582298279}\n",
      "{'type': 'title', 'bbox': [292, 741, 380, 757], 'res': [{'text': 'Abstract', 'confidence': 0.9991759061813354, 'text_region': [[291.0, 739.0], [384.0, 739.0], [384.0, 762.0], [291.0, 762.0]]}], 'img_idx': 0, 'score': 0.7431987524032593}\n",
      "{'type': 'figure', 'bbox': [93, 438, 1101, 661], 'res': [{'text': 'fon', 'confidence': 0.8302487730979919, 'text_region': [[236.0, 499.0], [270.0, 504.0], [267.0, 524.0], [234.0, 519.0]]}, {'text': 'ali erat', 'confidence': 0.8414570093154907, 'text_region': [[826.0, 532.0], [899.0, 528.0], [900.0, 552.0], [828.0, 556.0]]}, {'text': 'PSz', 'confidence': 0.7466602921485901, 'text_region': [[141.0, 551.0], [171.0, 551.0], [171.0, 569.0], [141.0, 569.0]]}, {'text': 're der', 'confidence': 0.9081944823265076, 'text_region': [[828.0, 556.0], [901.0, 556.0], [901.0, 587.0], [828.0, 587.0]]}, {'text': '$BP', 'confidence': 0.9890658855438232, 'text_region': [[933.0, 549.0], [958.0, 549.0], [958.0, 563.0], [933.0, 563.0]]}, {'text': 'piorrru', 'confidence': 0.5373148322105408, 'text_region': [[838.0, 601.0], [914.0, 597.0], [915.0, 615.0], [839.0, 619.0]]}, {'text': 'Lighting & Shadows', 'confidence': 0.9711035490036011, 'text_region': [[191.0, 648.0], [328.0, 648.0], [328.0, 670.0], [191.0, 670.0]]}, {'text': 'Suitable Text Region', 'confidence': 0.9951336979866028, 'text_region': [[526.0, 648.0], [664.0, 648.0], [664.0, 670.0], [526.0, 670.0]]}, {'text': 'Occlusion', 'confidence': 0.9946353435516357, 'text_region': [[895.0, 650.0], [961.0, 650.0], [961.0, 668.0], [895.0, 668.0]]}], 'img_idx': 0, 'score': 0.915780246257782}\n",
      "{'type': 'figure_caption', 'bbox': [101, 676, 1087, 719], 'res': [{'text': 'Figure 1: Demonstration of the proposed UnrealText synthesis engine, which achieves photo-realistic lighting conditions,', 'confidence': 0.9844684600830078, 'text_region': [[95.0, 670.0], [1094.0, 673.0], [1094.0, 701.0], [95.0, 698.0]]}, {'text': 'finds suitable text regions, and realizes natural occlusion (from left to right, zoomed-in views marked with green squares).', 'confidence': 0.9925628304481506, 'text_region': [[93.0, 693.0], [1076.0, 696.0], [1076.0, 724.0], [93.0, 721.0]]}], 'img_idx': 0, 'score': 0.7013322114944458}\n",
      "{'type': 'footer', 'bbox': [590, 1468, 598, 1482], 'res': [{'text': '1', 'confidence': 0.9976256489753723, 'text_region': [[589.0, 1468.0], [602.0, 1468.0], [602.0, 1483.0], [589.0, 1483.0]]}], 'img_idx': 0, 'score': 0.5806668400764465}\n",
      "{'type': 'text', 'bbox': [614, 447, 1091, 1427], 'res': [{'text': 'The synthesis of photo-realistic datasets has been a pop-', 'confidence': 0.9803723692893982, 'text_region': [[639.0, 441.0], [1091.0, 442.0], [1091.0, 470.0], [639.0, 469.0]]}, {'text': 'ular topic, since they provide detailed ground-truth annota-', 'confidence': 0.9853770732879639, 'text_region': [[615.0, 469.0], [1089.0, 469.0], [1089.0, 492.0], [615.0, 492.0]]}, {'text': 'tions at multiple granularity, and cost less than manual an-', 'confidence': 0.9852893948554993, 'text_region': [[615.0, 493.0], [1091.0, 493.0], [1091.0, 516.0], [615.0, 516.0]]}, {'text': 'notations. In scene text detection and recognition, the use of', 'confidence': 0.9927619099617004, 'text_region': [[615.0, 518.0], [1094.0, 518.0], [1094.0, 540.0], [615.0, 540.0]]}, {'text': 'synthetic datasets has become a standard practice.For scene', 'confidence': 0.9867510199546814, 'text_region': [[615.0, 540.0], [1093.0, 540.0], [1093.0, 563.0], [615.0, 563.0]]}, {'text': 'text recognition, where images contain only one word, syn-', 'confidence': 0.9852734804153442, 'text_region': [[615.0, 564.0], [1091.0, 564.0], [1091.0, 587.0], [615.0, 587.0]]}, {'text': 'thetic images are rendered through several steps [46, 10],', 'confidence': 0.9939011931419373, 'text_region': [[615.0, 587.0], [1091.0, 587.0], [1091.0, 610.0], [615.0, 610.0]]}, {'text': 'including font rendering, coloring, homography transfor-', 'confidence': 0.9919405579566956, 'text_region': [[615.0, 612.0], [1091.0, 612.0], [1091.0, 635.0], [615.0, 635.0]]}, {'text': 'mation, and background blending. Later, GANs [5] are', 'confidence': 0.9922312498092651, 'text_region': [[614.0, 634.0], [1094.0, 634.0], [1094.0, 662.0], [614.0, 662.0]]}, {'text': 'incorporated to maintain style consistency for implanted', 'confidence': 0.9823899865150452, 'text_region': [[617.0, 660.0], [1093.0, 660.0], [1093.0, 683.0], [617.0, 683.0]]}, {'text': 'text [51], but it is only for single-word images. As a re-', 'confidence': 0.9739071130752563, 'text_region': [[614.0, 680.0], [1093.0, 683.0], [1092.0, 710.0], [614.0, 706.0]]}, {'text': 'sult of these progresses, synthetic data alone are enough to', 'confidence': 0.9949098825454712, 'text_region': [[615.0, 708.0], [1093.0, 708.0], [1093.0, 731.0], [615.0, 731.0]]}, {'text': 'train state-of-the-art recognizers.', 'confidence': 0.9836062788963318, 'text_region': [[615.0, 733.0], [880.0, 733.0], [880.0, 756.0], [615.0, 756.0]]}, {'text': 'To train scene text detectors, SynthText [6] proposes to', 'confidence': 0.9919213056564331, 'text_region': [[639.0, 754.0], [1093.0, 757.0], [1093.0, 781.0], [639.0, 777.0]]}, {'text': 'generate synthetic data by printing text on background im-', 'confidence': 0.9909251928329468, 'text_region': [[615.0, 782.0], [1093.0, 782.0], [1093.0, 805.0], [615.0, 805.0]]}, {'text': ' ages. It first analyzes images with off-the-shelf models, and', 'confidence': 0.9571004509925842, 'text_region': [[612.0, 804.0], [1094.0, 800.0], [1094.0, 828.0], [612.0, 832.0]]}, {'text': 'search suitable text regions on semantically consistent re-', 'confidence': 0.9810186624526978, 'text_region': [[617.0, 828.0], [1089.0, 828.0], [1089.0, 851.0], [617.0, 851.0]]}, {'text': 'gions. Text are implanted with perspective transformation', 'confidence': 0.9937366843223572, 'text_region': [[617.0, 853.0], [1093.0, 853.0], [1093.0, 876.0], [617.0, 876.0]]}, {'text': 'based on estimated depth. To maintain semantic coherency,', 'confidence': 0.99212247133255, 'text_region': [[612.0, 873.0], [1093.0, 875.0], [1093.0, 903.0], [612.0, 901.0]]}, {'text': 'VISD [50] proposes to use semantic segmentation to filter', 'confidence': 0.9682654142379761, 'text_region': [[615.0, 899.0], [1091.0, 899.0], [1091.0, 922.0], [615.0, 922.0]]}, {'text': 'out unreasonable surfaces such as human faces. They also', 'confidence': 0.9837158918380737, 'text_region': [[615.0, 926.0], [1093.0, 926.0], [1093.0, 949.0], [615.0, 949.0]]}, {'text': 'adopt an adaptive coloring scheme to fit the text into the', 'confidence': 0.9745261073112488, 'text_region': [[617.0, 950.0], [1093.0, 950.0], [1093.0, 972.0], [617.0, 972.0]]}, {'text': 'artistic style of backgrounds. However, without consider-', 'confidence': 0.9935166239738464, 'text_region': [[615.0, 972.0], [1091.0, 972.0], [1091.0, 995.0], [615.0, 995.0]]}, {'text': 'ing the scene as a whole, these methods fail to render text', 'confidence': 0.9814046025276184, 'text_region': [[615.0, 997.0], [1093.0, 997.0], [1093.0, 1020.0], [615.0, 1020.0]]}, {'text': 'instances in a photo-realistic way, and text instances are too', 'confidence': 0.9914547801017761, 'text_region': [[615.0, 1020.0], [1093.0, 1020.0], [1093.0, 1043.0], [615.0, 1043.0]]}, {'text': ' outstanding from backgrounds. So far, the training of de-', 'confidence': 0.9921372532844543, 'text_region': [[612.0, 1040.0], [1093.0, 1040.0], [1093.0, 1068.0], [612.0, 1068.0]]}, {'text': 'tectors still relies heavily on real images.', 'confidence': 0.9761550426483154, 'text_region': [[612.0, 1064.0], [945.0, 1066.0], [945.0, 1094.0], [612.0, 1092.0]]}, {'text': 'Although GANs and other learning-based methods have', 'confidence': 0.9961960911750793, 'text_region': [[640.0, 1094.0], [1093.0, 1094.0], [1093.0, 1117.0], [640.0, 1117.0]]}, {'text': 'also shown great potential in generating realistic im-', 'confidence': 0.9574751853942871, 'text_region': [[615.0, 1115.0], [1091.0, 1115.0], [1091.0, 1138.0], [615.0, 1138.0]]}, {'text': 'ages [48, 17, 12], the generation of scene text images still', 'confidence': 0.983187735080719, 'text_region': [[614.0, 1138.0], [1094.0, 1138.0], [1094.0, 1167.0], [614.0, 1167.0]]}, {'text': 'require a large amount of manually labeled data [51]. Fur-', 'confidence': 0.9811127185821533, 'text_region': [[615.0, 1165.0], [1091.0, 1165.0], [1091.0, 1188.0], [615.0, 1188.0]]}, {'text': 'thermore, such data are sometimes not easy to collect, es-', 'confidence': 0.9920262694358826, 'text_region': [[612.0, 1185.0], [1094.0, 1186.0], [1094.0, 1214.0], [612.0, 1213.0]]}, {'text': 'pecially for cases such as low resource languages.', 'confidence': 0.9834665656089783, 'text_region': [[614.0, 1213.0], [1018.0, 1214.0], [1018.0, 1238.0], [614.0, 1236.0]]}, {'text': 'More recently, synthesizing images with 3D graph-', 'confidence': 0.968410074710846, 'text_region': [[640.0, 1239.0], [1089.0, 1239.0], [1089.0, 1262.0], [640.0, 1262.0]]}, {'text': 'ics engine has become popular in several fields, in-', 'confidence': 0.9947589039802551, 'text_region': [[614.0, 1261.0], [1093.0, 1261.0], [1093.0, 1289.0], [614.0, 1289.0]]}, {'text': 'cluding human pose estimation [43], scene understand-', 'confidence': 0.9833099246025085, 'text_region': [[615.0, 1285.0], [1091.0, 1285.0], [1091.0, 1308.0], [615.0, 1308.0]]}, {'text': 'ing/segmentation [28,24, 33, 35, 37], and object detec-', 'confidence': 0.9710463881492615, 'text_region': [[615.0, 1310.0], [1089.0, 1310.0], [1089.0, 1333.0], [615.0, 1333.0]]}, {'text': 'tion [29,42, 8]. However, these methods either consider', 'confidence': 0.9837772250175476, 'text_region': [[615.0, 1333.0], [1093.0, 1333.0], [1093.0, 1356.0], [615.0, 1356.0]]}, {'text': 'simplistic cases, e.g. rendering 3D objects on top of static', 'confidence': 0.9775848984718323, 'text_region': [[614.0, 1356.0], [1093.0, 1356.0], [1093.0, 1384.0], [614.0, 1384.0]]}, {'text': 'background images [29, 43] and randomly arranging scenes', 'confidence': 0.9958273768424988, 'text_region': [[615.0, 1383.0], [1091.0, 1383.0], [1091.0, 1406.0], [615.0, 1406.0]]}, {'text': 'filled with objects [28, 24, 35, 8], or passively use off-the-', 'confidence': 0.9843093156814575, 'text_region': [[615.0, 1406.0], [1089.0, 1406.0], [1089.0, 1429.0], [615.0, 1429.0]]}], 'img_idx': 1, 'score': 0.9966526627540588}\n",
      "{'type': 'text', 'bbox': [616, 150, 1089, 334], 'res': [{'text': 'to carry out comprehensive evaluations, and tend to over-', 'confidence': 0.9773552417755127, 'text_region': [[615.0, 148.0], [1091.0, 148.0], [1091.0, 172.0], [615.0, 172.0]]}, {'text': 'estimate the progress of scene text recognition algorithms.', 'confidence': 0.9822968244552612, 'text_region': [[614.0, 170.0], [1093.0, 170.0], [1093.0, 198.0], [614.0, 198.0]]}, {'text': 'To address this issue, we re-annotate these datasets to in-', 'confidence': 0.9929389953613281, 'text_region': [[617.0, 196.0], [1091.0, 196.0], [1091.0, 219.0], [617.0, 219.0]]}, {'text': 'clude both upper-case and lower-case characters,digits,', 'confidence': 0.9842343330383301, 'text_region': [[615.0, 218.0], [1091.0, 218.0], [1091.0, 241.0], [615.0, 241.0]]}, {'text': 'punctuation marks, and spaces if there are any. We urge', 'confidence': 0.9767993092536926, 'text_region': [[612.0, 241.0], [1094.0, 243.0], [1094.0, 271.0], [612.0, 269.0]]}, {'text': 'researchers to use the new annotations and evaluate in such', 'confidence': 0.9890762567520142, 'text_region': [[615.0, 267.0], [1093.0, 267.0], [1093.0, 290.0], [615.0, 290.0]]}, {'text': 'a full-symbol mode for better understanding of the advan-', 'confidence': 0.9851045608520508, 'text_region': [[615.0, 290.0], [1091.0, 290.0], [1091.0, 314.0], [615.0, 314.0]]}, {'text': 'tages and disadvantages of different algorithms.', 'confidence': 0.9987747073173523, 'text_region': [[614.0, 315.0], [998.0, 315.0], [998.0, 338.0], [614.0, 338.0]]}], 'img_idx': 1, 'score': 0.9936220645904541}\n",
      "{'type': 'text', 'bbox': [98, 153, 574, 1419], 'res': [{'text': 'tion follow the same paradigm. First, they analyze back-', 'confidence': 0.9875685572624207, 'text_region': [[98.0, 148.0], [572.0, 148.0], [572.0, 172.0], [98.0, 172.0]]}, {'text': 'ground images, e.g. by performing semantic segmentation', 'confidence': 0.990578830242157, 'text_region': [[95.0, 170.0], [577.0, 168.0], [577.0, 196.0], [95.0, 198.0]]}, {'text': 'and depth estimation using off-the-shelf models. Then, po-', 'confidence': 0.9706556797027588, 'text_region': [[95.0, 191.0], [575.0, 195.0], [575.0, 223.0], [95.0, 219.0]]}, {'text': 'tential locations for text embedding are extracted from the', 'confidence': 0.9900808334350586, 'text_region': [[98.0, 218.0], [574.0, 218.0], [574.0, 241.0], [98.0, 241.0]]}, {'text': 'segmented regions. Finally, text images (foregrounds) are', 'confidence': 0.9911878108978271, 'text_region': [[98.0, 244.0], [575.0, 244.0], [575.0, 267.0], [98.0, 267.0]]}, {'text': 'blended into the background images, with perceptive trans-', 'confidence': 0.9940567016601562, 'text_region': [[98.0, 267.0], [574.0, 267.0], [574.0, 290.0], [98.0, 290.0]]}, {'text': 'formation inferred from estimated depth. However, the', 'confidence': 0.9801831245422363, 'text_region': [[98.0, 290.0], [575.0, 290.0], [575.0, 314.0], [98.0, 314.0]]}, {'text': 'analysis of background images with off-the-shelf models', 'confidence': 0.9955949783325195, 'text_region': [[98.0, 315.0], [575.0, 315.0], [575.0, 338.0], [98.0, 338.0]]}, {'text': 'may be rough and imprecise. The errors further propagate', 'confidence': 0.9787765145301819, 'text_region': [[95.0, 335.0], [577.0, 337.0], [577.0, 365.0], [95.0, 363.0]]}, {'text': 'to text proposal modules and result in text being embedded', 'confidence': 0.9972422122955322, 'text_region': [[98.0, 363.0], [575.0, 363.0], [575.0, 386.0], [98.0, 386.0]]}, {'text': ' onto unsuitable locations. Moreover, the text embedding', 'confidence': 0.977604866027832, 'text_region': [[95.0, 383.0], [577.0, 384.0], [577.0, 413.0], [95.0, 411.0]]}, {'text': 'process is ignorant of the overall image conditions such as', 'confidence': 0.9934335350990295, 'text_region': [[98.0, 411.0], [575.0, 411.0], [575.0, 434.0], [98.0, 434.0]]}, {'text': 'illumination and occlusions of the scene.These two factors', 'confidence': 0.9771761894226074, 'text_region': [[97.0, 432.0], [575.0, 434.0], [575.0, 457.0], [96.0, 455.0]]}, {'text': 'make text instances outstanding from backgrounds, leading', 'confidence': 0.9802488088607788, 'text_region': [[95.0, 454.0], [577.0, 457.0], [577.0, 485.0], [95.0, 482.0]]}, {'text': 'to a gap between synthetic and real images.', 'confidence': 0.9939976334571838, 'text_region': [[98.0, 483.0], [446.0, 483.0], [446.0, 507.0], [98.0, 507.0]]}, {'text': 'In this paper, we propose a synthetic engine that syn-', 'confidence': 0.9927743077278137, 'text_region': [[121.0, 510.0], [574.0, 510.0], [574.0, 533.0], [121.0, 533.0]]}, {'text': 'thesizes scene text images from 3D virtual world.', 'confidence': 0.9793893694877625, 'text_region': [[93.0, 531.0], [530.0, 530.0], [531.0, 558.0], [93.0, 559.0]]}, {'text': 'The', 'confidence': 0.9997889995574951, 'text_region': [[521.0, 536.0], [574.0, 536.0], [574.0, 554.0], [521.0, 554.0]]}, {'text': 'proposed engine is based on the famous Unreal Engine 4', 'confidence': 0.9944997429847717, 'text_region': [[95.0, 556.0], [579.0, 554.0], [579.0, 582.0], [95.0, 584.0]]}, {'text': '(UE4), and is therefore named as UnrealText. Specifically,', 'confidence': 0.9756883978843689, 'text_region': [[98.0, 582.0], [574.0, 582.0], [574.0, 606.0], [98.0, 606.0]]}, {'text': 'text instances are regarded as planar polygon meshes with', 'confidence': 0.9829094409942627, 'text_region': [[95.0, 604.0], [577.0, 602.0], [577.0, 630.0], [95.0, 632.0]]}, {'text': 'text foregrounds loaded as texture. These meshes are placed', 'confidence': 0.9935897588729858, 'text_region': [[98.0, 630.0], [575.0, 630.0], [575.0, 653.0], [98.0, 653.0]]}, {'text': 'in suitable positions in 3D world, and rendered together', 'confidence': 0.9922081828117371, 'text_region': [[98.0, 653.0], [575.0, 653.0], [575.0, 676.0], [98.0, 676.0]]}, {'text': 'with the scene as a whole.', 'confidence': 0.9800058007240295, 'text_region': [[98.0, 678.0], [309.0, 678.0], [309.0, 700.0], [98.0, 700.0]]}, {'text': 'As shown in Fig. 1, the proposed synthesis engine, by', 'confidence': 0.9910548329353333, 'text_region': [[123.0, 705.0], [572.0, 705.0], [572.0, 728.0], [123.0, 728.0]]}, {'text': 'its very nature, enjoys the following advantages over pre-', 'confidence': 0.9970638751983643, 'text_region': [[98.0, 729.0], [574.0, 729.0], [574.0, 752.0], [98.0, 752.0]]}, {'text': 'vious methods: (1) Text and scenes are rendered together,', 'confidence': 0.9879262447357178, 'text_region': [[98.0, 752.0], [575.0, 752.0], [575.0, 776.0], [98.0, 776.0]]}, {'text': 'achieving realistic visual effects, e.g. illumination, occlu-', 'confidence': 0.9887470006942749, 'text_region': [[96.0, 776.0], [575.0, 776.0], [575.0, 804.0], [96.0, 804.0]]}, {'text': 'sion, and perspective transformation. (2) The method has', 'confidence': 0.9879733324050903, 'text_region': [[98.0, 800.0], [575.0, 800.0], [575.0, 823.0], [98.0, 823.0]]}, {'text': 'access to precise scene information, e.g. normal, depth, and', 'confidence': 0.9852427244186401, 'text_region': [[98.0, 825.0], [575.0, 825.0], [575.0, 848.0], [98.0, 848.0]]}, {'text': 'object meshes, and therefore can generate better text region', 'confidence': 0.9768375158309937, 'text_region': [[98.0, 848.0], [574.0, 848.0], [574.0, 871.0], [98.0, 871.0]]}, {'text': ' proposals. These aspects are crucial in training detectors.', 'confidence': 0.9905708432197571, 'text_region': [[93.0, 871.0], [557.0, 870.0], [557.0, 898.0], [93.0, 899.0]]}, {'text': 'To further exploit the potential of UnrealText, we design', 'confidence': 0.980921745300293, 'text_region': [[118.0, 896.0], [577.0, 898.0], [577.0, 926.0], [118.0, 924.0]]}, {'text': 'three key components: (1) A view finding algorithm that', 'confidence': 0.9765242338180542, 'text_region': [[98.0, 922.0], [575.0, 922.0], [575.0, 945.0], [98.0, 945.0]]}, {'text': 'explores the virtual scenes and generates camera viewpoints', 'confidence': 0.9931870698928833, 'text_region': [[100.0, 949.0], [575.0, 949.0], [575.0, 972.0], [100.0, 972.0]]}, {'text': 'to obtain more diverse and natural backgrounds. (2) An en-', 'confidence': 0.9922683835029602, 'text_region': [[98.0, 972.0], [572.0, 972.0], [572.0, 995.0], [98.0, 995.0]]}, {'text': 'vironment randomization module that changes the lighting', 'confidence': 0.9969350695610046, 'text_region': [[98.0, 997.0], [575.0, 997.0], [575.0, 1020.0], [98.0, 1020.0]]}, {'text': 'conditions regularly, to simulate real-world variations.', 'confidence': 0.995219349861145, 'text_region': [[98.0, 1020.0], [542.0, 1020.0], [542.0, 1043.0], [98.0, 1043.0]]}, {'text': '.(3)', 'confidence': 0.863472580909729, 'text_region': [[534.0, 1021.0], [574.0, 1021.0], [574.0, 1041.0], [534.0, 1041.0]]}, {'text': 'A mesh-based text region generation method that finds suit-', 'confidence': 0.9925909638404846, 'text_region': [[98.0, 1044.0], [575.0, 1044.0], [575.0, 1068.0], [98.0, 1068.0]]}, {'text': ' able positions for text by probing the 3D meshes.', 'confidence': 0.9894160628318787, 'text_region': [[95.0, 1064.0], [494.0, 1066.0], [494.0, 1094.0], [95.0, 1092.0]]}, {'text': 'The contributions of this paper are summarized as fol-', 'confidence': 0.9971570372581482, 'text_region': [[118.0, 1092.0], [575.0, 1091.0], [575.0, 1119.0], [118.0, 1120.0]]}, {'text': 'lows: (1) We propose a brand-new scene text image syn-', 'confidence': 0.9715552926063538, 'text_region': [[96.0, 1117.0], [575.0, 1117.0], [575.0, 1145.0], [96.0, 1145.0]]}, {'text': 'thesis engine that renders images from 3D world, which is', 'confidence': 0.9838660359382629, 'text_region': [[98.0, 1143.0], [575.0, 1143.0], [575.0, 1167.0], [98.0, 1167.0]]}, {'text': 'entirely different from previous approaches that embed text', 'confidence': 0.9871304035186768, 'text_region': [[96.0, 1165.0], [577.0, 1165.0], [577.0, 1193.0], [96.0, 1193.0]]}, {'text': 'on 2D background images, termed as UnrealText. The pro-', 'confidence': 0.991174042224884, 'text_region': [[98.0, 1190.0], [572.0, 1190.0], [572.0, 1213.0], [98.0, 1213.0]]}, {'text': ' posed engine achieves realistic rendering effects and high', 'confidence': 0.9874369502067566, 'text_region': [[95.0, 1213.0], [577.0, 1211.0], [577.0, 1239.0], [95.0, 1241.0]]}, {'text': 'scalability. (2) With the proposed techniques, the synthe-', 'confidence': 0.9654176235198975, 'text_region': [[98.0, 1238.0], [570.0, 1238.0], [570.0, 1261.0], [98.0, 1261.0]]}, {'text': 'sis engine improves the performance of detectors and rec-', 'confidence': 0.9895492792129517, 'text_region': [[95.0, 1261.0], [574.0, 1259.0], [574.0, 1287.0], [95.0, 1289.0]]}, {'text': 'ognizers significantly. (3) We also generate a large scale', 'confidence': 0.9729381203651428, 'text_region': [[96.0, 1284.0], [575.0, 1284.0], [575.0, 1312.0], [96.0, 1312.0]]}, {'text': 'multilingual scene text dataset that will aid further research.', 'confidence': 0.9913449883460999, 'text_region': [[98.0, 1310.0], [574.0, 1310.0], [574.0, 1333.0], [98.0, 1333.0]]}, {'text': '(4) Additionally, we notice that many of the popular scene', 'confidence': 0.9869991540908813, 'text_region': [[98.0, 1335.0], [575.0, 1335.0], [575.0, 1358.0], [98.0, 1358.0]]}, {'text': 'text recognition datasets are only annotated in an incom-', 'confidence': 0.9763432741165161, 'text_region': [[98.0, 1360.0], [574.0, 1360.0], [574.0, 1381.0], [98.0, 1381.0]]}, {'text': 'plete way, providing only case-insensitive word annota-', 'confidence': 0.9897478818893433, 'text_region': [[98.0, 1383.0], [574.0, 1383.0], [574.0, 1406.0], [98.0, 1406.0]]}, {'text': 'tions. With such limited annotations, researchers are unable', 'confidence': 0.9904988408088684, 'text_region': [[98.0, 1406.0], [575.0, 1406.0], [575.0, 1429.0], [98.0, 1429.0]]}], 'img_idx': 1, 'score': 0.988400399684906}\n",
      "{'type': 'title', 'bbox': [617, 408, 818, 426], 'res': [{'text': '2.1. Synthetic Images', 'confidence': 0.9938879609107971, 'text_region': [[614.0, 401.0], [820.0, 404.0], [820.0, 432.0], [613.0, 429.0]]}], 'img_idx': 1, 'score': 0.961324155330658}\n",
      "{'type': 'title', 'bbox': [617, 367, 784, 385], 'res': [{'text': '2.Related Work', 'confidence': 0.969277024269104, 'text_region': [[617.0, 365.0], [785.0, 365.0], [785.0, 388.0], [617.0, 388.0]]}], 'img_idx': 1, 'score': 0.9474605321884155}\n",
      "{'type': 'text', 'bbox': [615, 282, 1091, 778], 'res': [{'text': ' In this section, we give a detailed introduction to our', 'confidence': 0.9742662310600281, 'text_region': [[635.0, 276.0], [1094.0, 277.0], [1094.0, 305.0], [635.0, 304.0]]}, {'text': 'scene text image synthesis engine, UnrealText, which is de-', 'confidence': 0.9673098921775818, 'text_region': [[615.0, 304.0], [1091.0, 304.0], [1091.0, 327.0], [615.0, 327.0]]}, {'text': 'veloped upon UE4 and the UnrealCV plugin [31]. The syn-', 'confidence': 0.983841061592102, 'text_region': [[617.0, 328.0], [1091.0, 328.0], [1091.0, 351.0], [617.0, 351.0]]}, {'text': 'thesis engine: (1) produces photo-realistic images, (2) is', 'confidence': 0.9855144619941711, 'text_region': [[614.0, 351.0], [1091.0, 351.0], [1091.0, 375.0], [614.0, 375.0]]}, {'text': 'efficient, taking about only 1-1.5 second to render and gen-', 'confidence': 0.9902315735816956, 'text_region': [[615.0, 375.0], [1091.0, 375.0], [1091.0, 398.0], [615.0, 398.0]]}, {'text': 'erate a new scene text image and, (3) is general and com-', 'confidence': 0.9963226318359375, 'text_region': [[615.0, 399.0], [1091.0, 399.0], [1091.0, 422.0], [615.0, 422.0]]}, {'text': 'patible to off-the-shelf 3D scene models. As shown in Fig.', 'confidence': 0.9965386390686035, 'text_region': [[614.0, 421.0], [1091.0, 421.0], [1091.0, 449.0], [614.0, 449.0]]}, {'text': '2,the pipeline mainly consists of a Viewfinder module (sec-', 'confidence': 0.9838486909866333, 'text_region': [[617.0, 447.0], [1089.0, 447.0], [1089.0, 470.0], [617.0, 470.0]]}, {'text': 'tion 3.2),an Environment Randomization module (section', 'confidence': 0.9781250953674316, 'text_region': [[615.0, 470.0], [1093.0, 470.0], [1093.0, 493.0], [615.0, 493.0]]}, {'text': '3.3),a Text Region Generation module (section 3.4),and a', 'confidence': 0.9754845499992371, 'text_region': [[617.0, 495.0], [1093.0, 495.0], [1093.0, 516.0], [617.0, 516.0]]}, {'text': 'Text Rendering module (section 3.5).', 'confidence': 0.9774126410484314, 'text_region': [[615.0, 518.0], [913.0, 518.0], [913.0, 541.0], [615.0, 541.0]]}, {'text': 'Firstly, the viewfinder module explores around the 3D', 'confidence': 0.9973341822624207, 'text_region': [[639.0, 544.0], [1093.0, 544.0], [1093.0, 568.0], [639.0, 568.0]]}, {'text': 'scene with the camera, generating camera viewpoints.', 'confidence': 0.9768588542938232, 'text_region': [[614.0, 568.0], [1093.0, 568.0], [1093.0, 596.0], [614.0, 596.0]]}, {'text': 'Then, the environment lighting is randomly adjusted. Next,', 'confidence': 0.9962544441223145, 'text_region': [[615.0, 592.0], [1091.0, 592.0], [1091.0, 615.0], [615.0, 615.0]]}, {'text': 'the text regions are proposed based on 2D scene informa-', 'confidence': 0.9877961874008179, 'text_region': [[614.0, 615.0], [1088.0, 615.0], [1088.0, 639.0], [614.0, 639.0]]}, {'text': 'tion and refined with 3D mesh information in the graph-', 'confidence': 0.9841106534004211, 'text_region': [[615.0, 640.0], [1089.0, 640.0], [1089.0, 663.0], [615.0, 663.0]]}, {'text': 'ics engine. After that, text foregrounds are generated with', 'confidence': 0.9838641285896301, 'text_region': [[617.0, 663.0], [1093.0, 663.0], [1093.0, 686.0], [617.0, 686.0]]}, {'text': 'randomly sampled fonts, colors, and text content, and are', 'confidence': 0.9906230568885803, 'text_region': [[615.0, 688.0], [1093.0, 688.0], [1093.0, 711.0], [615.0, 711.0]]}, {'text': 'loaded as planar meshes. Finally, we retrieve the RGB im-', 'confidence': 0.9820765852928162, 'text_region': [[615.0, 709.0], [1091.0, 711.0], [1091.0, 734.0], [615.0, 733.0]]}, {'text': 'age and corresponding text locations as well as text content', 'confidence': 0.9797459244728088, 'text_region': [[617.0, 736.0], [1093.0, 736.0], [1093.0, 759.0], [617.0, 759.0]]}, {'text': 'to make the synthetic dataset.', 'confidence': 0.9841274619102478, 'text_region': [[617.0, 759.0], [853.0, 759.0], [853.0, 782.0], [617.0, 782.0]]}], 'img_idx': 2, 'score': 0.9953912496566772}\n",
      "{'type': 'text', 'bbox': [98, 149, 573, 573], 'res': [{'text': 'shelf 3D scenes without further changing it [33]. In con-', 'confidence': 0.9777377843856812, 'text_region': [[100.0, 148.0], [574.0, 148.0], [574.0, 172.0], [100.0, 172.0]]}, {'text': 'trast to these researches, our proposed synthesis engine im-', 'confidence': 0.9775963425636292, 'text_region': [[96.0, 170.0], [575.0, 170.0], [575.0, 198.0], [96.0, 198.0]]}, {'text': 'plements active and regular interaction with 3D scenes, to', 'confidence': 0.9859516024589539, 'text_region': [[98.0, 196.0], [577.0, 196.0], [577.0, 219.0], [98.0, 219.0]]}, {'text': ' generate realistic and diverse scene text images.', 'confidence': 0.9911795854568481, 'text_region': [[96.0, 216.0], [481.0, 216.0], [481.0, 244.0], [96.0, 244.0]]}, {'text': 'This paper is also a sequel to our previous attempt, the', 'confidence': 0.9930113554000854, 'text_region': [[121.0, 243.0], [577.0, 243.0], [577.0, 271.0], [121.0, 271.0]]}, {'text': 'SynthText3D[16].SynthText3D closelyfollows the designs', 'confidence': 0.9840239882469177, 'text_region': [[97.0, 267.0], [575.0, 269.0], [575.0, 292.0], [96.0, 290.0]]}, {'text': 'of the SynthText method. While SynthText uses off-the-', 'confidence': 0.9756768345832825, 'text_region': [[98.0, 292.0], [575.0, 292.0], [575.0, 315.0], [98.0, 315.0]]}, {'text': 'shelf computer vision models to estimate segmentation and', 'confidence': 0.9949306845664978, 'text_region': [[98.0, 317.0], [577.0, 317.0], [577.0, 340.0], [98.0, 340.0]]}, {'text': 'depth maps for background images, SynthText3D uses the', 'confidence': 0.9878916144371033, 'text_region': [[100.0, 342.0], [575.0, 342.0], [575.0, 365.0], [100.0, 365.0]]}, {'text': 'ground-truth segmentation and depth maps provided by the', 'confidence': 0.9880377650260925, 'text_region': [[95.0, 363.0], [577.0, 361.0], [577.0, 389.0], [95.0, 391.0]]}, {'text': '3D engines.The rendering process of SynthText3D does', 'confidence': 0.9855884909629822, 'text_region': [[100.0, 389.0], [575.0, 389.0], [575.0, 412.0], [100.0, 412.0]]}, {'text': 'not involve interactions with the 3D worlds, such as the ob-', 'confidence': 0.985358476638794, 'text_region': [[98.0, 412.0], [574.0, 412.0], [574.0, 436.0], [98.0, 436.0]]}, {'text': 'ject meshes. As a result, SynthText3D is faced with at least', 'confidence': 0.9820518493652344, 'text_region': [[98.0, 437.0], [577.0, 437.0], [577.0, 460.0], [98.0, 460.0]]}, {'text': 'these two limitations: (1) the camera locations and rotations', 'confidence': 0.9862303137779236, 'text_region': [[98.0, 460.0], [575.0, 460.0], [575.0, 483.0], [98.0, 483.0]]}, {'text': 'are labeled by human, limiting the scalability as well as di-', 'confidence': 0.980918824672699, 'text_region': [[98.0, 483.0], [574.0, 483.0], [574.0, 507.0], [98.0, 507.0]]}, {'text': 'versity; (2) the generated text regions are limited to well', 'confidence': 0.9889984130859375, 'text_region': [[98.0, 508.0], [575.0, 508.0], [575.0, 531.0], [98.0, 531.0]]}, {'text': ' defined regions that the camera is facing upfront, resulting', 'confidence': 0.9822320938110352, 'text_region': [[95.0, 528.0], [577.0, 530.0], [577.0, 558.0], [95.0, 556.0]]}, {'text': 'in a unfavorable location bias.', 'confidence': 0.991153359413147, 'text_region': [[97.0, 554.0], [341.0, 556.0], [341.0, 579.0], [96.0, 577.0]]}], 'img_idx': 2, 'score': 0.9951606392860413}\n",
      "{'type': 'text', 'bbox': [98, 639, 573, 1426], 'res': [{'text': 'Scene text detection and recognition, possibly as the', 'confidence': 0.9750134944915771, 'text_region': [[121.0, 639.0], [574.0, 639.0], [574.0, 662.0], [121.0, 662.0]]}, {'text': 'most human-centric computer vision task, has been a pop-', 'confidence': 0.995816171169281, 'text_region': [[98.0, 663.0], [574.0, 663.0], [574.0, 686.0], [98.0, 686.0]]}, {'text': 'ular research topic for many years [49, 21]. In scene text', 'confidence': 0.9906029105186462, 'text_region': [[96.0, 685.0], [577.0, 685.0], [577.0, 713.0], [96.0, 713.0]]}, {'text': 'detection, there are mainly two branches of methodolo-', 'confidence': 0.9887304902076721, 'text_region': [[98.0, 711.0], [572.0, 711.0], [572.0, 734.0], [98.0, 734.0]]}, {'text': 'gies: Top-down methods that inherit the idea of region pro-', 'confidence': 0.9888976216316223, 'text_region': [[96.0, 733.0], [575.0, 733.0], [575.0, 761.0], [96.0, 761.0]]}, {'text': 'posal networks from general object detectors that detect text', 'confidence': 0.9941814541816711, 'text_region': [[98.0, 759.0], [575.0, 759.0], [575.0, 782.0], [98.0, 782.0]]}, {'text': 'instances as rotated rectangles and polygons [19, 53, 11,', 'confidence': 0.9919931888580322, 'text_region': [[96.0, 780.0], [575.0, 779.0], [575.0, 807.0], [97.0, 809.0]]}, {'text': '52, 47]; Bottom-up approaches that predict local segments', 'confidence': 0.9970842599868774, 'text_region': [[98.0, 804.0], [577.0, 804.0], [577.0, 832.0], [98.0, 832.0]]}, {'text': 'and local geometric attributes, and compose them into in-', 'confidence': 0.9867135882377625, 'text_region': [[97.0, 828.0], [572.0, 830.0], [572.0, 853.0], [96.0, 851.0]]}, {'text': 'dividual text instances [38, 22, 2, 40]. Despite significant', 'confidence': 0.9772546887397766, 'text_region': [[98.0, 851.0], [574.0, 851.0], [574.0, 874.0], [98.0, 874.0]]}, {'text': 'improvements on individual datasets, those most widely', 'confidence': 0.9893398284912109, 'text_region': [[98.0, 878.0], [574.0, 878.0], [574.0, 901.0], [98.0, 901.0]]}, {'text': 'used benchmark datasets are usually very small, with only', 'confidence': 0.9811667799949646, 'text_region': [[95.0, 898.0], [575.0, 901.0], [575.0, 927.0], [95.0, 924.0]]}, {'text': 'around 500 to 1000 images in test sets, and are therefore', 'confidence': 0.9959352016448975, 'text_region': [[98.0, 926.0], [575.0, 926.0], [575.0, 949.0], [98.0, 949.0]]}, {'text': ' prone to over-fitting. The generalization ability across dif-', 'confidence': 0.96291583776474, 'text_region': [[95.0, 947.0], [575.0, 945.0], [575.0, 973.0], [95.0, 975.0]]}, {'text': 'ferent domains remains an open question, and is not studied', 'confidence': 0.9934168457984924, 'text_region': [[96.0, 972.0], [575.0, 972.0], [575.0, 1000.0], [96.0, 1000.0]]}, {'text': 'yet.The reason lies in the very limited real data and that', 'confidence': 0.9646404385566711, 'text_region': [[96.0, 998.0], [575.0, 997.0], [575.0, 1020.0], [97.0, 1021.0]]}, {'text': 'synthetic data are not effective enough. Therefore, one im-', 'confidence': 0.9853670001029968, 'text_region': [[98.0, 1021.0], [574.0, 1021.0], [574.0, 1044.0], [98.0, 1044.0]]}, {'text': 'portant motivation of our synthesis engine is to serve as a', 'confidence': 0.9894543290138245, 'text_region': [[98.0, 1046.0], [577.0, 1046.0], [577.0, 1069.0], [98.0, 1069.0]]}, {'text': 'stepping stone towards general scene text detection.', 'confidence': 0.9974256157875061, 'text_region': [[96.0, 1069.0], [514.0, 1069.0], [514.0, 1092.0], [96.0, 1092.0]]}, {'text': 'Most scene text recognition models consist of CNN-', 'confidence': 0.985697329044342, 'text_region': [[123.0, 1096.0], [574.0, 1096.0], [574.0, 1119.0], [123.0, 1119.0]]}, {'text': 'based image feature extractors and attentional LSTM [9] or', 'confidence': 0.9826579093933105, 'text_region': [[98.0, 1119.0], [575.0, 1119.0], [575.0, 1142.0], [98.0, 1142.0]]}, {'text': 'transformer[44]-based encoder-decoder to predict the tex-', 'confidence': 0.9809489250183105, 'text_region': [[98.0, 1143.0], [574.0, 1143.0], [574.0, 1165.0], [98.0, 1165.0]]}, {'text': 'tual content [3,39,15,23]. Since the encoder-decoder mod-', 'confidence': 0.9829311966896057, 'text_region': [[98.0, 1167.0], [574.0, 1167.0], [574.0, 1188.0], [98.0, 1188.0]]}, {'text': 'ule is a language model in essence, scene text recognizers', 'confidence': 0.9918033480644226, 'text_region': [[98.0, 1191.0], [575.0, 1191.0], [575.0, 1214.0], [98.0, 1214.0]]}, {'text': 'have a high demand for training data with a large vocabu-', 'confidence': 0.9911783933639526, 'text_region': [[98.0, 1214.0], [574.0, 1214.0], [574.0, 1238.0], [98.0, 1238.0]]}, {'text': 'lary, which is extremely difficult for real-world data. Be-', 'confidence': 0.9769684672355652, 'text_region': [[98.0, 1238.0], [570.0, 1238.0], [570.0, 1261.0], [98.0, 1261.0]]}, {'text': 'sides, scene text recognizers work on image crops that have', 'confidence': 0.9662124514579773, 'text_region': [[97.0, 1259.0], [577.0, 1261.0], [577.0, 1289.0], [96.0, 1287.0]]}, {'text': 'simple backgrounds,which are easy to synthesize. There-', 'confidence': 0.9826065301895142, 'text_region': [[98.0, 1285.0], [572.0, 1285.0], [572.0, 1308.0], [98.0, 1308.0]]}, {'text': 'fore, synthetic data are necessary for scene text recogniz-', 'confidence': 0.9899640083312988, 'text_region': [[98.0, 1310.0], [572.0, 1310.0], [572.0, 1333.0], [98.0, 1333.0]]}, {'text': 'ers, and synthetic data alone are usually enough to achieve', 'confidence': 0.9796911478042603, 'text_region': [[98.0, 1332.0], [577.0, 1332.0], [577.0, 1360.0], [98.0, 1360.0]]}, {'text': 'state-of-the-art performance. Moreover, since the recogni-', 'confidence': 0.996437132358551, 'text_region': [[98.0, 1358.0], [572.0, 1358.0], [572.0, 1381.0], [98.0, 1381.0]]}, {'text': 'tion modules require a large amount of data, synthetic data', 'confidence': 0.9865597486495972, 'text_region': [[98.0, 1383.0], [575.0, 1383.0], [575.0, 1406.0], [98.0, 1406.0]]}, {'text': 'are also necessary in training end-to-end text spotting sys-', 'confidence': 0.9809840321540833, 'text_region': [[97.0, 1406.0], [572.0, 1407.0], [572.0, 1431.0], [96.0, 1429.0]]}], 'img_idx': 2, 'score': 0.9943821430206299}\n",
      "{'type': 'text', 'bbox': [615, 846, 1089, 1104], 'res': [{'text': 'The aim of the viewfinder module is to automatically de-', 'confidence': 0.9871965050697327, 'text_region': [[640.0, 842.0], [1089.0, 842.0], [1089.0, 865.0], [640.0, 865.0]]}, {'text': 'termine a set of camera locations and rotations from the', 'confidence': 0.9743191599845886, 'text_region': [[615.0, 868.0], [1091.0, 866.0], [1091.0, 889.0], [615.0, 891.0]]}, {'text': 'whole space of 3D scenes that are reasonable and non-', 'confidence': 0.9913859367370605, 'text_region': [[615.0, 889.0], [1093.0, 889.0], [1093.0, 917.0], [615.0, 917.0]]}, {'text': 'trivial, getting rid of unsuitable viewpoints such as from', 'confidence': 0.9953888654708862, 'text_region': [[615.0, 916.0], [1093.0, 916.0], [1093.0, 939.0], [615.0, 939.0]]}, {'text': 'inside object meshes (e.g. Fig. 3 bottom right).', 'confidence': 0.9942662119865417, 'text_region': [[615.0, 940.0], [998.0, 940.0], [998.0, 964.0], [615.0, 964.0]]}, {'text': 'Learning-based methods such as navigation and explo-', 'confidence': 0.9974682927131653, 'text_region': [[639.0, 965.0], [1091.0, 965.0], [1091.0, 988.0], [639.0, 988.0]]}, {'text': 'ration algorithms may require extra training data and are', 'confidence': 0.9833803176879883, 'text_region': [[615.0, 990.0], [1093.0, 990.0], [1093.0, 1013.0], [615.0, 1013.0]]}, {'text': 'not guaranteed to generalize to different 3D scenes. There-', 'confidence': 0.9741594195365906, 'text_region': [[612.0, 1011.0], [1091.0, 1010.0], [1091.0, 1038.0], [612.0, 1040.0]]}, {'text': 'fore, we turn to rule-based methods and design a physically-', 'confidence': 0.9837377667427063, 'text_region': [[612.0, 1033.0], [1093.0, 1035.0], [1093.0, 1063.0], [612.0, 1061.0]]}, {'text': 'constrained 3D random walk (Fig. 3 first row) equipped', 'confidence': 0.9642661809921265, 'text_region': [[612.0, 1056.0], [1094.0, 1059.0], [1094.0, 1087.0], [612.0, 1084.0]]}, {'text': 'withauxiliarycamera anchors.', 'confidence': 0.97679603099823, 'text_region': [[615.0, 1086.0], [870.0, 1086.0], [870.0, 1109.0], [615.0, 1109.0]]}], 'img_idx': 2, 'score': 0.9942435026168823}\n",
      "{'type': 'text', 'bbox': [615, 1192, 1090, 1424], 'res': [{'text': 'Starting from a valid location, the physically-constrained', 'confidence': 0.9845849275588989, 'text_region': [[617.0, 1191.0], [1093.0, 1191.0], [1093.0, 1213.0], [617.0, 1213.0]]}, {'text': '3D random walk aims to find the next valid and non-trivial', 'confidence': 0.9705942273139954, 'text_region': [[615.0, 1213.0], [1093.0, 1214.0], [1093.0, 1238.0], [615.0, 1236.0]]}, {'text': 'location. In contrast to being valid, locations are invalid if', 'confidence': 0.9980869889259338, 'text_region': [[617.0, 1239.0], [1093.0, 1239.0], [1093.0, 1262.0], [617.0, 1262.0]]}, {'text': 'they are inside object meshes or far away from the scene', 'confidence': 0.9930166602134705, 'text_region': [[612.0, 1261.0], [1093.0, 1259.0], [1093.0, 1287.0], [612.0, 1289.0]]}, {'text': 'boundary, for example. A non-trivial location should be not', 'confidence': 0.9906011819839478, 'text_region': [[615.0, 1285.0], [1093.0, 1285.0], [1093.0, 1308.0], [615.0, 1308.0]]}, {'text': 'too close to the current location. Otherwise, the new view-', 'confidence': 0.9814153909683228, 'text_region': [[615.0, 1310.0], [1089.0, 1310.0], [1089.0, 1333.0], [615.0, 1333.0]]}, {'text': 'point will be similar to the current one. The proposed 3D', 'confidence': 0.9967840909957886, 'text_region': [[612.0, 1332.0], [1094.0, 1330.0], [1094.0, 1358.0], [612.0, 1360.0]]}, {'text': 'random walk uses ray-casting [36], which is constrained by', 'confidence': 0.9876514673233032, 'text_region': [[615.0, 1358.0], [1091.0, 1358.0], [1091.0, 1381.0], [615.0, 1381.0]]}, {'text': 'physically, to inspect the physical environment to determine', 'confidence': 0.9944819211959839, 'text_region': [[615.0, 1383.0], [1091.0, 1383.0], [1091.0, 1406.0], [615.0, 1406.0]]}, {'text': 'valid and non-triviallocations.', 'confidence': 0.9785479307174683, 'text_region': [[615.0, 1404.0], [862.0, 1406.0], [861.0, 1429.0], [615.0, 1427.0]]}], 'img_idx': 2, 'score': 0.9934934377670288}\n",
      "{'type': 'title', 'bbox': [617, 200, 960, 220], 'res': [{'text': '3.Scene Text in 3DVirtual World', 'confidence': 0.940263569355011, 'text_region': [[615.0, 200.0], [963.0, 200.0], [963.0, 223.0], [615.0, 223.0]]}], 'img_idx': 2, 'score': 0.9558064341545105}\n",
      "{'type': 'title', 'bbox': [98, 601, 490, 620], 'res': [{'text': '2.2.Scene Text Detection and Recognition', 'confidence': 0.9924556016921997, 'text_region': [[97.0, 597.0], [494.0, 599.0], [494.0, 624.0], [96.0, 622.0]]}], 'img_idx': 2, 'score': 0.951274573802948}\n",
      "{'type': 'title', 'bbox': [617, 806, 758, 822], 'res': [{'text': '3.2.Viewfinder', 'confidence': 0.997185230255127, 'text_region': [[615.0, 805.0], [762.0, 805.0], [762.0, 827.0], [615.0, 827.0]]}], 'img_idx': 2, 'score': 0.948306679725647}\n",
      "{'type': 'title', 'bbox': [617, 243, 745, 258], 'res': [{'text': '3.1.Overview', 'confidence': 0.9927756786346436, 'text_region': [[615.0, 241.0], [748.0, 241.0], [748.0, 262.0], [615.0, 262.0]]}], 'img_idx': 2, 'score': 0.9468831419944763}\n",
      "{'type': 'title', 'bbox': [617, 1151, 1032, 1169], 'res': [{'text': '3.2.1', 'confidence': 0.9998174905776978, 'text_region': [[617.0, 1152.0], [682.0, 1152.0], [682.0, 1170.0], [617.0, 1170.0]]}, {'text': 'Physically-Constrained3DRandomWalk', 'confidence': 0.9966251254081726, 'text_region': [[674.0, 1150.0], [1034.0, 1150.0], [1034.0, 1171.0], [674.0, 1171.0]]}], 'img_idx': 2, 'score': 0.9422235488891602}\n",
      "{'type': 'header', 'bbox': [616, 149, 746, 168], 'res': [{'text': 'tems [18, 7, 30].', 'confidence': 0.9726613163948059, 'text_region': [[615.0, 147.0], [748.0, 147.0], [748.0, 170.0], [615.0, 170.0]]}], 'img_idx': 2, 'score': 0.6621649265289307}\n",
      "{'type': 'text', 'bbox': [615, 1169, 1089, 1426], 'res': [{'text': 'In real-world,text instances are usually embedded on', 'confidence': 0.9648271203041077, 'text_region': [[639.0, 1167.0], [1093.0, 1167.0], [1093.0, 1188.0], [639.0, 1188.0]]}, {'text': 'well-defined surfaces, e.g. traffic signs, to maintain good', 'confidence': 0.9892122149467468, 'text_region': [[615.0, 1191.0], [1093.0, 1191.0], [1093.0, 1214.0], [615.0, 1214.0]]}, {'text': 'legibility. Previous works find suitable regions by using es-', 'confidence': 0.9986042380332947, 'text_region': [[612.0, 1211.0], [1093.0, 1213.0], [1093.0, 1241.0], [612.0, 1239.0]]}, {'text': 'timated scene information, such as gPb-UCM [1] in Syn-', 'confidence': 0.9879529476165771, 'text_region': [[615.0, 1239.0], [1089.0, 1239.0], [1089.0, 1262.0], [615.0, 1262.0]]}, {'text': 'thText [6] or saliency map in VISD [50] for approxima-', 'confidence': 0.9672675728797913, 'text_region': [[612.0, 1259.0], [1091.0, 1261.0], [1091.0, 1289.0], [612.0, 1287.0]]}, {'text': 'tion. However, these methods are imprecise and often fail', 'confidence': 0.9897341728210449, 'text_region': [[615.0, 1285.0], [1091.0, 1285.0], [1091.0, 1308.0], [615.0, 1308.0]]}, {'text': 'to find appropriate regions. Therefore, we propose to find', 'confidence': 0.9931994676589966, 'text_region': [[614.0, 1308.0], [1094.0, 1308.0], [1094.0, 1336.0], [614.0, 1336.0]]}, {'text': 'text regions by probing around object meshes in 3D world.', 'confidence': 0.9794372916221619, 'text_region': [[612.0, 1333.0], [1092.0, 1330.0], [1093.0, 1356.0], [612.0, 1360.0]]}, {'text': 'Since inspecting all object meshes is time-consuming, we', 'confidence': 0.9875405430793762, 'text_region': [[614.0, 1358.0], [1091.0, 1358.0], [1091.0, 1381.0], [614.0, 1381.0]]}, {'text': 'propose a 2-staged pipeline: (1) We retrieve ground truth', 'confidence': 0.9776772856712341, 'text_region': [[615.0, 1383.0], [1093.0, 1383.0], [1093.0, 1406.0], [615.0, 1406.0]]}, {'text': 'surface normal map to generate initial text region propos-', 'confidence': 0.9796411395072937, 'text_region': [[614.0, 1406.0], [1088.0, 1406.0], [1088.0, 1429.0], [614.0, 1429.0]]}], 'img_idx': 3, 'score': 0.994692862033844}\n",
      "{'type': 'text', 'bbox': [98, 611, 573, 892], 'res': [{'text': 'In each step, we first randomly change the pitch and yaw', 'confidence': 0.9836760759353638, 'text_region': [[121.0, 609.0], [574.0, 609.0], [574.0, 632.0], [121.0, 632.0]]}, {'text': 'values of the camera rotation, making the camera pointing', 'confidence': 0.9967791438102722, 'text_region': [[100.0, 634.0], [574.0, 634.0], [574.0, 657.0], [100.0, 657.0]]}, {'text': 'to a new direction. Then, we cast a ray from the camera lo-', 'confidence': 0.9791880249977112, 'text_region': [[98.0, 657.0], [570.0, 657.0], [570.0, 680.0], [98.0, 680.0]]}, {'text': 'cation towards the direction of the viewpoint. The ray stops', 'confidence': 0.9869264364242554, 'text_region': [[97.0, 676.0], [575.0, 678.0], [575.0, 706.0], [96.0, 705.0]]}, {'text': 'when it hits any object meshes or reaches a fixed maximum', 'confidence': 0.9811688661575317, 'text_region': [[100.0, 705.0], [575.0, 705.0], [575.0, 728.0], [100.0, 728.0]]}, {'text': 'length. By design, the path from the current location to the', 'confidence': 0.9929565787315369, 'text_region': [[98.0, 729.0], [575.0, 729.0], [575.0, 752.0], [98.0, 752.0]]}, {'text': 'stopping position is free of any barrier, i.e. not inside of', 'confidence': 0.9866246581077576, 'text_region': [[95.0, 751.0], [579.0, 747.0], [579.0, 775.0], [95.0, 779.0]]}, {'text': 'any object meshes. Therefore, points along this ray path are', 'confidence': 0.9961614012718201, 'text_region': [[98.0, 777.0], [575.0, 777.0], [575.0, 800.0], [98.0, 800.0]]}, {'text': 'all valid. Finally, we randomly sample one point between', 'confidence': 0.9870732426643372, 'text_region': [[98.0, 800.0], [575.0, 800.0], [575.0, 823.0], [98.0, 823.0]]}, {'text': 'the ↓-th and 2-th of this path, and set it as the new location', 'confidence': 0.973308801651001, 'text_region': [[98.0, 823.0], [577.0, 823.0], [577.0, 846.0], [98.0, 846.0]]}, {'text': ' of the camera, which is non-trivial. The proposed random', 'confidence': 0.9867282509803772, 'text_region': [[95.0, 845.0], [579.0, 846.0], [579.0, 875.0], [95.0, 873.0]]}, {'text': 'walk algorithm can generate diverse camera viewpoints.', 'confidence': 0.9748692512512207, 'text_region': [[97.0, 870.0], [549.0, 871.0], [549.0, 894.0], [96.0, 893.0]]}], 'img_idx': 3, 'score': 0.9940662980079651}\n",
      "{'type': 'text', 'bbox': [98, 970, 573, 1253], 'res': [{'text': 'The proposed random walk algorithm, however, is ineffi-', 'confidence': 0.9949632287025452, 'text_region': [[98.0, 969.0], [572.0, 969.0], [572.0, 992.0], [98.0, 992.0]]}, {'text': 'cient in terms of exploration. Therefore, we manually select', 'confidence': 0.9758991599082947, 'text_region': [[98.0, 992.0], [575.0, 992.0], [575.0, 1015.0], [98.0, 1015.0]]}, {'text': 'a set of N camera anchors across the3Dscenes as start-', 'confidence': 0.949741780757904, 'text_region': [[98.0, 1016.0], [574.0, 1016.0], [574.0, 1038.0], [98.0, 1038.0]]}, {'text': 'ing points. After every T steps, we reset the location of', 'confidence': 0.9847475290298462, 'text_region': [[95.0, 1038.0], [579.0, 1034.0], [579.0, 1063.0], [95.0, 1066.0]]}, {'text': 'the camera to arandomly sampled camera anchor.Weset', 'confidence': 0.9565311670303345, 'text_region': [[98.0, 1064.0], [577.0, 1064.0], [577.0, 1086.0], [98.0, 1086.0]]}, {'text': 'N = 150-200 and T = 100. Note that the selection of cam-', 'confidence': 0.9771376252174377, 'text_region': [[100.0, 1084.0], [572.0, 1084.0], [572.0, 1107.0], [100.0, 1107.0]]}, {'text': 'era anchors requires only little carefulness. We only need to', 'confidence': 0.9883823990821838, 'text_region': [[98.0, 1112.0], [575.0, 1112.0], [575.0, 1135.0], [98.0, 1135.0]]}, {'text': 'ensure coverage over the space. It takes around 20 to 30 sec-', 'confidence': 0.9880588054656982, 'text_region': [[96.0, 1135.0], [575.0, 1132.0], [575.0, 1158.0], [97.0, 1162.0]]}, {'text': 'ondsforeachscene,which is trivial and not abottleneckof', 'confidence': 0.9672386050224304, 'text_region': [[100.0, 1160.0], [577.0, 1160.0], [577.0, 1181.0], [100.0, 1181.0]]}, {'text': 'scalability. The manual but efficient selection of camera is', 'confidence': 0.9942262768745422, 'text_region': [[100.0, 1183.0], [575.0, 1183.0], [575.0, 1206.0], [100.0, 1206.0]]}, {'text': 'compatible with the proposed random walk algorithm that', 'confidence': 0.9801084995269775, 'text_region': [[100.0, 1206.0], [575.0, 1206.0], [575.0, 1229.0], [100.0, 1229.0]]}, {'text': 'generates diverse viewpoints.', 'confidence': 0.9918479323387146, 'text_region': [[98.0, 1233.0], [334.0, 1233.0], [334.0, 1256.0], [98.0, 1256.0]]}], 'img_idx': 3, 'score': 0.994035005569458}\n",
      "{'type': 'text', 'bbox': [99, 1311, 573, 1426], 'res': [{'text': 'To produce real-world variations such as lighting condi-', 'confidence': 0.9939749836921692, 'text_region': [[121.0, 1310.0], [574.0, 1310.0], [574.0, 1333.0], [121.0, 1333.0]]}, {'text': 'tions, we randomly change the intensity, color, and direction', 'confidence': 0.9873207211494446, 'text_region': [[98.0, 1333.0], [575.0, 1333.0], [575.0, 1356.0], [98.0, 1356.0]]}, {'text': 'of all light sources in the scene. In addition to illuminations,', 'confidence': 0.9898601174354553, 'text_region': [[98.0, 1358.0], [575.0, 1358.0], [575.0, 1381.0], [98.0, 1381.0]]}, {'text': 'wealsoaddfogconditions andrandomly adjustitsinten-', 'confidence': 0.9833459258079529, 'text_region': [[100.0, 1383.0], [574.0, 1383.0], [574.0, 1404.0], [100.0, 1404.0]]}, {'text': 'sity. The environment randomization proves to increase the', 'confidence': 0.9910038709640503, 'text_region': [[98.0, 1406.0], [574.0, 1406.0], [574.0, 1429.0], [98.0, 1429.0]]}], 'img_idx': 3, 'score': 0.9909243583679199}\n",
      "{'type': 'text', 'bbox': [615, 854, 1089, 992], 'res': [{'text': 'Figure 3:1', 'confidence': 0.9156943559646606, 'text_region': [[614.0, 849.0], [724.0, 853.0], [723.0, 876.0], [613.0, 873.0]]}, {'text': 'In the first row (1)-(4),we illustrate the', 'confidence': 0.9520401954650879, 'text_region': [[713.0, 853.0], [1093.0, 853.0], [1093.0, 874.0], [713.0, 874.0]]}, {'text': 'physically-constrained 3Drandom walk.Forbetter visu-', 'confidence': 0.9737735390663147, 'text_region': [[614.0, 876.0], [1091.0, 874.0], [1091.0, 898.0], [614.0, 899.0]]}, {'text': 'alization, we use a camera object to represent the viewpoint', 'confidence': 0.9656820297241211, 'text_region': [[614.0, 899.0], [1091.0, 899.0], [1091.0, 922.0], [614.0, 922.0]]}, {'text': '(marked with green boxes and arrows). In the second row,', 'confidence': 0.9927898049354553, 'text_region': [[615.0, 924.0], [1091.0, 924.0], [1091.0, 947.0], [615.0, 947.0]]}, {'text': 'we compare viewpoints from the proposed method with ran-', 'confidence': 0.9891567230224609, 'text_region': [[617.0, 949.0], [1089.0, 949.0], [1089.0, 972.0], [617.0, 972.0]]}, {'text': 'domly sampled viewpoints.', 'confidence': 0.9995967745780945, 'text_region': [[617.0, 973.0], [837.0, 973.0], [837.0, 997.0], [617.0, 997.0]]}], 'img_idx': 3, 'score': 0.9883981943130493}\n",
      "{'type': 'text', 'bbox': [615, 1043, 1091, 1109], 'res': [{'text': 'diversity of the generated images and results in stronger de-', 'confidence': 0.9881585836410522, 'text_region': [[617.0, 1041.0], [1089.0, 1041.0], [1089.0, 1064.0], [617.0, 1064.0]]}, {'text': 'tector performance. The proposed randomization can also', 'confidence': 0.9901728630065918, 'text_region': [[615.0, 1064.0], [1094.0, 1064.0], [1094.0, 1087.0], [615.0, 1087.0]]}, {'text': 'benefit sim-to-real domain adaptation [41].', 'confidence': 0.9924309253692627, 'text_region': [[614.0, 1087.0], [960.0, 1087.0], [960.0, 1110.0], [614.0, 1110.0]]}], 'img_idx': 3, 'score': 0.9803655743598938}\n",
      "{'type': 'title', 'bbox': [99, 1274, 409, 1291], 'res': [{'text': '3.3.EnvironmentRandomization', 'confidence': 0.9984589219093323, 'text_region': [[98.0, 1274.0], [412.0, 1274.0], [412.0, 1295.0], [98.0, 1295.0]]}], 'img_idx': 3, 'score': 0.9587211012840271}\n",
      "{'type': 'title', 'bbox': [99, 931, 389, 949], 'res': [{'text': '3.2.2Auxiliary CameraAnchors', 'confidence': 0.971541702747345, 'text_region': [[98.0, 929.0], [392.0, 929.0], [392.0, 952.0], [98.0, 952.0]]}], 'img_idx': 3, 'score': 0.9515886306762695}\n",
      "{'type': 'title', 'bbox': [636, 1130, 861, 1149], 'res': [{'text': '3.4. Text Region Generation', 'confidence': 0.9614615440368652, 'text_region': [[615.0, 1129.0], [881.0, 1129.0], [881.0, 1152.0], [615.0, 1152.0]]}], 'img_idx': 3, 'score': 0.8225803971290588}\n",
      "{'type': 'figure', 'bbox': [612, 604, 1087, 844], 'res': [], 'img_idx': 3, 'score': 0.9629294276237488}\n",
      "{'type': 'figure', 'bbox': [125, 135, 1072, 509], 'res': [{'text': 'Viewfinding', 'confidence': 0.9984807968139648, 'text_region': [[351.0, 153.0], [422.0, 153.0], [422.0, 170.0], [351.0, 170.0]]}, {'text': 'Environment', 'confidence': 0.9968519806861877, 'text_region': [[474.0, 144.0], [552.0, 144.0], [552.0, 160.0], [474.0, 160.0]]}, {'text': 'Randomization', 'confidence': 0.9978455901145935, 'text_region': [[471.0, 163.0], [562.0, 163.0], [562.0, 180.0], [471.0, 180.0]]}, {'text': 'TextRegionGeneration', 'confidence': 0.9950510859489441, 'text_region': [[632.0, 155.0], [765.0, 155.0], [765.0, 167.0], [632.0, 167.0]]}, {'text': 'lextRendering', 'confidence': 0.9645837545394897, 'text_region': [[896.0, 249.0], [981.0, 249.0], [981.0, 261.0], [896.0, 261.0]]}, {'text': 'Miranda) >jbsErti', 'confidence': 0.9063529372215271, 'text_region': [[866.0, 449.0], [1008.0, 449.0], [1008.0, 465.0], [866.0, 465.0]]}, {'text': 'Font', 'confidence': 0.9974099397659302, 'text_region': [[787.0, 459.0], [828.0, 459.0], [828.0, 474.0], [787.0, 474.0]]}, {'text': 'Rendering', 'confidence': 0.9908229112625122, 'text_region': [[759.0, 470.0], [837.0, 474.0], [836.0, 490.0], [758.0, 486.0]]}, {'text': 'Stoneybut\"', 'confidence': 0.979648232460022, 'text_region': [[866.0, 464.0], [965.0, 464.0], [965.0, 480.0], [866.0, 480.0]]}, {'text': 'actions Hdj. Bull', 'confidence': 0.9540870785713196, 'text_region': [[865.0, 477.0], [1003.0, 477.0], [1003.0, 498.0], [865.0, 498.0]]}], 'img_idx': 3, 'score': 0.9470424652099609}\n",
      "{'type': 'figure_caption', 'bbox': [101, 524, 996, 590], 'res': [{'text': 'Figure 2: The pipeline of the proposed synthesis method. The arrows indicate the order. For simplicity, we only show one text', 'confidence': 0.9895504117012024, 'text_region': [[98.0, 523.0], [1093.0, 523.0], [1093.0, 546.0], [98.0, 546.0]]}, {'text': 'region. From left to right: scene overview, diverse viewpoints, various lighting conditions (light color, intensity, shadows,', 'confidence': 0.9895957112312317, 'text_region': [[95.0, 545.0], [1093.0, 543.0], [1093.0, 571.0], [95.0, 573.0]]}, {'text': 'etc.), text region generation and text rendering.', 'confidence': 0.9981447458267212, 'text_region': [[95.0, 568.0], [476.0, 569.0], [476.0, 597.0], [95.0, 596.0]]}], 'img_idx': 3, 'score': 0.8207054734230042}\n",
      "{'type': 'text', 'bbox': [99, 359, 573, 663], 'res': [{'text': 'In computer graphics, normal values are unit vectors that', 'confidence': 0.9871883988380432, 'text_region': [[98.0, 356.0], [575.0, 356.0], [575.0, 380.0], [98.0, 380.0]]}, {'text': 'are perpendicular to a surface. Therefore, when projected', 'confidence': 0.9905283451080322, 'text_region': [[98.0, 381.0], [575.0, 381.0], [575.0, 404.0], [98.0, 404.0]]}, {'text': 'to 2D screen space, a region with similar normal values', 'confidence': 0.9847919344902039, 'text_region': [[95.0, 403.0], [577.0, 403.0], [577.0, 431.0], [95.0, 431.0]]}, {'text': 'tends to be a well-defined region to embed text on.Wefind', 'confidence': 0.9598590731620789, 'text_region': [[98.0, 429.0], [575.0, 429.0], [575.0, 450.0], [98.0, 450.0]]}, {'text': 'valid image regions by applying sliding windows of 64 × 64', 'confidence': 0.9890794157981873, 'text_region': [[95.0, 450.0], [577.0, 449.0], [577.0, 477.0], [95.0, 479.0]]}, {'text': 'pixels across the surface normal map, and retrieve those', 'confidence': 0.9737070798873901, 'text_region': [[96.0, 475.0], [577.0, 475.0], [577.0, 503.0], [96.0, 503.0]]}, {'text': 'with smooth surface normal:the minimum cosine similar-', 'confidence': 0.9756894707679749, 'text_region': [[100.0, 502.0], [574.0, 502.0], [574.0, 523.0], [100.0, 523.0]]}, {'text': 'ity value between any two pixels is larger than a threshold', 'confidence': 0.9952290058135986, 'text_region': [[98.0, 523.0], [574.0, 523.0], [574.0, 546.0], [98.0, 546.0]]}, {'text': 't.We set t to 0.95,which proves to produce reasonable', 'confidence': 0.9872725605964661, 'text_region': [[96.0, 548.0], [574.0, 548.0], [574.0, 571.0], [96.0, 571.0]]}, {'text': 'results. We randomly sample at most 10 non-overlapping', 'confidence': 0.9957294464111328, 'text_region': [[95.0, 568.0], [577.0, 571.0], [577.0, 599.0], [95.0, 596.0]]}, {'text': 'valid image regions to make the initial proposals. Making', 'confidence': 0.9914719462394714, 'text_region': [[97.0, 592.0], [577.0, 594.0], [577.0, 622.0], [96.0, 620.0]]}, {'text': ' proposals from normal maps is an efficient way to find po-', 'confidence': 0.9782814383506775, 'text_region': [[95.0, 619.0], [577.0, 617.0], [577.0, 645.0], [95.0, 647.0]]}, {'text': 'tential and visible regions.', 'confidence': 0.9625396728515625, 'text_region': [[95.0, 640.0], [313.0, 642.0], [312.0, 670.0], [95.0, 668.0]]}], 'img_idx': 4, 'score': 0.9943445920944214}\n",
      "{'type': 'text', 'bbox': [615, 401, 1089, 681], 'res': [{'text': 'Figure 4: Ilustration of the refinement of initial proposals.', 'confidence': 0.9829236268997192, 'text_region': [[615.0, 399.0], [1089.0, 399.0], [1089.0, 422.0], [615.0, 422.0]]}, {'text': 'We draw green bounding boxes to represent proposals in 2D', 'confidence': 0.992377758026123, 'text_region': [[614.0, 421.0], [1094.0, 421.0], [1094.0, 449.0], [614.0, 449.0]]}, {'text': 'screen space, and use planar meshes to represent proposals', 'confidence': 0.9858890175819397, 'text_region': [[615.0, 446.0], [1093.0, 444.0], [1093.0, 469.0], [615.0, 470.0]]}, {'text': 'in 3D space.(1) Initial proposals are made in 2D space.', 'confidence': 0.987524151802063, 'text_region': [[615.0, 470.0], [1091.0, 470.0], [1091.0, 493.0], [615.0, 493.0]]}, {'text': '(2) When we project them into 3D world and inspect them', 'confidence': 0.9806265234947205, 'text_region': [[617.0, 493.0], [1093.0, 493.0], [1093.0, 516.0], [617.0, 516.0]]}, {'text': 'from the front view, they are in distorted forms. (3) Based', 'confidence': 0.995823085308075, 'text_region': [[614.0, 516.0], [1091.0, 516.0], [1091.0, 540.0], [614.0, 540.0]]}, {'text': 'on the sizes of the distorted proposals and the positions of', 'confidence': 0.9757964015007019, 'text_region': [[614.0, 540.0], [1094.0, 540.0], [1094.0, 568.0], [614.0, 568.0]]}, {'text': 'the center points, we re-initialize orthogonal squares on the', 'confidence': 0.988208532333374, 'text_region': [[614.0, 564.0], [1089.0, 564.0], [1089.0, 587.0], [614.0, 587.0]]}, {'text': 'same surfaces with horizontal sides orthogonal to the grav-', 'confidence': 0.987126886844635, 'text_region': [[612.0, 586.0], [1093.0, 587.0], [1093.0, 615.0], [612.0, 614.0]]}, {'text': 'ity direction. (5) Then we expand the squares. (6) Finally,', 'confidence': 0.9945838451385498, 'text_region': [[615.0, 614.0], [1091.0, 614.0], [1091.0, 637.0], [615.0, 637.0]]}, {'text': 'we obtain text regions in 2D screen space with natural per-', 'confidence': 0.9882382750511169, 'text_region': [[615.0, 639.0], [1091.0, 639.0], [1091.0, 662.0], [615.0, 662.0]]}, {'text': 'spective distortion.', 'confidence': 0.9662759900093079, 'text_region': [[612.0, 660.0], [771.0, 656.0], [772.0, 685.0], [612.0, 688.0]]}], 'img_idx': 4, 'score': 0.993869960308075}\n",
      "{'type': 'text', 'bbox': [615, 977, 1090, 1233], 'res': [{'text': 'The proposed synthesis engine is implemented based on', 'confidence': 0.9981674551963806, 'text_region': [[642.0, 973.0], [1093.0, 973.0], [1093.0, 997.0], [642.0, 997.0]]}, {'text': 'UE4.22 and the UnrealCV plugin. On an ubuntu worksta-', 'confidence': 0.976628303527832, 'text_region': [[617.0, 998.0], [1091.0, 998.0], [1091.0, 1021.0], [617.0, 1021.0]]}, {'text': 'tion with an 8-core Intel CPU,an NVIDIA GeForce RTX', 'confidence': 0.9564399719238281, 'text_region': [[615.0, 1023.0], [1093.0, 1023.0], [1093.0, 1044.0], [615.0, 1044.0]]}, {'text': '2070 GPU, and 16G RAM, the synthesis speed is 0.7-1.5', 'confidence': 0.9980452656745911, 'text_region': [[614.0, 1044.0], [1094.0, 1044.0], [1094.0, 1072.0], [614.0, 1072.0]]}, {'text': 'seconds per image with a resolution of 1080 x 720, depend-', 'confidence': 0.9739994406700134, 'text_region': [[615.0, 1071.0], [1091.0, 1071.0], [1091.0, 1094.0], [615.0, 1094.0]]}, {'text': 'ing on the complexity of the scene model.', 'confidence': 0.9853241443634033, 'text_region': [[615.0, 1094.0], [951.0, 1094.0], [951.0, 1117.0], [615.0, 1117.0]]}, {'text': 'We collect 30 scene models from the official UE4 mar-', 'confidence': 0.9816034436225891, 'text_region': [[639.0, 1117.0], [1091.0, 1117.0], [1091.0, 1140.0], [639.0, 1140.0]]}, {'text': 'ketplace. The engine is used to generate 600K scene text', 'confidence': 0.991623044013977, 'text_region': [[615.0, 1142.0], [1093.0, 1142.0], [1093.0, 1165.0], [615.0, 1165.0]]}, {'text': 'images with English words. With the same configura-', 'confidence': 0.9966678023338318, 'text_region': [[614.0, 1163.0], [1093.0, 1163.0], [1093.0, 1191.0], [614.0, 1191.0]]}, {'text': 'tion, we also generate a multilingual version, making it the', 'confidence': 0.9770654439926147, 'text_region': [[615.0, 1190.0], [1093.0, 1190.0], [1093.0, 1213.0], [615.0, 1213.0]]}, {'text': 'largest multilingual scene text dataset.', 'confidence': 0.9928494691848755, 'text_region': [[617.0, 1214.0], [923.0, 1214.0], [923.0, 1238.0], [617.0, 1238.0]]}], 'img_idx': 4, 'score': 0.9938281178474426}\n",
      "{'type': 'text', 'bbox': [99, 736, 574, 1111], 'res': [{'text': 'As shown in Fig. 4, rectangular initial proposals in 2D', 'confidence': 0.9825069904327393, 'text_region': [[96.0, 731.0], [577.0, 729.0], [577.0, 757.0], [97.0, 759.0]]}, {'text': 'screen space will be distorted when projected into 3D', 'confidence': 0.993583083152771, 'text_region': [[95.0, 756.0], [577.0, 752.0], [577.0, 780.0], [95.0, 784.0]]}, {'text': 'world. Thus, we need to first rectify the proposals in 3D', 'confidence': 0.9696496725082397, 'text_region': [[98.0, 780.0], [577.0, 780.0], [577.0, 804.0], [98.0, 804.0]]}, {'text': 'world. We project the center point of the initial proposals', 'confidence': 0.9926193952560425, 'text_region': [[98.0, 805.0], [575.0, 805.0], [575.0, 828.0], [98.0, 828.0]]}, {'text': 'into 3D space, and re-initialize orthogonal squares on the', 'confidence': 0.9894900321960449, 'text_region': [[95.0, 825.0], [577.0, 827.0], [577.0, 855.0], [95.0, 853.0]]}, {'text': 'corresponding mesh surfaces around the center points: the', 'confidence': 0.9915006160736084, 'text_region': [[100.0, 851.0], [574.0, 851.0], [574.0, 874.0], [100.0, 874.0]]}, {'text': 'horizontal sides are orthogonal to the gravity direction. The', 'confidence': 0.9910674095153809, 'text_region': [[98.0, 876.0], [575.0, 876.0], [575.0, 899.0], [98.0, 899.0]]}, {'text': 'side lengths are set to the shortest sides of the quadrilaterals', 'confidence': 0.9737000465393066, 'text_region': [[98.0, 901.0], [574.0, 901.0], [574.0, 924.0], [98.0, 924.0]]}, {'text': 'created by projecting the four corners of initial proposals', 'confidence': 0.988192617893219, 'text_region': [[100.0, 926.0], [574.0, 926.0], [574.0, 949.0], [100.0, 949.0]]}, {'text': 'into the 3D space. Then we enlarge the widths and heights', 'confidence': 0.9677750468254089, 'text_region': [[98.0, 949.0], [574.0, 949.0], [574.0, 972.0], [98.0, 972.0]]}, {'text': 'along the horizontal and vertical sides alternatively.', 'confidence': 0.9779385328292847, 'text_region': [[98.0, 973.0], [536.0, 973.0], [536.0, 995.0], [98.0, 995.0]]}, {'text': 'The', 'confidence': 0.9998106360435486, 'text_region': [[527.0, 975.0], [574.0, 975.0], [574.0, 993.0], [527.0, 993.0]]}, {'text': 'expansion of one direction stops when the sides of that di-', 'confidence': 0.9892188310623169, 'text_region': [[96.0, 995.0], [574.0, 992.0], [574.0, 1020.0], [97.0, 1023.0]]}, {'text': \"rection get off the surface', hit other meshes, or reach the\", 'confidence': 0.9846895337104797, 'text_region': [[98.0, 1018.0], [572.0, 1018.0], [572.0, 1041.0], [98.0, 1041.0]]}, {'text': 'preset maximum expansion ratio. The proposed refining al-', 'confidence': 0.9932166934013367, 'text_region': [[98.0, 1044.0], [574.0, 1044.0], [574.0, 1068.0], [98.0, 1068.0]]}, {'text': 'gorithm works in 3D world space, and is able to produce', 'confidence': 0.9825371503829956, 'text_region': [[96.0, 1066.0], [575.0, 1066.0], [575.0, 1094.0], [96.0, 1094.0]]}, {'text': 'natural homography transformation in 2D screen space.', 'confidence': 0.9929669499397278, 'text_region': [[98.0, 1092.0], [544.0, 1092.0], [544.0, 1115.0], [98.0, 1115.0]]}], 'img_idx': 4, 'score': 0.9934237599372864}\n",
      "{'type': 'text', 'bbox': [99, 149, 573, 284], 'res': [{'text': 'als; (2) Initial proposals are then projected to and refined', 'confidence': 0.9900580048561096, 'text_region': [[98.0, 148.0], [577.0, 148.0], [577.0, 172.0], [98.0, 172.0]]}, {'text': 'in the 3D world using object meshes. Finally, we sample', 'confidence': 0.9932624101638794, 'text_region': [[98.0, 172.0], [574.0, 172.0], [574.0, 195.0], [98.0, 195.0]]}, {'text': 'a subset from the refined proposals to render. To avoid oc-', 'confidence': 0.9910478591918945, 'text_region': [[96.0, 195.0], [574.0, 195.0], [574.0, 223.0], [96.0, 223.0]]}, {'text': 'clusion among proposals, we project them back to screen', 'confidence': 0.998878002166748, 'text_region': [[96.0, 218.0], [577.0, 218.0], [577.0, 246.0], [96.0, 246.0]]}, {'text': 'space, and discard regions that overlap with each other one', 'confidence': 0.9852104187011719, 'text_region': [[98.0, 244.0], [577.0, 244.0], [577.0, 267.0], [98.0, 267.0]]}, {'text': 'by one in a shuffled order until occlusion is eliminated.', 'confidence': 0.9884856343269348, 'text_region': [[96.0, 267.0], [539.0, 267.0], [539.0, 290.0], [96.0, 290.0]]}], 'img_idx': 4, 'score': 0.9921624660491943}\n",
      "{'type': 'text', 'bbox': [99, 1167, 573, 1352], 'res': [{'text': 'Generating Text Images: Given text regions as proposed', 'confidence': 0.9966570734977722, 'text_region': [[97.0, 1162.0], [577.0, 1163.0], [577.0, 1191.0], [96.0, 1190.0]]}, {'text': 'and refined in section 3.4, the text generation module sam-', 'confidence': 0.9795243144035339, 'text_region': [[98.0, 1190.0], [574.0, 1190.0], [574.0, 1213.0], [98.0, 1213.0]]}, {'text': 'ples text content and renders text images with certain fonts', 'confidence': 0.9885724186897278, 'text_region': [[98.0, 1214.0], [575.0, 1214.0], [575.0, 1238.0], [98.0, 1238.0]]}, {'text': 'and text colors. The numbers of lines and characters per', 'confidence': 0.9877505898475647, 'text_region': [[95.0, 1234.0], [575.0, 1236.0], [575.0, 1264.0], [95.0, 1262.0]]}, {'text': 'line are determined by the font size and the size of refined', 'confidence': 0.9673702120780945, 'text_region': [[98.0, 1262.0], [575.0, 1262.0], [575.0, 1284.0], [98.0, 1284.0]]}, {'text': 'proposals in 2D space to make sure the characters are not', 'confidence': 0.9890094995498657, 'text_region': [[95.0, 1284.0], [577.0, 1282.0], [577.0, 1310.0], [95.0, 1312.0]]}, {'text': 'too small and ensure legibility. For a fairer comparison, we', 'confidence': 0.9935045838356018, 'text_region': [[98.0, 1310.0], [575.0, 1310.0], [575.0, 1333.0], [98.0, 1333.0]]}, {'text': 'also use the same font set from Google Fonts 2 as SynthText', 'confidence': 0.9696027636528015, 'text_region': [[98.0, 1333.0], [575.0, 1333.0], [575.0, 1356.0], [98.0, 1356.0]]}], 'img_idx': 4, 'score': 0.989393413066864}\n",
      "{'type': 'text', 'bbox': [616, 1335, 1090, 1425], 'res': [{'text': 'We first verify the effectiveness of the proposed engine', 'confidence': 0.982707679271698, 'text_region': [[639.0, 1332.0], [1093.0, 1332.0], [1093.0, 1360.0], [639.0, 1360.0]]}, {'text': 'by training detectors on the synthesized images and evaluat-', 'confidence': 0.9834885001182556, 'text_region': [[614.0, 1356.0], [1091.0, 1356.0], [1091.0, 1384.0], [614.0, 1384.0]]}, {'text': 'ing them on real image datasets. We use a previous yet time-', 'confidence': 0.9964216351509094, 'text_region': [[614.0, 1383.0], [1088.0, 1383.0], [1088.0, 1406.0], [614.0, 1406.0]]}, {'text': 'tested state-of-the-art model, EAST [53], which is fast and', 'confidence': 0.9957655668258667, 'text_region': [[615.0, 1406.0], [1093.0, 1406.0], [1093.0, 1429.0], [615.0, 1429.0]]}], 'img_idx': 4, 'score': 0.9875823259353638}\n",
      "{'type': 'text', 'bbox': [615, 778, 1090, 919], 'res': [{'text': 'pixels, and non zero for others.', 'confidence': 0.9806617498397827, 'text_region': [[615.0, 757.0], [863.0, 757.0], [863.0, 780.0], [615.0, 780.0]]}, {'text': 'Rendering Text in 3D World: We first perform triangula-', 'confidence': 0.982795000076294, 'text_region': [[615.0, 780.0], [1089.0, 780.0], [1089.0, 804.0], [615.0, 804.0]]}, {'text': 'tion for the refined proposals to generate planar triangular', 'confidence': 0.9903717041015625, 'text_region': [[615.0, 805.0], [1093.0, 805.0], [1093.0, 828.0], [615.0, 828.0]]}, {'text': 'meshes that are closely attached to the underlying surface.', 'confidence': 0.9985024929046631, 'text_region': [[617.0, 828.0], [1089.0, 828.0], [1089.0, 851.0], [617.0, 851.0]]}, {'text': 'Then we load the text images as texture onto the generated', 'confidence': 0.9857916831970215, 'text_region': [[617.0, 853.0], [1093.0, 853.0], [1093.0, 876.0], [617.0, 876.0]]}, {'text': 'meshes. We also randomly sample the texture attributes,', 'confidence': 0.9734281897544861, 'text_region': [[615.0, 876.0], [1093.0, 876.0], [1093.0, 899.0], [615.0, 899.0]]}, {'text': 'such as the ratio of diffuse and specular reflection.', 'confidence': 0.9822595715522766, 'text_region': [[612.0, 896.0], [1021.0, 898.0], [1021.0, 926.0], [612.0, 924.0]]}], 'img_idx': 4, 'score': 0.9579854011535645}\n",
      "{'type': 'text', 'bbox': [619, 711, 1091, 774], 'res': [{'text': 'does. We also use the same text corpus, Newsgroup20. The', 'confidence': 0.9983749389648438, 'text_region': [[614.0, 705.0], [1094.0, 706.0], [1094.0, 734.0], [614.0, 733.0]]}, {'text': 'generated text images have zero alpha values on non-stroke', 'confidence': 0.9937488436698914, 'text_region': [[615.0, 733.0], [1093.0, 733.0], [1093.0, 756.0], [615.0, 756.0]]}, {'text': 'pixels, and non zero for others.', 'confidence': 0.9806617498397827, 'text_region': [[615.0, 757.0], [863.0, 757.0], [863.0, 780.0], [615.0, 780.0]]}], 'img_idx': 4, 'score': 0.8443024158477783}\n",
      "{'type': 'title', 'bbox': [616, 1259, 1020, 1317], 'res': [{'text': '4.Experiments on Scene Text Detection', 'confidence': 0.9791558980941772, 'text_region': [[614.0, 1257.0], [1021.0, 1257.0], [1021.0, 1280.0], [614.0, 1280.0]]}, {'text': '4.1. Settings', 'confidence': 0.9620243310928345, 'text_region': [[614.0, 1292.0], [734.0, 1297.0], [733.0, 1322.0], [613.0, 1316.0]]}], 'img_idx': 4, 'score': 0.9704607129096985}\n",
      "{'type': 'title', 'bbox': [99, 697, 436, 715], 'res': [{'text': '3.4.2Refining Proposals in 3D Worlds', 'confidence': 0.9720126390457153, 'text_region': [[96.0, 695.0], [437.0, 695.0], [437.0, 718.0], [96.0, 718.0]]}], 'img_idx': 4, 'score': 0.961539626121521}\n",
      "{'type': 'title', 'bbox': [99, 321, 469, 340], 'res': [{'text': '3.4.1Initial Proposals from Normal Maps', 'confidence': 0.9660478234291077, 'text_region': [[97.0, 317.0], [471.0, 320.0], [471.0, 343.0], [96.0, 340.0]]}], 'img_idx': 4, 'score': 0.9582034945487976}\n",
      "{'type': 'title', 'bbox': [618, 940, 877, 957], 'res': [{'text': '3.6.ImplementationDetails', 'confidence': 0.9981504678726196, 'text_region': [[615.0, 937.0], [878.0, 937.0], [878.0, 960.0], [615.0, 960.0]]}], 'img_idx': 4, 'score': 0.9519665837287903}\n",
      "{'type': 'title', 'bbox': [100, 1130, 283, 1148], 'res': [{'text': '3.5. Text Rendering', 'confidence': 0.9904859662055969, 'text_region': [[95.0, 1123.0], [286.0, 1127.0], [286.0, 1155.0], [95.0, 1152.0]]}], 'img_idx': 4, 'score': 0.9500176310539246}\n",
      "{'type': 'figure', 'bbox': [611, 136, 1090, 384], 'res': [{'text': '(2)FrontView', 'confidence': 0.9432487487792969, 'text_region': [[805.0, 243.0], [885.0, 243.0], [885.0, 259.0], [805.0, 259.0]]}, {'text': '(3)CreateOrtho', 'confidence': 0.9503726959228516, 'text_region': [[923.0, 244.0], [1014.0, 244.0], [1014.0, 261.0], [923.0, 261.0]]}, {'text': '(4) Original View', 'confidence': 0.977798342704773, 'text_region': [[635.0, 366.0], [738.0, 366.0], [738.0, 383.0], [635.0, 383.0]]}, {'text': '(5) Refined Proposal', 'confidence': 0.9881289601325989, 'text_region': [[787.0, 366.0], [908.0, 366.0], [908.0, 383.0], [787.0, 383.0]]}, {'text': '(6) Embedded Text', 'confidence': 0.9362160563468933, 'text_region': [[946.0, 365.0], [1059.0, 363.0], [1059.0, 381.0], [946.0, 383.0]]}], 'img_idx': 4, 'score': 0.9578417539596558}\n",
      "{'type': 'reference', 'bbox': [99, 1371, 569, 1425], 'res': [{'text': \"'when the distancesfrom the rectangular proposals' cornersto the near-\", 'confidence': 0.9539401531219482, 'text_region': [[118.0, 1365.0], [575.0, 1366.0], [575.0, 1394.0], [118.0, 1393.0]]}, {'text': 'est point on the underlying surface mesh exceed certain threshold', 'confidence': 0.9786821603775024, 'text_region': [[98.0, 1389.0], [521.0, 1389.0], [521.0, 1411.0], [98.0, 1411.0]]}, {'text': 'https://fonts.google.com/', 'confidence': 0.9991648197174072, 'text_region': [[123.0, 1409.0], [368.0, 1409.0], [368.0, 1431.0], [123.0, 1431.0]]}], 'img_idx': 4, 'score': 0.8363333940505981}\n",
      "{'type': 'text', 'bbox': [99, 631, 572, 1389], 'res': [{'text': 'Pure Synthetic DataWe first train theEAST models on', 'confidence': 0.9599887728691101, 'text_region': [[98.0, 629.0], [575.0, 629.0], [575.0, 650.0], [98.0, 650.0]]}, {'text': 'different synthetic datasets alone, to compare our method', 'confidence': 0.9814927577972412, 'text_region': [[98.0, 653.0], [575.0, 653.0], [575.0, 676.0], [98.0, 676.0]]}, {'text': 'with previous ones in a direct and quantitative way. Note', 'confidence': 0.9957026243209839, 'text_region': [[95.0, 673.0], [577.0, 675.0], [577.0, 703.0], [95.0, 701.0]]}, {'text': 'that UnrealText,SynthText3D,SynthText,andVISD have', 'confidence': 0.9741716980934143, 'text_region': [[98.0, 701.0], [575.0, 701.0], [575.0, 723.0], [98.0, 723.0]]}, {'text': 'different numbers of images, so we also need to control the', 'confidence': 0.990774393081665, 'text_region': [[95.0, 721.0], [577.0, 723.0], [577.0, 751.0], [95.0, 749.0]]}, {'text': 'number of images used in experiments. Results are summa-', 'confidence': 0.9877049326896667, 'text_region': [[98.0, 749.0], [572.0, 749.0], [572.0, 772.0], [98.0, 772.0]]}, {'text': 'rized in Tab. 1.', 'confidence': 0.9500272274017334, 'text_region': [[98.0, 774.0], [220.0, 774.0], [220.0, 792.0], [98.0, 792.0]]}, {'text': 'Firstly, we control the total number of images to', 'confidence': 0.9863781929016113, 'text_region': [[121.0, 797.0], [575.0, 797.0], [575.0, 820.0], [121.0, 820.0]]}, {'text': '10K, which is also the full size of the smallest synthetic', 'confidence': 0.9851536154747009, 'text_region': [[97.0, 818.0], [575.0, 820.0], [575.0, 845.0], [96.0, 843.0]]}, {'text': 'datasets, VISD and SynthText3D. We observe a consider-', 'confidence': 0.9684958457946777, 'text_region': [[98.0, 845.0], [574.0, 845.0], [574.0, 868.0], [98.0, 868.0]]}, {'text': 'able improvement on IC15 over previous state-of-the-art by', 'confidence': 0.9954438805580139, 'text_region': [[96.0, 865.0], [574.0, 865.0], [574.0, 893.0], [96.0, 893.0]]}, {'text': '+0.9% in F1-score, and significant improvements on IC13', 'confidence': 0.9930464029312134, 'text_region': [[98.0, 889.0], [575.0, 889.0], [575.0, 917.0], [98.0, 917.0]]}, {'text': '(+2.7%) and MLT 2017 (+2.8%). Secondly,we also train', 'confidence': 0.9532361626625061, 'text_region': [[97.0, 914.0], [575.0, 916.0], [575.0, 939.0], [96.0, 937.0]]}, {'text': 'models on the full set of SynthText and ours, since scalabil-', 'confidence': 0.9595727324485779, 'text_region': [[96.0, 941.0], [575.0, 937.0], [575.0, 960.0], [97.0, 964.0]]}, {'text': 'ity is also an important factor for synthetic scene text im-', 'confidence': 0.9891089797019958, 'text_region': [[98.0, 964.0], [574.0, 964.0], [574.0, 987.0], [98.0, 987.0]]}, {'text': 'ages, especially when considering the demand to train rec-', 'confidence': 0.9921072125434875, 'text_region': [[95.0, 987.0], [574.0, 985.0], [574.0, 1013.0], [95.0, 1015.0]]}, {'text': ' ognizers. Extra training images further improve F1 scores', 'confidence': 0.9684869050979614, 'text_region': [[95.0, 1010.0], [577.0, 1008.0], [577.0, 1036.0], [95.0, 1038.0]]}, {'text': 'on IC15,IC13, and MLT by +2.6%,+2.3%,and +2.1%.', 'confidence': 0.9839942455291748, 'text_region': [[98.0, 1035.0], [572.0, 1035.0], [572.0, 1058.0], [98.0, 1058.0]]}, {'text': 'Models trained with our UnrealText data outperform all', 'confidence': 0.9768742322921753, 'text_region': [[98.0, 1059.0], [577.0, 1059.0], [577.0, 1082.0], [98.0, 1082.0]]}, {'text': 'other synthetic datasets. Besides, the subset of 10K images', 'confidence': 0.9844635725021362, 'text_region': [[98.0, 1084.0], [575.0, 1084.0], [575.0, 1107.0], [98.0, 1107.0]]}, {'text': 'with our method even surpasses 800K SynthText images', 'confidence': 0.9909597039222717, 'text_region': [[98.0, 1106.0], [577.0, 1106.0], [577.0, 1134.0], [98.0, 1134.0]]}, {'text': 'significantly on all datasets. The experiment results demon-', 'confidence': 0.989178478717804, 'text_region': [[98.0, 1132.0], [572.0, 1132.0], [572.0, 1155.0], [98.0, 1155.0]]}, {'text': 'strate the effectiveness of our proposed synthetic engine and', 'confidence': 0.9901936650276184, 'text_region': [[98.0, 1155.0], [575.0, 1155.0], [575.0, 1178.0], [98.0, 1178.0]]}, {'text': 'datasets.', 'confidence': 0.9995613098144531, 'text_region': [[98.0, 1178.0], [170.0, 1178.0], [170.0, 1201.0], [98.0, 1201.0]]}, {'text': 'Complementary Synthetic Data One unique characteristic', 'confidence': 0.9842231869697571, 'text_region': [[98.0, 1203.0], [575.0, 1203.0], [575.0, 1226.0], [98.0, 1226.0]]}, {'text': 'of the proposed UnrealText is that, the images are generated', 'confidence': 0.9889166951179504, 'text_region': [[98.0, 1228.0], [575.0, 1228.0], [575.0, 1251.0], [98.0, 1251.0]]}, {'text': 'from 3D scene models, instead of real background images,', 'confidence': 0.980171799659729, 'text_region': [[98.0, 1251.0], [574.0, 1251.0], [574.0, 1274.0], [98.0, 1274.0]]}, {'text': 'resulting in potential domain gap due to different artistic', 'confidence': 0.9756669998168945, 'text_region': [[98.0, 1275.0], [575.0, 1275.0], [575.0, 1299.0], [98.0, 1299.0]]}, {'text': 'styles. We conduct experiments by training on both Unre-', 'confidence': 0.9829251170158386, 'text_region': [[98.0, 1299.0], [574.0, 1299.0], [574.0, 1322.0], [98.0, 1322.0]]}, {'text': 'alText data (5K) and VISD (5K), as also shown in Tab.1', 'confidence': 0.9681724905967712, 'text_region': [[98.0, 1323.0], [574.0, 1323.0], [574.0, 1345.0], [98.0, 1345.0]]}, {'text': '(last row, marked with italics), which achieves better perfor-', 'confidence': 0.9567700028419495, 'text_region': [[98.0, 1346.0], [574.0, 1346.0], [574.0, 1370.0], [98.0, 1370.0]]}, {'text': 'mance than other 10K synthetic datasets. The combination', 'confidence': 0.9798917770385742, 'text_region': [[98.0, 1371.0], [575.0, 1371.0], [575.0, 1394.0], [98.0, 1394.0]]}], 'img_idx': 5, 'score': 0.9959689378738403}\n",
      "{'type': 'text', 'bbox': [98, 150, 573, 571], 'res': [{'text': 'accurate.EAST also forms the basis of several widely rec-', 'confidence': 0.9868921041488647, 'text_region': [[98.0, 148.0], [574.0, 148.0], [574.0, 170.0], [98.0, 170.0]]}, {'text': 'ognized end-to-end text spotting models [18, 7]. We adopt', 'confidence': 0.9908203482627869, 'text_region': [[98.0, 172.0], [577.0, 172.0], [577.0, 195.0], [98.0, 195.0]]}, {'text': 'an opensource implementation?. In all experiments, models', 'confidence': 0.9840661883354187, 'text_region': [[98.0, 196.0], [575.0, 196.0], [575.0, 219.0], [98.0, 219.0]]}, {'text': 'are trained on 4 GPU with a batch size of 56. During the', 'confidence': 0.9802865386009216, 'text_region': [[97.0, 218.0], [575.0, 219.0], [575.0, 243.0], [96.0, 241.0]]}, {'text': 'evaluation, the test images are resized to match a short side', 'confidence': 0.9840513467788696, 'text_region': [[98.0, 244.0], [575.0, 244.0], [575.0, 267.0], [98.0, 267.0]]}, {'text': 'length of 800 pixels. For each experiment setting, we report', 'confidence': 0.9833042025566101, 'text_region': [[98.0, 267.0], [577.0, 267.0], [577.0, 290.0], [98.0, 290.0]]}, {'text': ' the mean performance in 5 independent trials.', 'confidence': 0.9782834053039551, 'text_region': [[95.0, 289.0], [467.0, 287.0], [467.0, 315.0], [95.0, 317.0]]}, {'text': 'Benchmark Datasets We use the following scene text', 'confidence': 0.9869515895843506, 'text_region': [[97.0, 313.0], [577.0, 315.0], [577.0, 338.0], [96.0, 337.0]]}, {'text': 'detection datasets for evaluation:(1）ICDAR 2013 Fo-', 'confidence': 0.9619598388671875, 'text_region': [[98.0, 338.0], [575.0, 338.0], [575.0, 360.0], [98.0, 360.0]]}, {'text': 'cused Scene Text (IC13) [14] containing horizontal text with', 'confidence': 0.9711658358573914, 'text_region': [[98.0, 363.0], [575.0, 363.0], [575.0, 386.0], [98.0, 386.0]]}, {'text': 'zoomed-in views.(2)ICDAR 2015Incidental Scene Text', 'confidence': 0.955193042755127, 'text_region': [[100.0, 386.0], [575.0, 386.0], [575.0, 409.0], [100.0, 409.0]]}, {'text': '(IC15) [13] consisting of images taken without carefulness', 'confidence': 0.9877541661262512, 'text_region': [[98.0, 411.0], [574.0, 411.0], [574.0, 434.0], [98.0, 434.0]]}, {'text': 'with Google Glass. Images are blurred and text are small.', 'confidence': 0.9847410917282104, 'text_region': [[100.0, 436.0], [574.0, 436.0], [574.0, 459.0], [100.0, 459.0]]}, {'text': '(3) MLT 2017 [27] for multilingual scene text detection,', 'confidence': 0.9615322351455688, 'text_region': [[100.0, 459.0], [574.0, 459.0], [574.0, 482.0], [100.0, 482.0]]}, {'text': 'which is composed of scene text images of 9 languages.', 'confidence': 0.9866061806678772, 'text_region': [[96.0, 480.0], [574.0, 480.0], [574.0, 508.0], [96.0, 508.0]]}, {'text': 'Note that the images in IC13 and MLT17 have varying res-', 'confidence': 0.998353123664856, 'text_region': [[98.0, 507.0], [574.0, 507.0], [574.0, 530.0], [98.0, 530.0]]}, {'text': 'olutions. Therefore, it is necessary to resize them to the', 'confidence': 0.9894837737083435, 'text_region': [[98.0, 530.0], [575.0, 530.0], [575.0, 553.0], [98.0, 553.0]]}, {'text': 'same level of resolutions before evaluation.', 'confidence': 0.9830002188682556, 'text_region': [[98.0, 554.0], [447.0, 554.0], [447.0, 576.0], [98.0, 576.0]]}], 'img_idx': 5, 'score': 0.9951693415641785}\n",
      "{'type': 'text', 'bbox': [616, 475, 1090, 1381], 'res': [{'text': 'of UnrealText and VISD is also superior to the combina-', 'confidence': 0.9939177632331848, 'text_region': [[614.0, 469.0], [1091.0, 472.0], [1091.0, 498.0], [614.0, 495.0]]}, {'text': 'tion of SynthText3D and VISD.This result demonstrates', 'confidence': 0.9689182043075562, 'text_region': [[615.0, 497.0], [1091.0, 497.0], [1091.0, 518.0], [615.0, 518.0]]}, {'text': 'that, our UnrealText is complementary to existing syn-', 'confidence': 0.9861821532249451, 'text_region': [[615.0, 521.0], [1091.0, 521.0], [1091.0, 544.0], [615.0, 544.0]]}, {'text': 'thetic datasets that use real images as backgrounds. While', 'confidence': 0.9971824884414673, 'text_region': [[615.0, 544.0], [1091.0, 544.0], [1091.0, 568.0], [615.0, 568.0]]}, {'text': 'UnrealText simulates photo-realistic effects, synthetic data', 'confidence': 0.9890820384025574, 'text_region': [[617.0, 569.0], [1093.0, 569.0], [1093.0, 592.0], [617.0, 592.0]]}, {'text': 'with real background images can help adapt to real-world', 'confidence': 0.9838728308677673, 'text_region': [[615.0, 591.0], [1094.0, 591.0], [1094.0, 619.0], [615.0, 619.0]]}, {'text': 'datasets.', 'confidence': 0.9996199011802673, 'text_region': [[615.0, 615.0], [687.0, 615.0], [687.0, 639.0], [615.0, 639.0]]}, {'text': 'Combining Synthetic and Real Data One important role', 'confidence': 0.9685952663421631, 'text_region': [[615.0, 642.0], [1093.0, 642.0], [1093.0, 665.0], [615.0, 665.0]]}, {'text': 'of synthetic data is to serve as data for pretraining, and to', 'confidence': 0.9916229248046875, 'text_region': [[615.0, 667.0], [1093.0, 667.0], [1093.0, 690.0], [615.0, 690.0]]}, {'text': 'further improve the performance on domain specific real', 'confidence': 0.9780886769294739, 'text_region': [[615.0, 690.0], [1093.0, 690.0], [1093.0, 713.0], [615.0, 713.0]]}, {'text': 'datasets. We first pretrain the EAST models with differ-', 'confidence': 0.981500506401062, 'text_region': [[614.0, 711.0], [1093.0, 711.0], [1093.0, 739.0], [614.0, 739.0]]}, {'text': 'ent synthetic data, and then use domain data to finetune the', 'confidence': 0.9931058883666992, 'text_region': [[617.0, 738.0], [1093.0, 738.0], [1093.0, 761.0], [617.0, 761.0]]}, {'text': 'models. The results are summarized in Tab. 2.', 'confidence': 0.9730479121208191, 'text_region': [[614.0, 762.0], [1041.0, 762.0], [1041.0, 784.0], [614.0, 784.0]]}, {'text': 'On all', 'confidence': 0.9644830226898193, 'text_region': [[1034.0, 764.0], [1093.0, 764.0], [1093.0, 782.0], [1034.0, 782.0]]}, {'text': 'domain-specific datasets, models pretrained with our syn-', 'confidence': 0.9971115589141846, 'text_region': [[614.0, 785.0], [1089.0, 785.0], [1089.0, 809.0], [614.0, 809.0]]}, {'text': 'thetic dataset surpasses others by considerable margins, ver-', 'confidence': 0.99156653881073, 'text_region': [[615.0, 810.0], [1091.0, 810.0], [1091.0, 833.0], [615.0, 833.0]]}, {'text': 'ifying the effectiveness of our synthesis method in the con-', 'confidence': 0.9745980501174927, 'text_region': [[614.0, 832.0], [1091.0, 835.0], [1091.0, 858.0], [614.0, 855.0]]}, {'text': 'text of boosting performance on domain specific datasets.', 'confidence': 0.9824920892715454, 'text_region': [[615.0, 858.0], [1079.0, 858.0], [1079.0, 881.0], [615.0, 881.0]]}, {'text': 'Pretraining on Full Dataset As shown in the last rows', 'confidence': 0.9865476489067078, 'text_region': [[615.0, 884.0], [1093.0, 884.0], [1093.0, 908.0], [615.0, 908.0]]}, {'text': 'of Tab. 2, when we pretrain the detector models with our', 'confidence': 0.9834380745887756, 'text_region': [[615.0, 908.0], [1091.0, 908.0], [1091.0, 931.0], [615.0, 931.0]]}, {'text': 'full dataset, the performances are improved significantly,', 'confidence': 0.9876524806022644, 'text_region': [[615.0, 932.0], [1091.0, 932.0], [1091.0, 955.0], [615.0, 955.0]]}, {'text': 'demonstrating the advantage of the scalability of our en-', 'confidence': 0.9711899161338806, 'text_region': [[614.0, 954.0], [1089.0, 954.0], [1089.0, 977.0], [614.0, 977.0]]}, {'text': 'gine. Especially, The EAST model achieves an F1 score', 'confidence': 0.9903736710548401, 'text_region': [[615.0, 980.0], [1093.0, 980.0], [1093.0, 1003.0], [615.0, 1003.0]]}, {'text': 'of 74.1 on MLT17, which is even better than recent state-', 'confidence': 0.9899063110351562, 'text_region': [[615.0, 1003.0], [1091.0, 1003.0], [1091.0, 1026.0], [615.0, 1026.0]]}, {'text': 'of-the-art results, including 73.9 by CRAFT[2] and 73.1 by', 'confidence': 0.9959205389022827, 'text_region': [[617.0, 1028.0], [1091.0, 1028.0], [1091.0, 1051.0], [617.0, 1051.0]]}, {'text': 'LOMO [52]. Although the margin is not great, it suffices', 'confidence': 0.972446084022522, 'text_region': [[615.0, 1051.0], [1091.0, 1051.0], [1091.0, 1074.0], [615.0, 1074.0]]}, {'text': 'to claim that the EAST model revives and reclaims state-of-', 'confidence': 0.9783868193626404, 'text_region': [[615.0, 1074.0], [1091.0, 1074.0], [1091.0, 1097.0], [615.0, 1097.0]]}, {'text': 'the-art performance with the help of our synthetic dataset.', 'confidence': 0.9889162182807922, 'text_region': [[615.0, 1099.0], [1081.0, 1099.0], [1081.0, 1122.0], [615.0, 1122.0]]}, {'text': 'Results with Mask-RCNN As the EAST algorithm we use', 'confidence': 0.9824994206428528, 'text_region': [[615.0, 1125.0], [1093.0, 1125.0], [1093.0, 1148.0], [615.0, 1148.0]]}, {'text': 'above is specifically designed for scene text and that the', 'confidence': 0.9886384010314941, 'text_region': [[615.0, 1148.0], [1093.0, 1148.0], [1093.0, 1172.0], [615.0, 1172.0]]}, {'text': 'evaluation with F1 scores may not be comprehensive,we', 'confidence': 0.9780717492103577, 'text_region': [[615.0, 1173.0], [1093.0, 1173.0], [1093.0, 1196.0], [615.0, 1196.0]]}, {'text': 'provide results with Mask-RCNN [?] which is a general', 'confidence': 0.9915873408317566, 'text_region': [[615.0, 1196.0], [1093.0, 1196.0], [1093.0, 1219.0], [615.0, 1219.0]]}, {'text': 'object detector.We evaluate the models using the Average', 'confidence': 0.98267662525177, 'text_region': [[614.0, 1219.0], [1093.0, 1221.0], [1093.0, 1244.0], [614.0, 1242.0]]}, {'text': 'Precision (AP) metrics which are more comprehensive and', 'confidence': 0.9859904646873474, 'text_region': [[615.0, 1244.0], [1093.0, 1244.0], [1093.0, 1267.0], [615.0, 1267.0]]}, {'text': 'less affected by the tricky choice of threshold values. We', 'confidence': 0.9907530546188354, 'text_region': [[615.0, 1269.0], [1093.0, 1269.0], [1093.0, 1292.0], [615.0, 1292.0]]}, {'text': 'use the opensource implementation Detectron2 4. The ro-', 'confidence': 0.9942906498908997, 'text_region': [[614.0, 1290.0], [1093.0, 1289.0], [1093.0, 1317.0], [614.0, 1318.0]]}, {'text': 'tated bounding boxes of text instances are used as the mask', 'confidence': 0.990068256855011, 'text_region': [[617.0, 1317.0], [1093.0, 1317.0], [1093.0, 1340.0], [617.0, 1340.0]]}, {'text': 'annotations. We select a default Mask-RCNN configuration', 'confidence': 0.991911768913269, 'text_region': [[615.0, 1341.0], [1091.0, 1341.0], [1091.0, 1365.0], [615.0, 1365.0]]}, {'text': 'with ResNet-50+FPN as the backbone and train the model', 'confidence': 0.996753990650177, 'text_region': [[617.0, 1365.0], [1093.0, 1365.0], [1093.0, 1388.0], [617.0, 1388.0]]}], 'img_idx': 5, 'score': 0.9937582612037659}\n",
      "{'type': 'text', 'bbox': [617, 378, 1090, 421], 'res': [{'text': 'Table 1: Detection results (F1-scores) of EAST models', 'confidence': 0.976190447807312, 'text_region': [[615.0, 376.0], [1094.0, 376.0], [1094.0, 399.0], [615.0, 399.0]]}, {'text': 'trained on different synthetic data.', 'confidence': 0.9992225170135498, 'text_region': [[615.0, 401.0], [893.0, 401.0], [893.0, 424.0], [615.0, 424.0]]}], 'img_idx': 5, 'score': 0.8598160743713379}\n",
      "{'type': 'title', 'bbox': [100, 594, 332, 614], 'res': [{'text': '4.2.Experiments Results', 'confidence': 0.99400794506073, 'text_region': [[98.0, 592.0], [333.0, 592.0], [333.0, 615.0], [98.0, 615.0]]}], 'img_idx': 5, 'score': 0.9602776169776917}\n",
      "{'type': 'table', 'bbox': [623, 138, 1107, 370], 'res': '', 'img_idx': 5, 'score': 0.9718571901321411}\n",
      "{'type': 'reference', 'bbox': [126, 1409, 410, 1426], 'res': [{'text': '3https://github.com/argman/EAST', 'confidence': 0.9987757205963135, 'text_region': [[120.0, 1404.0], [417.0, 1406.0], [417.0, 1429.0], [120.0, 1427.0]]}], 'img_idx': 5, 'score': 0.876189649105072}\n",
      "{'type': 'reference', 'bbox': [630, 1409, 1083, 1427], 'res': [{'text': '+https://github.com/facebookresearch/detectron2', 'confidence': 0.9900050163269043, 'text_region': [[639.0, 1407.0], [1088.0, 1407.0], [1088.0, 1431.0], [639.0, 1431.0]]}], 'img_idx': 5, 'score': 0.6566647887229919}\n",
      "{'type': 'text', 'bbox': [617, 148, 1090, 1424], 'res': [{'text': 'ule and the environment randomization module in increas-', 'confidence': 0.9632007479667664, 'text_region': [[615.0, 148.0], [1091.0, 148.0], [1091.0, 170.0], [615.0, 170.0]]}, {'text': 'ing the diversity of synthetic images.', 'confidence': 0.9866830706596375, 'text_region': [[614.0, 172.0], [910.0, 172.0], [910.0, 195.0], [614.0, 195.0]]}, {'text': 'Ablating Viewfinder Module We derive two baselines', 'confidence': 0.9797889590263367, 'text_region': [[615.0, 196.0], [1094.0, 196.0], [1094.0, 224.0], [615.0, 224.0]]}, {'text': 'from the proposed viewfinder module: (1) Random View-', 'confidence': 0.9768537282943726, 'text_region': [[615.0, 223.0], [1091.0, 223.0], [1091.0, 246.0], [615.0, 246.0]]}, {'text': 'point + Manual Anchor that randomly samples camera lo-', 'confidence': 0.9668562412261963, 'text_region': [[615.0, 246.0], [1091.0, 246.0], [1091.0, 269.0], [615.0, 269.0]]}, {'text': 'cations and rotations from the norm-ball spaces centered', 'confidence': 0.9933801293373108, 'text_region': [[615.0, 271.0], [1093.0, 271.0], [1093.0, 294.0], [615.0, 294.0]]}, {'text': 'around auxiliary camera anchors.(2) Random Viewpoint', 'confidence': 0.9668127298355103, 'text_region': [[614.0, 294.0], [1091.0, 294.0], [1091.0, 317.0], [614.0, 317.0]]}, {'text': 'Only that randomly samples camera locations and rotations', 'confidence': 0.9918391704559326, 'text_region': [[614.0, 317.0], [1093.0, 318.0], [1093.0, 342.0], [614.0, 340.0]]}, {'text': 'from the whole scene space, without checking their qual-', 'confidence': 0.9741240739822388, 'text_region': [[617.0, 343.0], [1091.0, 343.0], [1091.0, 366.0], [617.0, 366.0]]}, {'text': 'ity. For experiments, we fix the number of scenes to 10 to', 'confidence': 0.9788930416107178, 'text_region': [[617.0, 366.0], [1093.0, 366.0], [1093.0, 389.0], [617.0, 389.0]]}, {'text': 'control scene diversity and generate different numbers of', 'confidence': 0.9816933870315552, 'text_region': [[617.0, 391.0], [1094.0, 391.0], [1094.0, 414.0], [617.0, 414.0]]}, {'text': 'images, and compare their performance curve.By fixing', 'confidence': 0.9821376204490662, 'text_region': [[614.0, 412.0], [1093.0, 416.0], [1093.0, 439.0], [614.0, 436.0]]}, {'text': 'the number of scenes, we compare how well different view', 'confidence': 0.977297842502594, 'text_region': [[615.0, 439.0], [1093.0, 439.0], [1093.0, 462.0], [615.0, 462.0]]}, {'text': 'finding methods can exploit the scenes.', 'confidence': 0.9805281162261963, 'text_region': [[615.0, 462.0], [931.0, 462.0], [931.0, 485.0], [615.0, 485.0]]}, {'text': 'Ablating Environment RandomizationWeremove the', 'confidence': 0.9738298654556274, 'text_region': [[617.0, 490.0], [1093.0, 490.0], [1093.0, 513.0], [617.0, 513.0]]}, {'text': 'environment randomization module, and keep the scene', 'confidence': 0.9871034622192383, 'text_region': [[617.0, 513.0], [1093.0, 513.0], [1093.0, 536.0], [617.0, 536.0]]}, {'text': 'models unchanged during synthesis. For experiments, we', 'confidence': 0.991531252861023, 'text_region': [[615.0, 538.0], [1093.0, 538.0], [1093.0, 561.0], [615.0, 561.0]]}, {'text': 'fix the total number of images to 10K and use different', 'confidence': 0.9942877888679504, 'text_region': [[614.0, 559.0], [1096.0, 559.0], [1096.0, 587.0], [614.0, 587.0]]}, {'text': 'number of scenes. In this way, we can compare the diversity', 'confidence': 0.9883796572685242, 'text_region': [[615.0, 586.0], [1093.0, 586.0], [1093.0, 609.0], [615.0, 609.0]]}, {'text': 'of images generated with different methods.', 'confidence': 0.9963371157646179, 'text_region': [[615.0, 610.0], [970.0, 610.0], [970.0, 634.0], [615.0, 634.0]]}, {'text': 'We train theEAST modelswith different numbers ofim-', 'confidence': 0.9637536406517029, 'text_region': [[639.0, 637.0], [1089.0, 637.0], [1089.0, 658.0], [639.0, 658.0]]}, {'text': 'ages or scenes, evaluate them on the 3 real datasets, and', 'confidence': 0.9632732272148132, 'text_region': [[614.0, 662.0], [1094.0, 660.0], [1094.0, 683.0], [614.0, 685.0]]}, {'text': 'compute the arithmetic mean of the F1-scores. As shown', 'confidence': 0.9933381676673889, 'text_region': [[615.0, 685.0], [1093.0, 685.0], [1093.0, 708.0], [615.0, 708.0]]}, {'text': 'in Fig. 5 (a), we observe that the proposed combination,', 'confidence': 0.9836013913154602, 'text_region': [[615.0, 710.0], [1091.0, 710.0], [1091.0, 733.0], [615.0, 733.0]]}, {'text': 'i.e.RandomWalk +Manual Anchor,achieves significantly', 'confidence': 0.979521632194519, 'text_region': [[615.0, 733.0], [1091.0, 733.0], [1091.0, 756.0], [615.0, 756.0]]}, {'text': 'higher F1-scores consistently for different numbers of im-', 'confidence': 0.9775602221488953, 'text_region': [[615.0, 757.0], [1091.0, 757.0], [1091.0, 779.0], [615.0, 779.0]]}, {'text': 'ages. Especially, larger sizes of training sets result in greater', 'confidence': 0.9867222905158997, 'text_region': [[614.0, 782.0], [1093.0, 780.0], [1093.0, 804.0], [614.0, 805.0]]}, {'text': 'performance gaps.', 'confidence': 0.958559513092041, 'text_region': [[614.0, 804.0], [785.0, 804.0], [785.0, 832.0], [614.0, 832.0]]}, {'text': 'We also inspect the images generated', 'confidence': 0.9732241034507751, 'text_region': [[773.0, 805.0], [1093.0, 805.0], [1093.0, 828.0], [773.0, 828.0]]}, {'text': 'with these methods respectively.', 'confidence': 0.9893754720687866, 'text_region': [[617.0, 828.0], [888.0, 828.0], [888.0, 851.0], [617.0, 851.0]]}, {'text': 'When starting from the', 'confidence': 0.9823290109634399, 'text_region': [[893.0, 828.0], [1093.0, 828.0], [1093.0, 851.0], [893.0, 851.0]]}, {'text': 'same anchor point, the proposed random walk can gener-', 'confidence': 0.9864489436149597, 'text_region': [[614.0, 850.0], [1093.0, 850.0], [1093.0, 878.0], [614.0, 878.0]]}, {'text': 'ate more diverse viewpoints and can traverse much larger', 'confidence': 0.9805541634559631, 'text_region': [[615.0, 876.0], [1093.0, 876.0], [1093.0, 899.0], [615.0, 899.0]]}, {'text': 'area.In contrast, the Random Viewpoint + Manual An-', 'confidence': 0.975347101688385, 'text_region': [[615.0, 899.0], [1093.0, 899.0], [1093.0, 922.0], [615.0, 922.0]]}, {'text': 'chor method degenerates either into random rotation only', 'confidence': 0.9757218956947327, 'text_region': [[615.0, 924.0], [1091.0, 924.0], [1091.0, 947.0], [615.0, 947.0]]}, {'text': 'when we set a small norm ball size for random location, or', 'confidence': 0.986621081829071, 'text_region': [[617.0, 947.0], [1093.0, 947.0], [1093.0, 970.0], [617.0, 970.0]]}, {'text': 'intoRandomViewpoint Only whenweset a large norm ball', 'confidence': 0.9755445122718811, 'text_region': [[615.0, 972.0], [1093.0, 972.0], [1093.0, 995.0], [615.0, 995.0]]}, {'text': 'size.As a result,the Random Viewpoint +Manual Anchor', 'confidence': 0.9705953001976013, 'text_region': [[615.0, 995.0], [1093.0, 995.0], [1093.0, 1018.0], [615.0, 1018.0]]}, {'text': 'method requires careful manual selection of anchors, and', 'confidence': 0.9842027425765991, 'text_region': [[615.0, 1020.0], [1093.0, 1020.0], [1093.0, 1043.0], [615.0, 1043.0]]}, {'text': 'we also need to manually tune the norm ball sizes for dif-', 'confidence': 0.9814894199371338, 'text_region': [[617.0, 1044.0], [1093.0, 1044.0], [1093.0, 1066.0], [617.0, 1066.0]]}, {'text': 'ferent scenes, which restricts the scalability of the synthe-', 'confidence': 0.9463653564453125, 'text_region': [[617.0, 1068.0], [1091.0, 1068.0], [1091.0, 1091.0], [617.0, 1091.0]]}, {'text': 'sis engine. Meanwhile, our proposed random walk based', 'confidence': 0.9836345314979553, 'text_region': [[615.0, 1091.0], [1093.0, 1091.0], [1093.0, 1114.0], [615.0, 1114.0]]}, {'text': 'method is more flexible and robust to the selection of man-', 'confidence': 0.973418116569519, 'text_region': [[615.0, 1115.0], [1091.0, 1115.0], [1091.0, 1137.0], [615.0, 1137.0]]}, {'text': 'ual anchors. As for the Random Viewpoint Only method,', 'confidence': 0.9894477128982544, 'text_region': [[615.0, 1138.0], [1091.0, 1138.0], [1091.0, 1162.0], [615.0, 1162.0]]}, {'text': 'a large proportion of generated viewpoints are invalid, e.g.', 'confidence': 0.991032600402832, 'text_region': [[612.0, 1160.0], [1093.0, 1162.0], [1093.0, 1190.0], [612.0, 1188.0]]}, {'text': 'inside other object meshes, which is out-of-distribution for', 'confidence': 0.9828511476516724, 'text_region': [[615.0, 1186.0], [1093.0, 1186.0], [1093.0, 1209.0], [615.0, 1209.0]]}, {'text': 'real images. This explains why it results in the worst per-', 'confidence': 0.9897196888923645, 'text_region': [[614.0, 1209.0], [1089.0, 1209.0], [1089.0, 1233.0], [614.0, 1233.0]]}, {'text': 'formances.', 'confidence': 0.9992116093635559, 'text_region': [[614.0, 1232.0], [707.0, 1236.0], [706.0, 1259.0], [613.0, 1255.0]]}, {'text': 'From Fig. 5 (b), the major observation is that environ-', 'confidence': 0.9920642971992493, 'text_region': [[639.0, 1262.0], [1091.0, 1262.0], [1091.0, 1285.0], [639.0, 1285.0]]}, {'text': 'ment randomization module improves performances over', 'confidence': 0.9813951849937439, 'text_region': [[615.0, 1285.0], [1093.0, 1285.0], [1093.0, 1308.0], [615.0, 1308.0]]}, {'text': 'different scene numbers consistently. Besides,the improve-', 'confidence': 0.9859135150909424, 'text_region': [[615.0, 1310.0], [1089.0, 1310.0], [1089.0, 1333.0], [615.0, 1333.0]]}, {'text': 'ment is more significant as we use fewer scenes. Therefore,', 'confidence': 0.9837381839752197, 'text_region': [[615.0, 1335.0], [1091.0, 1335.0], [1091.0, 1358.0], [615.0, 1358.0]]}, {'text': 'we can draw a conclusion that, the environment random-', 'confidence': 0.9857609868049622, 'text_region': [[615.0, 1358.0], [1091.0, 1358.0], [1091.0, 1381.0], [615.0, 1381.0]]}, {'text': 'ization helps increase image diversity and at the same time,', 'confidence': 0.9671211242675781, 'text_region': [[615.0, 1383.0], [1091.0, 1383.0], [1091.0, 1404.0], [615.0, 1404.0]]}, {'text': 'can reduce the number of scenes needed.Furthermore, the', 'confidence': 0.9710474014282227, 'text_region': [[614.0, 1404.0], [1093.0, 1406.0], [1093.0, 1429.0], [614.0, 1427.0]]}], 'img_idx': 6, 'score': 0.9936621189117432}\n",
      "{'type': 'text', 'bbox': [98, 833, 573, 1040], 'res': [{'text': 'for 1x schedule long.A', 'confidence': 0.9297643303871155, 'text_region': [[97.0, 828.0], [298.0, 832.0], [297.0, 855.0], [96.0, 851.0]]}, {'text': 'All the hyperparameters are set to', 'confidence': 0.9878150820732117, 'text_region': [[288.0, 832.0], [575.0, 832.0], [575.0, 855.0], [288.0, 855.0]]}, {'text': 'default values. The results are summarized in Tab. 3. We', 'confidence': 0.9933575391769409, 'text_region': [[98.0, 853.0], [575.0, 853.0], [575.0, 876.0], [98.0, 876.0]]}, {'text': 'notice that the two synthetic datasets with natural images', 'confidence': 0.9921959638595581, 'text_region': [[97.0, 878.0], [575.0, 879.0], [575.0, 903.0], [96.0, 901.0]]}, {'text': 'as backgrounds, i.e. SynthText and VISD, result in similar', 'confidence': 0.9613441228866577, 'text_region': [[98.0, 903.0], [574.0, 903.0], [574.0, 926.0], [98.0, 926.0]]}, {'text': 'performances. SynthText3D and our UnrealText are signif-', 'confidence': 0.9837767481803894, 'text_region': [[98.0, 926.0], [575.0, 926.0], [575.0, 949.0], [98.0, 949.0]]}, {'text': 'icantly better than them. UnrealText is further a significant', 'confidence': 0.9890398979187012, 'text_region': [[98.0, 950.0], [577.0, 950.0], [577.0, 973.0], [98.0, 973.0]]}, {'text': 'improvement over SynthText3D. When we combine Unre-', 'confidence': 0.9771531820297241, 'text_region': [[98.0, 973.0], [574.0, 973.0], [574.0, 997.0], [98.0, 997.0]]}, {'text': 'alText and SynthText, the two highly scalable engines, the', 'confidence': 0.9981221556663513, 'text_region': [[98.0, 998.0], [577.0, 998.0], [577.0, 1021.0], [98.0, 1021.0]]}, {'text': 'performances are even better.', 'confidence': 0.9604386687278748, 'text_region': [[98.0, 1021.0], [336.0, 1021.0], [336.0, 1044.0], [98.0, 1044.0]]}], 'img_idx': 6, 'score': 0.992391049861908}\n",
      "{'type': 'text', 'bbox': [99, 1359, 574, 1426], 'res': [{'text': 'One reasonable concern about synthesizing from 3D vir', 'confidence': 0.9961167573928833, 'text_region': [[123.0, 1358.0], [570.0, 1358.0], [570.0, 1381.0], [123.0, 1381.0]]}, {'text': 'tual scenes lies in the scene diversity.In this section, we', 'confidence': 0.969307005405426, 'text_region': [[98.0, 1383.0], [575.0, 1383.0], [575.0, 1404.0], [98.0, 1404.0]]}, {'text': 'address the importance of the proposed view finding mod-', 'confidence': 0.9956257939338684, 'text_region': [[98.0, 1406.0], [574.0, 1406.0], [574.0, 1429.0], [98.0, 1429.0]]}], 'img_idx': 6, 'score': 0.981358528137207}\n",
      "{'type': 'title', 'bbox': [98, 1322, 435, 1341], 'res': [{'text': '4.3.ModuleLevelAblationAnalysis', 'confidence': 0.9974321126937866, 'text_region': [[98.0, 1322.0], [439.0, 1322.0], [439.0, 1343.0], [98.0, 1343.0]]}], 'img_idx': 6, 'score': 0.9320409893989563}\n",
      "{'type': 'figure_caption', 'bbox': [113, 742, 562, 783], 'res': [{'text': 'Table 2:Detection performances of EAST models pre-', 'confidence': 0.9730668067932129, 'text_region': [[98.0, 739.0], [572.0, 739.0], [572.0, 762.0], [98.0, 762.0]]}, {'text': 'trained on synthetic and then finetuned on real datasets.', 'confidence': 0.974063515663147, 'text_region': [[98.0, 764.0], [542.0, 764.0], [542.0, 787.0], [98.0, 787.0]]}], 'img_idx': 6, 'score': 0.9122299551963806}\n",
      "{'type': 'table', 'bbox': [100, 1060, 573, 1222], 'res': '', 'img_idx': 6, 'score': 0.9526150226593018}\n",
      "{'type': 'table', 'bbox': [101, 158, 570, 738], 'res': '', 'img_idx': 6, 'score': 0.9335588216781616}\n",
      "{'type': 'table_caption', 'bbox': [220, 146, 448, 161], 'res': [{'text': 'Evaluation on ICDAR 2015', 'confidence': 0.9933980107307434, 'text_region': [[223.0, 144.0], [451.0, 144.0], [451.0, 167.0], [223.0, 167.0]]}], 'img_idx': 6, 'score': 0.8847468495368958}\n",
      "{'type': 'reference', 'bbox': [99, 1232, 571, 1274], 'res': [{'text': 'Table 3:Detection results (Box-AP/Mask-AP)of Mask-', 'confidence': 0.9740464091300964, 'text_region': [[100.0, 1231.0], [572.0, 1231.0], [572.0, 1252.0], [100.0, 1252.0]]}, {'text': 'RCNN models trained on different synthetic data.', 'confidence': 0.9655900001525879, 'text_region': [[97.0, 1252.0], [497.0, 1254.0], [497.0, 1277.0], [96.0, 1275.0]]}], 'img_idx': 6, 'score': 0.7373721599578857}\n",
      "{'type': 'text', 'bbox': [615, 857, 1090, 1365], 'res': [{'text': 'Although MLT 2017 has been widely used as a benchmark', 'confidence': 0.9739022850990295, 'text_region': [[619.0, 855.0], [1091.0, 855.0], [1091.0, 878.0], [619.0, 878.0]]}, {'text': 'for detection, the task of recognizing multilingual scene text', 'confidence': 0.9829868674278259, 'text_region': [[617.0, 879.0], [1093.0, 879.0], [1093.0, 903.0], [617.0, 903.0]]}, {'text': 'still remains largely untouched, mainly due to lack of a', 'confidence': 0.9852809906005859, 'text_region': [[615.0, 901.0], [1091.0, 901.0], [1091.0, 924.0], [615.0, 924.0]]}, {'text': 'proper training dataset. To pave the way for future research,', 'confidence': 0.9764809012413025, 'text_region': [[614.0, 927.0], [1091.0, 924.0], [1091.0, 947.0], [614.0, 950.0]]}, {'text': 'we also generate a multilingual version with 600K images', 'confidence': 0.9848029017448425, 'text_region': [[617.0, 949.0], [1093.0, 949.0], [1093.0, 972.0], [617.0, 972.0]]}, {'text': 'containing 10 languages as included in MLT 2019 [26]:', 'confidence': 0.995590329170227, 'text_region': [[615.0, 972.0], [1093.0, 972.0], [1093.0, 1000.0], [615.0, 1000.0]]}, {'text': 'Arabic,Bangla,Chinese,English,French,German,Hindi，', 'confidence': 0.9647841453552246, 'text_region': [[617.0, 998.0], [1091.0, 998.0], [1091.0, 1021.0], [617.0, 1021.0]]}, {'text': 'Italian,Japanese,and Korean.Text contents are sampled', 'confidence': 0.9834051132202148, 'text_region': [[614.0, 1020.0], [1091.0, 1020.0], [1091.0, 1043.0], [614.0, 1043.0]]}, {'text': 'from corpus extracted from the Wikimedia dump6.', 'confidence': 0.9805520176887512, 'text_region': [[615.0, 1046.0], [1021.0, 1046.0], [1021.0, 1069.0], [615.0, 1069.0]]}, {'text': 'Model We use the same model and implementation as Sec-', 'confidence': 0.9644287824630737, 'text_region': [[617.0, 1076.0], [1089.0, 1076.0], [1089.0, 1099.0], [617.0, 1099.0]]}, {'text': 'tion 5.1, except that the symbols to recognize are expanded', 'confidence': 0.9927077889442444, 'text_region': [[615.0, 1101.0], [1093.0, 1101.0], [1093.0, 1124.0], [615.0, 1124.0]]}, {'text': 'to all characters that appear in the generated dataset.', 'confidence': 0.9837115406990051, 'text_region': [[615.0, 1125.0], [1034.0, 1125.0], [1034.0, 1148.0], [615.0, 1148.0]]}, {'text': 'Training and Evaluation Data We crop from the proposed', 'confidence': 0.998497724533081, 'text_region': [[615.0, 1152.0], [1094.0, 1152.0], [1094.0, 1180.0], [615.0, 1180.0]]}, {'text': 'multilingual dataset. We discard images with widths shorter', 'confidence': 0.997459352016449, 'text_region': [[617.0, 1178.0], [1093.0, 1178.0], [1093.0, 1201.0], [617.0, 1201.0]]}, {'text': 'than 32 pixels as they are too blurry, and obtain 4.1M word', 'confidence': 0.9866488575935364, 'text_region': [[617.0, 1203.0], [1093.0, 1203.0], [1093.0, 1226.0], [617.0, 1226.0]]}, {'text': 'images in total. We compare with the multilingual version', 'confidence': 0.9851393699645996, 'text_region': [[615.0, 1226.0], [1093.0, 1226.0], [1093.0, 1249.0], [615.0, 1249.0]]}, {'text': 'of SynthText provided by MLT 2019 competition that con-', 'confidence': 0.9660202860832214, 'text_region': [[615.0, 1249.0], [1089.0, 1249.0], [1089.0, 1272.0], [615.0, 1272.0]]}, {'text': 'tains a total number 1.2M images. For evaluation, we ran-', 'confidence': 0.9913843274116516, 'text_region': [[615.0, 1274.0], [1089.0, 1274.0], [1089.0, 1297.0], [615.0, 1297.0]]}, {'text': 'domly split 1500images for each language (including sym-', 'confidence': 0.9599055051803589, 'text_region': [[615.0, 1299.0], [1089.0, 1299.0], [1089.0, 1320.0], [615.0, 1320.0]]}, {'text': 'bols and mixed) from the training set of MLT 2019. The', 'confidence': 0.9869202375411987, 'text_region': [[615.0, 1322.0], [1093.0, 1322.0], [1093.0, 1345.0], [615.0, 1345.0]]}, {'text': 'rest of the training set is used for training.', 'confidence': 0.9988272190093994, 'text_region': [[615.0, 1346.0], [951.0, 1346.0], [951.0, 1370.0], [615.0, 1370.0]]}], 'img_idx': 7, 'score': 0.9962812662124634}\n",
      "{'type': 'text', 'bbox': [615, 198, 1090, 724], 'res': [{'text': 'Experiment results are summarized in Tab. 7. First, we', 'confidence': 0.9801115989685059, 'text_region': [[617.0, 196.0], [1093.0, 196.0], [1093.0, 219.0], [617.0, 219.0]]}, {'text': 'compare our method with previous synthetic datasets. We', 'confidence': 0.989126443862915, 'text_region': [[617.0, 221.0], [1093.0, 221.0], [1093.0, 244.0], [617.0, 244.0]]}, {'text': 'have to limit the size of training datasets to 1M since', 'confidence': 0.9837574362754822, 'text_region': [[615.0, 244.0], [1091.0, 244.0], [1091.0, 267.0], [615.0, 267.0]]}, {'text': 'VISD only publishes 1M word images. Our synthetic data', 'confidence': 0.9848089814186096, 'text_region': [[617.0, 269.0], [1093.0, 269.0], [1093.0, 292.0], [617.0, 292.0]]}, {'text': 'achieves consistent improvements on all datasets. Espe-', 'confidence': 0.9909741282463074, 'text_region': [[614.0, 292.0], [1089.0, 292.0], [1089.0, 315.0], [614.0, 315.0]]}, {'text': 'cially, it surpasses other synthetic datasets by a consider-', 'confidence': 0.9918091297149658, 'text_region': [[617.0, 317.0], [1091.0, 317.0], [1091.0, 340.0], [617.0, 340.0]]}, {'text': 'able margin on datasets with diverse text styles and complex', 'confidence': 0.9817214608192444, 'text_region': [[615.0, 340.0], [1091.0, 340.0], [1091.0, 363.0], [615.0, 363.0]]}, {'text': 'backgrounds such as SVTP (+2.4%). The experiments ver-', 'confidence': 0.9779967665672302, 'text_region': [[615.0, 365.0], [1091.0, 365.0], [1091.0, 388.0], [615.0, 388.0]]}, {'text': 'ify the effectiveness of our synthesis method in scene text', 'confidence': 0.9893297553062439, 'text_region': [[615.0, 388.0], [1093.0, 388.0], [1093.0, 411.0], [615.0, 411.0]]}, {'text': 'recognition especially in the complex cases.', 'confidence': 0.9913833141326904, 'text_region': [[615.0, 412.0], [970.0, 412.0], [970.0, 436.0], [615.0, 436.0]]}, {'text': 'Since small scale experiments are not very helpful in', 'confidence': 0.9854227900505066, 'text_region': [[640.0, 442.0], [1093.0, 442.0], [1093.0, 465.0], [640.0, 465.0]]}, {'text': 'how researchers should utilize these datasets, we further', 'confidence': 0.9675043225288391, 'text_region': [[615.0, 465.0], [1093.0, 465.0], [1093.0, 488.0], [615.0, 488.0]]}, {'text': 'train models on combinations of Synth90K, SynthText, and', 'confidence': 0.9749721884727478, 'text_region': [[615.0, 490.0], [1093.0, 490.0], [1093.0, 513.0], [615.0, 513.0]]}, {'text': 'ours. We first limit the total number of training images to', 'confidence': 0.9744146466255188, 'text_region': [[615.0, 513.0], [1093.0, 515.0], [1093.0, 538.0], [615.0, 536.0]]}, {'text': '9M. When we train on a combination of all 3 synthetic', 'confidence': 0.9862140417098999, 'text_region': [[612.0, 533.0], [1093.0, 535.0], [1093.0, 563.0], [612.0, 561.0]]}, {'text': 'datasets,with 3Meach,the model performs better than the', 'confidence': 0.9690985083580017, 'text_region': [[615.0, 563.0], [1091.0, 563.0], [1091.0, 584.0], [615.0, 584.0]]}, {'text': 'model trained on 4.5M × 2 datasets only. We further ob-', 'confidence': 0.9576768279075623, 'text_region': [[617.0, 586.0], [1089.0, 586.0], [1089.0, 609.0], [617.0, 609.0]]}, {'text': 'serve that training on 3M × 3 synthetic datasets is com-', 'confidence': 0.9753170013427734, 'text_region': [[615.0, 610.0], [1091.0, 610.0], [1091.0, 634.0], [615.0, 634.0]]}, {'text': 'parable to training on the whole Synth90K and SynthText,', 'confidence': 0.986801266670227, 'text_region': [[615.0, 634.0], [1091.0, 634.0], [1091.0, 657.0], [615.0, 657.0]]}, {'text': 'while using much fewer training data. This result suggests', 'confidence': 0.9941716194152832, 'text_region': [[614.0, 653.0], [1093.0, 655.0], [1093.0, 683.0], [614.0, 681.0]]}, {'text': 'that the best practice is to combine the proposed synthetic', 'confidence': 0.990123450756073, 'text_region': [[612.0, 676.0], [1093.0, 680.0], [1092.0, 706.0], [612.0, 703.0]]}, {'text': 'dataset with previous ones.', 'confidence': 0.9976393580436707, 'text_region': [[617.0, 705.0], [833.0, 705.0], [833.0, 728.0], [617.0, 728.0]]}], 'img_idx': 7, 'score': 0.9933221936225891}\n",
      "{'type': 'text', 'bbox': [99, 699, 573, 1054], 'res': [{'text': 'Model We select a widely accepted baseline method,', 'confidence': 0.974570631980896, 'text_region': [[95.0, 693.0], [574.0, 695.0], [574.0, 723.0], [95.0, 721.0]]}, {'text': 'ASTER [39], and adopt the implementation? that ranks top-', 'confidence': 0.9821949601173401, 'text_region': [[97.0, 718.0], [574.0, 719.0], [574.0, 747.0], [96.0, 746.0]]}, {'text': '1 on the ICDAR 2019 ArT competition on curved scene text', 'confidence': 0.9737233519554138, 'text_region': [[100.0, 746.0], [575.0, 746.0], [575.0, 769.0], [100.0, 769.0]]}, {'text': 'recognition (Latin) by [20]. The models are trained with a', 'confidence': 0.9811419248580933, 'text_region': [[98.0, 769.0], [574.0, 769.0], [574.0, 792.0], [98.0, 792.0]]}, {'text': 'batch size of 512. A total of 95 symbols are recognized,', 'confidence': 0.9769392013549805, 'text_region': [[97.0, 790.0], [575.0, 792.0], [575.0, 817.0], [96.0, 815.0]]}, {'text': 'including an End-of-Sentence mark, 52 case sensitive al-', 'confidence': 0.9710802435874939, 'text_region': [[98.0, 817.0], [574.0, 817.0], [574.0, 840.0], [98.0, 840.0]]}, {'text': 'phabets, 10 digits, and 32 printable punctuation symbols.', 'confidence': 0.9932325482368469, 'text_region': [[100.0, 842.0], [554.0, 842.0], [554.0, 865.0], [100.0, 865.0]]}, {'text': 'Training Datasets From the 600K English synthetic im-', 'confidence': 0.9734586477279663, 'text_region': [[100.0, 865.0], [570.0, 865.0], [570.0, 888.0], [100.0, 888.0]]}, {'text': 'ages, we obtain a total number of 12M word-level image', 'confidence': 0.9916660785675049, 'text_region': [[98.0, 891.0], [574.0, 891.0], [574.0, 914.0], [98.0, 914.0]]}, {'text': 'regions to make our training dataset. Also note that, our', 'confidence': 0.9897025227546692, 'text_region': [[98.0, 916.0], [575.0, 916.0], [575.0, 939.0], [98.0, 939.0]]}, {'text': 'synthetic dataset provide character level annotations, which', 'confidence': 0.9943373203277588, 'text_region': [[95.0, 936.0], [577.0, 934.0], [577.0, 962.0], [95.0, 964.0]]}, {'text': 'will be useful in some recognition algorithms.', 'confidence': 0.9599004983901978, 'text_region': [[97.0, 960.0], [467.0, 962.0], [467.0, 985.0], [96.0, 983.0]]}, {'text': 'Evaluation Datasets We evaluate models trained on dif-', 'confidence': 0.9847884178161621, 'text_region': [[95.0, 985.0], [575.0, 983.0], [575.0, 1011.0], [95.0, 1013.0]]}, {'text': 'ferent synthetic datasets on several widely used real image', 'confidence': 0.9897794723510742, 'text_region': [[98.0, 1011.0], [575.0, 1011.0], [575.0, 1035.0], [98.0, 1035.0]]}, {'text': 'datasets:IIIT [25], SVT [45],ICDAR 2015 (IC15)[13],', 'confidence': 0.9456235766410828, 'text_region': [[98.0, 1036.0], [574.0, 1036.0], [574.0, 1058.0], [98.0, 1058.0]]}], 'img_idx': 7, 'score': 0.9873678684234619}\n",
      "{'type': 'text', 'bbox': [98, 1087, 572, 1366], 'res': [{'text': 'Some of these datasets,however,have incomplete an-', 'confidence': 0.9735957384109497, 'text_region': [[121.0, 1086.0], [574.0, 1086.0], [574.0, 1107.0], [121.0, 1107.0]]}, {'text': 'notations, including IIT, SVT, SVTP, CUTE. While the', 'confidence': 0.9792498350143433, 'text_region': [[98.0, 1109.0], [575.0, 1109.0], [575.0, 1132.0], [98.0, 1132.0]]}, {'text': 'word images in these datasets contain punctuation symbols.', 'confidence': 0.9804876446723938, 'text_region': [[98.0, 1132.0], [570.0, 1132.0], [570.0, 1155.0], [98.0, 1155.0]]}, {'text': ' digits, upper-case and lower-case characters, the aforemen-', 'confidence': 0.9801051020622253, 'text_region': [[95.0, 1155.0], [572.0, 1153.0], [572.0, 1181.0], [95.0, 1183.0]]}, {'text': 'tioned datasets, in their current forms, only provide case-', 'confidence': 0.9892176985740662, 'text_region': [[98.0, 1181.0], [570.0, 1181.0], [570.0, 1204.0], [98.0, 1204.0]]}, {'text': ' insensitive annotations and ignore all punctuation symbols.', 'confidence': 0.9873183965682983, 'text_region': [[95.0, 1201.0], [572.0, 1203.0], [572.0, 1231.0], [95.0, 1229.0]]}, {'text': 'In order for more comprehensive evaluation of scene text', 'confidence': 0.9791092276573181, 'text_region': [[98.0, 1229.0], [575.0, 1229.0], [575.0, 1252.0], [98.0, 1252.0]]}, {'text': 'recognition,we re-annotate these 4 datasets in a case-', 'confidence': 0.9663134217262268, 'text_region': [[98.0, 1254.0], [574.0, 1254.0], [574.0, 1275.0], [98.0, 1275.0]]}, {'text': 'sensitive way and also include punctuation symbols. We', 'confidence': 0.9754579067230225, 'text_region': [[98.0, 1277.0], [575.0, 1277.0], [575.0, 1300.0], [98.0, 1300.0]]}, {'text': 'also release the new annotations and we believe that they', 'confidence': 0.9792709350585938, 'text_region': [[97.0, 1298.0], [572.0, 1302.0], [572.0, 1325.0], [96.0, 1322.0]]}, {'text': 'will become better benchmarks for scene text recognition', 'confidence': 0.9845571517944336, 'text_region': [[95.0, 1320.0], [577.0, 1322.0], [577.0, 1350.0], [95.0, 1348.0]]}, {'text': 'in the future.', 'confidence': 0.9996659159660339, 'text_region': [[96.0, 1348.0], [205.0, 1348.0], [205.0, 1371.0], [96.0, 1371.0]]}], 'img_idx': 7, 'score': 0.9831495881080627}\n",
      "{'type': 'text', 'bbox': [99, 529, 574, 595], 'res': [{'text': 'In addition to the superior performances in training scene', 'confidence': 0.9724850058555603, 'text_region': [[122.0, 525.0], [574.0, 528.0], [574.0, 551.0], [121.0, 548.0]]}, {'text': 'text detection models, we also verify its effectiveness in the', 'confidence': 0.9811576008796692, 'text_region': [[97.0, 549.0], [575.0, 551.0], [575.0, 574.0], [96.0, 573.0]]}, {'text': 'task of scene text recognition.', 'confidence': 0.9781603217124939, 'text_region': [[98.0, 576.0], [338.0, 576.0], [338.0, 599.0], [98.0, 599.0]]}], 'img_idx': 7, 'score': 0.9771161079406738}\n",
      "{'type': 'text', 'bbox': [100, 410, 564, 452], 'res': [{'text': 'random lighting conditions realize different real-world vari-', 'confidence': 0.986575722694397, 'text_region': [[96.0, 409.0], [574.0, 408.0], [574.0, 431.0], [97.0, 432.0]]}, {'text': 'ations, which we also attribute as a key factor.', 'confidence': 0.9857797622680664, 'text_region': [[98.0, 434.0], [469.0, 434.0], [469.0, 457.0], [98.0, 457.0]]}], 'img_idx': 7, 'score': 0.937526524066925}\n",
      "{'type': 'text', 'bbox': [109, 1061, 430, 1078], 'res': [{'text': 'SVTP [32], CUTE [34], and Total-Text[4].', 'confidence': 0.9813268780708313, 'text_region': [[98.0, 1059.0], [436.0, 1059.0], [436.0, 1082.0], [98.0, 1082.0]]}], 'img_idx': 7, 'score': 0.6481130123138428}\n",
      "{'type': 'title', 'bbox': [616, 764, 998, 823], 'res': [{'text': '5.2. Recognizing Multilingual Scene Text', 'confidence': 0.9959180951118469, 'text_region': [[612.0, 759.0], [1000.0, 761.0], [999.0, 789.0], [612.0, 787.0]]}, {'text': '5.2.1Settings', 'confidence': 0.9997094869613647, 'text_region': [[613.0, 798.0], [751.0, 806.0], [749.0, 834.0], [611.0, 826.0]]}], 'img_idx': 7, 'score': 0.9605708122253418}\n",
      "{'type': 'title', 'bbox': [99, 485, 528, 508], 'res': [{'text': '5. Experiments on Scene Text Recognition', 'confidence': 0.9903417825698853, 'text_region': [[98.0, 483.0], [531.0, 483.0], [531.0, 511.0], [98.0, 511.0]]}], 'img_idx': 7, 'score': 0.9427546262741089}\n",
      "{'type': 'title', 'bbox': [618, 149, 846, 167], 'res': [{'text': '5.1.2 Experiment Results', 'confidence': 0.9769027829170227, 'text_region': [[615.0, 147.0], [848.0, 147.0], [848.0, 170.0], [615.0, 170.0]]}], 'img_idx': 7, 'score': 0.9418621063232422}\n",
      "{'type': 'title', 'bbox': [101, 659, 223, 675], 'res': [{'text': '5.1.1Settings', 'confidence': 0.9996361136436462, 'text_region': [[96.0, 650.0], [232.0, 655.0], [230.0, 683.0], [94.0, 678.0]]}], 'img_idx': 7, 'score': 0.9216418266296387}\n",
      "{'type': 'figure', 'bbox': [109, 136, 552, 315], 'res': [{'text': 'wih Env Rand', 'confidence': 0.8002579212188721, 'text_region': [[461.0, 160.0], [522.0, 160.0], [522.0, 177.0], [461.0, 177.0]]}, {'text': '10m152025', 'confidence': 0.7767367362976074, 'text_region': [[396.0, 284.0], [509.0, 282.0], [509.0, 300.0], [396.0, 302.0]]}, {'text': 'Figure 5: Results of ablation tests: (a) ablating viewfinder', 'confidence': 0.9714148640632629, 'text_region': [[98.0, 315.0], [572.0, 315.0], [572.0, 338.0], [98.0, 338.0]]}], 'img_idx': 7, 'score': 0.9306920170783997}\n",
      "{'type': 'figure_caption', 'bbox': [98, 316, 571, 359], 'res': [{'text': 'Figure 5: Results of ablation tests: (a) ablating viewfinder', 'confidence': 0.9714148640632629, 'text_region': [[98.0, 315.0], [572.0, 315.0], [572.0, 338.0], [98.0, 338.0]]}, {'text': 'module; (b) ablating environment randomization module', 'confidence': 0.9932379126548767, 'text_region': [[98.0, 340.0], [555.0, 340.0], [555.0, 363.0], [98.0, 363.0]]}], 'img_idx': 7, 'score': 0.889987587928772}\n",
      "{'type': 'reference', 'bbox': [113, 1389, 417, 1425], 'res': [{'text': 'Shttps://github.com/Jyouhou/', 'confidence': 0.9859605431556702, 'text_region': [[117.0, 1384.0], [389.0, 1386.0], [389.0, 1411.0], [116.0, 1409.0]]}, {'text': 'ICDAR2019-ArT-Recognition-Alchemy', 'confidence': 0.9964471459388733, 'text_region': [[100.0, 1409.0], [417.0, 1409.0], [417.0, 1431.0], [100.0, 1431.0]]}], 'img_idx': 7, 'score': 0.9281263947486877}\n",
      "{'type': 'reference', 'bbox': [632, 1408, 904, 1427], 'res': [{'text': '6https://dumps.wikimedia.org', 'confidence': 0.9847373962402344, 'text_region': [[637.0, 1406.0], [906.0, 1409.0], [906.0, 1432.0], [637.0, 1429.0]]}], 'img_idx': 7, 'score': 0.7888036370277405}\n",
      "{'type': 'text', 'bbox': [98, 1096, 573, 1426], 'res': [{'text': ' There are several aspects that are worth diving deeper', 'confidence': 0.9861298203468323, 'text_region': [[118.0, 1091.0], [577.0, 1092.0], [577.0, 1120.0], [118.0, 1119.0]]}, {'text': 'into: (1) Overall, the engine is based on rules and human-', 'confidence': 0.984455943107605, 'text_region': [[98.0, 1119.0], [574.0, 1119.0], [574.0, 1142.0], [98.0, 1142.0]]}, {'text': 'selected parameters. The automation of the selection and', 'confidence': 0.9855233430862427, 'text_region': [[98.0, 1142.0], [577.0, 1142.0], [577.0, 1165.0], [98.0, 1165.0]]}, {'text': 'search for these parameters can save human efforts and help', 'confidence': 0.992008626461029, 'text_region': [[98.0, 1167.0], [575.0, 1167.0], [575.0, 1190.0], [98.0, 1190.0]]}, {'text': 'adapt to different scenarios. (2) While rendering small text', 'confidence': 0.982808530330658, 'text_region': [[98.0, 1190.0], [575.0, 1190.0], [575.0, 1213.0], [98.0, 1213.0]]}, {'text': 'can help training detectors, the low image quality of the', 'confidence': 0.9924018979072571, 'text_region': [[98.0, 1214.0], [575.0, 1214.0], [575.0, 1238.0], [98.0, 1238.0]]}, {'text': 'small text makes recognizers harder to train and harms the', 'confidence': 0.9948105216026306, 'text_region': [[98.0, 1239.0], [575.0, 1239.0], [575.0, 1262.0], [98.0, 1262.0]]}, {'text': 'performance. Designing a method to mark the illegible ones', 'confidence': 0.9888966679573059, 'text_region': [[98.0, 1262.0], [575.0, 1262.0], [575.0, 1285.0], [98.0, 1285.0]]}, {'text': 'as difficult and excluding them from loss calculation may', 'confidence': 0.9687724113464355, 'text_region': [[98.0, 1285.0], [574.0, 1285.0], [574.0, 1308.0], [98.0, 1308.0]]}, {'text': 'help mitigate this problem. (3) For multilingual scene text,', 'confidence': 0.9867030382156372, 'text_region': [[96.0, 1310.0], [574.0, 1310.0], [574.0, 1333.0], [96.0, 1333.0]]}, {'text': 'scripts except Latin have much fewer available fonts that we', 'confidence': 0.9889711141586304, 'text_region': [[95.0, 1332.0], [577.0, 1330.0], [577.0, 1358.0], [95.0, 1360.0]]}, {'text': 'have easy access to. To improve performance on more lan-', 'confidence': 0.9909709692001343, 'text_region': [[96.0, 1358.0], [574.0, 1358.0], [574.0, 1381.0], [96.0, 1381.0]]}, {'text': ' guages, researchers may consider learning-based methods', 'confidence': 0.9896305799484253, 'text_region': [[95.0, 1379.0], [577.0, 1378.0], [577.0, 1406.0], [95.0, 1407.0]]}, {'text': 'to transfer Latin fonts to other scripts.', 'confidence': 0.9807524085044861, 'text_region': [[95.0, 1404.0], [404.0, 1406.0], [404.0, 1429.0], [95.0, 1427.0]]}], 'img_idx': 8, 'score': 0.9946483373641968}\n",
      "{'type': 'text', 'bbox': [616, 421, 1091, 605], 'res': [{'text': 'In this paper, we introduce a scene text image synthesis', 'confidence': 0.9908609390258789, 'text_region': [[639.0, 419.0], [1093.0, 419.0], [1093.0, 442.0], [639.0, 442.0]]}, {'text': 'engine that renders images with 3D graphics engines, where', 'confidence': 0.9795994162559509, 'text_region': [[615.0, 442.0], [1093.0, 442.0], [1093.0, 465.0], [615.0, 465.0]]}, {'text': 'text instances and scenes are rendered as a whole. In exper-', 'confidence': 0.9690290093421936, 'text_region': [[612.0, 460.0], [1094.0, 464.0], [1094.0, 492.0], [612.0, 488.0]]}, {'text': 'iments, we verify the effectiveness of the proposed engine', 'confidence': 0.9644917249679565, 'text_region': [[614.0, 488.0], [1093.0, 490.0], [1093.0, 513.0], [614.0, 511.0]]}, {'text': 'in both scene text detection and recognition models. We', 'confidence': 0.9785439968109131, 'text_region': [[614.0, 511.0], [1094.0, 511.0], [1094.0, 540.0], [614.0, 540.0]]}, {'text': 'also study key components of the proposed engine. We be-', 'confidence': 0.9709753394126892, 'text_region': [[614.0, 536.0], [1088.0, 536.0], [1088.0, 559.0], [614.0, 559.0]]}, {'text': 'lieve our work will be a solid stepping stone towards better', 'confidence': 0.9921978116035461, 'text_region': [[612.0, 558.0], [1094.0, 559.0], [1094.0, 587.0], [612.0, 586.0]]}, {'text': 'synthesis algorithms.', 'confidence': 0.9994872212409973, 'text_region': [[615.0, 586.0], [788.0, 586.0], [788.0, 609.0], [615.0, 609.0]]}], 'img_idx': 8, 'score': 0.9943484663963318}\n",
      "{'type': 'text', 'bbox': [98, 667, 573, 994], 'res': [{'text': 'Experiment results are shown in Tab. 4. When we only use', 'confidence': 0.9952220320701599, 'text_region': [[98.0, 667.0], [575.0, 667.0], [575.0, 690.0], [98.0, 690.0]]}, {'text': 'synthetic data and control the number of images to 1.2M,', 'confidence': 0.9805254340171814, 'text_region': [[95.0, 688.0], [577.0, 686.0], [577.0, 714.0], [95.0, 716.0]]}, {'text': 'ours result in a considerable improvement of 1.6% in over-', 'confidence': 0.9969189763069153, 'text_region': [[98.0, 714.0], [575.0, 714.0], [575.0, 738.0], [98.0, 738.0]]}, {'text': 'all accuracy, and significant improvements on some scripts,', 'confidence': 0.9824081659317017, 'text_region': [[97.0, 736.0], [575.0, 738.0], [575.0, 761.0], [96.0, 759.0]]}, {'text': 'e.g. Latin (+7.6%) and Mixed (+21.6%). Using the whole', 'confidence': 0.9835116863250732, 'text_region': [[95.0, 759.0], [579.0, 757.0], [579.0, 785.0], [95.0, 787.0]]}, {'text': 'training set of 4.1M images further improves overall accu-', 'confidence': 0.9847218990325928, 'text_region': [[96.0, 785.0], [574.0, 785.0], [574.0, 809.0], [96.0, 809.0]]}, {'text': 'racy to 39.5%. When we train models on combinations of', 'confidence': 0.9873580932617188, 'text_region': [[93.0, 807.0], [577.0, 805.0], [577.0, 833.0], [93.0, 835.0]]}, {'text': 'synthetic data and our training split of MLT19, as shown', 'confidence': 0.9822039008140564, 'text_region': [[95.0, 832.0], [579.0, 830.0], [579.0, 858.0], [95.0, 860.0]]}, {'text': 'in thebottom of Tab.4,we can still observe a considerable', 'confidence': 0.953581690788269, 'text_region': [[98.0, 858.0], [575.0, 858.0], [575.0, 879.0], [98.0, 879.0]]}, {'text': 'margin of our method over SynthText by 3.2% in overall ac-', 'confidence': 0.9930679798126221, 'text_region': [[98.0, 881.0], [574.0, 881.0], [574.0, 904.0], [98.0, 904.0]]}, {'text': 'curacy. The experiment results demonstrate that our method', 'confidence': 0.994177520275116, 'text_region': [[98.0, 906.0], [575.0, 906.0], [575.0, 929.0], [98.0, 929.0]]}, {'text': 'is also superior in multilingual scene text recognition, and', 'confidence': 0.9850260615348816, 'text_region': [[96.0, 929.0], [575.0, 929.0], [575.0, 952.0], [96.0, 952.0]]}, {'text': 'we believe this result will become a stepping stone to fur-', 'confidence': 0.9838709831237793, 'text_region': [[98.0, 952.0], [572.0, 952.0], [572.0, 975.0], [98.0, 975.0]]}, {'text': 'ther research.', 'confidence': 0.992685854434967, 'text_region': [[96.0, 977.0], [210.0, 977.0], [210.0, 1000.0], [96.0, 1000.0]]}], 'img_idx': 8, 'score': 0.9941602349281311}\n",
      "{'type': 'text', 'bbox': [618, 660, 1088, 700], 'res': [{'text': 'This research was supported by National Key R&D Pro-', 'confidence': 0.9899672269821167, 'text_region': [[637.0, 653.0], [1091.0, 655.0], [1091.0, 683.0], [637.0, 681.0]]}, {'text': 'gram of China (No. 2017YFA0700800).', 'confidence': 0.9799442887306213, 'text_region': [[614.0, 681.0], [941.0, 680.0], [941.0, 703.0], [614.0, 705.0]]}], 'img_idx': 8, 'score': 0.9607256054878235}\n",
      "{'type': 'text', 'bbox': [98, 292, 1088, 335], 'res': [{'text': 'Table 4: Multilingual scene text recognition results (word level accuracy). Latin aggregates English, French, German, and', 'confidence': 0.9846962094306946, 'text_region': [[96.0, 285.0], [1094.0, 287.0], [1094.0, 315.0], [96.0, 313.0]]}, {'text': 'Italian, as they are all marked as Latin in the MLT dataset.', 'confidence': 0.9696460962295532, 'text_region': [[98.0, 314.0], [565.0, 314.0], [565.0, 337.0], [98.0, 337.0]]}], 'img_idx': 8, 'score': 0.9139441847801208}\n",
      "{'type': 'title', 'bbox': [99, 1048, 418, 1066], 'res': [{'text': '6. Limitation and Future Work', 'confidence': 0.9702661633491516, 'text_region': [[98.0, 1046.0], [417.0, 1046.0], [417.0, 1069.0], [98.0, 1069.0]]}], 'img_idx': 8, 'score': 0.955498218536377}\n",
      "{'type': 'title', 'bbox': [617, 727, 728, 744], 'res': [{'text': 'References', 'confidence': 0.996708869934082, 'text_region': [[614.0, 724.0], [734.0, 728.0], [733.0, 751.0], [613.0, 747.0]]}], 'img_idx': 8, 'score': 0.9523997902870178}\n",
      "{'type': 'title', 'bbox': [99, 615, 328, 633], 'res': [{'text': '5.2.2 Experiment Results', 'confidence': 0.9695145487785339, 'text_region': [[98.0, 614.0], [331.0, 614.0], [331.0, 637.0], [98.0, 637.0]]}], 'img_idx': 8, 'score': 0.9486263394355774}\n",
      "{'type': 'title', 'bbox': [617, 380, 755, 398], 'res': [{'text': '7. Conclusion', 'confidence': 0.978743851184845, 'text_region': [[614.0, 374.0], [759.0, 378.0], [758.0, 403.0], [613.0, 399.0]]}], 'img_idx': 8, 'score': 0.944165050983429}\n",
      "{'type': 'title', 'bbox': [619, 631, 804, 652], 'res': [{'text': 'Acknowledgement', 'confidence': 0.9990953803062439, 'text_region': [[615.0, 625.0], [808.0, 627.0], [808.0, 655.0], [615.0, 653.0]]}], 'img_idx': 8, 'score': 0.8221420049667358}\n",
      "{'type': 'table', 'bbox': [97, 375, 587, 523], 'res': '', 'img_idx': 8, 'score': 0.9613305330276489}\n",
      "{'type': 'table', 'bbox': [129, 138, 1053, 282], 'res': '', 'img_idx': 8, 'score': 0.9024681448936462}\n",
      "{'type': 'reference', 'bbox': [626, 767, 1089, 1421], 'res': [{'text': '[1]Pablo Arbelaez,Michael Maire,Charless Fowlkes,and Ji-', 'confidence': 0.9817557334899902, 'text_region': [[624.0, 764.0], [1089.0, 764.0], [1089.0, 785.0], [624.0, 785.0]]}, {'text': 'tendra Malik. Contour detection and hierarchical image seg-', 'confidence': 0.976404070854187, 'text_region': [[654.0, 784.0], [1091.0, 787.0], [1091.0, 810.0], [654.0, 807.0]]}, {'text': 'mentation.IEEE transactions on pattern analysis and ma-', 'confidence': 0.9791105389595032, 'text_region': [[655.0, 809.0], [1091.0, 809.0], [1091.0, 832.0], [655.0, 832.0]]}, {'text': 'chine intelligence, 33(5):898-916,2011.', 'confidence': 0.984925389289856, 'text_region': [[655.0, 830.0], [951.0, 830.0], [951.0, 853.0], [655.0, 853.0]]}, {'text': '[2] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun,', 'confidence': 0.9846956133842468, 'text_region': [[624.0, 853.0], [1093.0, 853.0], [1093.0, 876.0], [624.0, 876.0]]}, {'text': 'and Hwalsuk Lee. Character region awareness for text detec-', 'confidence': 0.9865809082984924, 'text_region': [[655.0, 874.0], [1091.0, 874.0], [1091.0, 898.0], [655.0, 898.0]]}, {'text': 'tion.InProceedings of theIEEE Conference on Computer', 'confidence': 0.9614017009735107, 'text_region': [[655.0, 898.0], [1093.0, 898.0], [1093.0, 919.0], [655.0, 919.0]]}, {'text': 'Vision and Pattern Recognition(CVPR),pages 9365-9374,', 'confidence': 0.9712094068527222, 'text_region': [[655.0, 919.0], [1094.0, 919.0], [1094.0, 942.0], [655.0, 942.0]]}, {'text': '2019.', 'confidence': 0.999503493309021, 'text_region': [[657.0, 940.0], [698.0, 940.0], [698.0, 960.0], [657.0, 960.0]]}, {'text': '[3] Zhanzhan Cheng, Xuyang Liu, Fan Bai, Yi Niu, Shiliang Pu,', 'confidence': 0.9714673161506653, 'text_region': [[624.0, 964.0], [1093.0, 964.0], [1093.0, 987.0], [624.0, 987.0]]}, {'text': 'and Shuigeng Zhou. Arbitrarily-oriented text recognition.', 'confidence': 0.9924642443656921, 'text_region': [[655.0, 985.0], [1093.0, 985.0], [1093.0, 1008.0], [655.0, 1008.0]]}, {'text': 'CVPR2018,2017.', 'confidence': 0.9976934790611267, 'text_region': [[655.0, 1005.0], [788.0, 1007.0], [788.0, 1030.0], [655.0, 1028.0]]}, {'text': \"[4]Chee Kheng Ch'ng and Chee Seng Chan. Total-text: A com-\", 'confidence': 0.9742546081542969, 'text_region': [[624.0, 1030.0], [1089.0, 1030.0], [1089.0, 1053.0], [624.0, 1053.0]]}, {'text': 'prehensive dataset for scene text detection and recognition.', 'confidence': 0.9898720979690552, 'text_region': [[655.0, 1053.0], [1091.0, 1053.0], [1091.0, 1076.0], [655.0, 1076.0]]}, {'text': 'In Proc. ICDAR, volume 1, pages 935-942, 2017.', 'confidence': 0.986649215221405, 'text_region': [[652.0, 1069.0], [1018.0, 1071.0], [1018.0, 1099.0], [652.0, 1097.0]]}, {'text': '[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing', 'confidence': 0.9712687134742737, 'text_region': [[622.0, 1092.0], [1094.0, 1094.0], [1094.0, 1122.0], [622.0, 1120.0]]}, {'text': 'Xu,David Warde-Farley,Sherjil Ozair,Aaron Courville,and', 'confidence': 0.9811122417449951, 'text_region': [[655.0, 1119.0], [1094.0, 1119.0], [1094.0, 1140.0], [655.0, 1140.0]]}, {'text': 'Yoshua Bengio. Generative adversarial nets. In Proc. NIPS,', 'confidence': 0.9824429750442505, 'text_region': [[655.0, 1140.0], [1093.0, 1140.0], [1093.0, 1163.0], [655.0, 1163.0]]}, {'text': 'pages 2672-2680, 2014.', 'confidence': 0.9656574130058289, 'text_region': [[653.0, 1160.0], [834.0, 1155.0], [835.0, 1183.0], [654.0, 1188.0]]}, {'text': '[6] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman.', 'confidence': 0.9830030798912048, 'text_region': [[624.0, 1185.0], [1091.0, 1185.0], [1091.0, 1208.0], [624.0, 1208.0]]}, {'text': 'Synthetic data for text localisation in natural images. In Proc.', 'confidence': 0.9920827150344849, 'text_region': [[655.0, 1208.0], [1091.0, 1208.0], [1091.0, 1231.0], [655.0, 1231.0]]}, {'text': 'CVPR, pages 2315-2324, 2016.', 'confidence': 0.9813662767410278, 'text_region': [[655.0, 1229.0], [888.0, 1229.0], [888.0, 1252.0], [655.0, 1252.0]]}, {'text': '[7] Tong He, Zhi Tian, Weilin Huang, Chunhua Shen, Yu Qiao,', 'confidence': 0.9672161936759949, 'text_region': [[624.0, 1252.0], [1093.0, 1252.0], [1093.0, 1275.0], [624.0, 1275.0]]}, {'text': 'and Changming Sun.An end-to-end textspotter with explicit', 'confidence': 0.9894373416900635, 'text_region': [[654.0, 1272.0], [1094.0, 1274.0], [1094.0, 1297.0], [654.0, 1295.0]]}, {'text': 'alignment and attention. In Proc. CVPR, pages 5020-5029,', 'confidence': 0.9765701293945312, 'text_region': [[655.0, 1295.0], [1094.0, 1295.0], [1094.0, 1318.0], [655.0, 1318.0]]}, {'text': '2018.', 'confidence': 0.9973315000534058, 'text_region': [[655.0, 1318.0], [698.0, 1318.0], [698.0, 1338.0], [655.0, 1338.0]]}, {'text': '[8]Stefan Hinterstoisser, Olivier Pauly,Hauke Heibel, Martina', 'confidence': 0.9837086796760559, 'text_region': [[622.0, 1340.0], [1093.0, 1338.0], [1093.0, 1361.0], [622.0, 1363.0]]}, {'text': 'Marek, and Martin Bokeloh. An annotation saved is an an-', 'confidence': 0.9864329099655151, 'text_region': [[655.0, 1363.0], [1093.0, 1363.0], [1093.0, 1384.0], [655.0, 1384.0]]}, {'text': 'notation earned: Using fully synthetic training for object in-', 'confidence': 0.984932541847229, 'text_region': [[654.0, 1384.0], [1091.0, 1384.0], [1091.0, 1407.0], [654.0, 1407.0]]}, {'text': 'stance detection. CoRR,abs/1902.09967, 2019.', 'confidence': 0.9694673418998718, 'text_region': [[654.0, 1406.0], [1001.0, 1404.0], [1001.0, 1427.0], [654.0, 1429.0]]}], 'img_idx': 8, 'score': 0.9949038624763489}\n",
      "{'type': 'reference', 'bbox': [101, 533, 572, 555], 'res': [{'text': 'Table 5: Results on English datasets (word level accuracy).', 'confidence': 0.9829257130622864, 'text_region': [[98.0, 530.0], [572.0, 531.0], [572.0, 554.0], [98.0, 553.0]]}], 'img_idx': 8, 'score': 0.6124870777130127}\n",
      "{'type': 'reference', 'bbox': [101, 152, 574, 1423], 'res': [{'text': '[9]  Sepp Hochreiter and Juirgen Schmidhuber. Long short-term', 'confidence': 0.9701235890388489, 'text_region': [[108.0, 148.0], [577.0, 148.0], [577.0, 172.0], [108.0, 172.0]]}, {'text': 'memory.Neural computation,9(8):1735-1780,1997.', 'confidence': 0.9924273490905762, 'text_region': [[135.0, 170.0], [529.0, 168.0], [529.0, 191.0], [135.0, 193.0]]}, {'text': '[10] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and An-', 'confidence': 0.9856269955635071, 'text_region': [[98.0, 195.0], [575.0, 195.0], [575.0, 218.0], [98.0, 218.0]]}, {'text': 'drew Zisserman. Synthetic data and artificial neural net-', 'confidence': 0.9885411858558655, 'text_region': [[136.0, 214.0], [574.0, 216.0], [574.0, 239.0], [136.0, 238.0]]}, {'text': 'works for natural scene text recognition.arXiv preprint', 'confidence': 0.9818533062934875, 'text_region': [[138.0, 239.0], [577.0, 239.0], [577.0, 262.0], [138.0, 262.0]]}, {'text': 'arXiv:1406.2227,2014.', 'confidence': 0.9955489635467529, 'text_region': [[138.0, 261.0], [311.0, 261.0], [311.0, 282.0], [138.0, 282.0]]}, {'text': '[11] Yingying Jiang, Xiangyu Zhu, Xiaobing Wang, Shuli Yang,', 'confidence': 0.9881327152252197, 'text_region': [[97.0, 284.0], [574.0, 285.0], [574.0, 309.0], [96.0, 307.0]]}, {'text': 'Wei Li, Hua Wang,Pei Fu, and Zhenbo Luo.R2cnn: rota-', 'confidence': 0.9695068001747131, 'text_region': [[138.0, 307.0], [574.0, 307.0], [574.0, 328.0], [138.0, 328.0]]}, {'text': 'tional region cnn for orientation robust scene text detection.', 'confidence': 0.9750311970710754, 'text_region': [[138.0, 328.0], [575.0, 328.0], [575.0, 351.0], [138.0, 351.0]]}, {'text': 'arXiv preprint arXiv:1706.09579, 2017.', 'confidence': 0.9787220358848572, 'text_region': [[135.0, 350.0], [431.0, 346.0], [431.0, 373.0], [135.0, 376.0]]}, {'text': '[12] Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci,', 'confidence': 0.99027419090271, 'text_region': [[98.0, 375.0], [574.0, 375.0], [574.0, 398.0], [98.0, 398.0]]}, {'text': 'Justin Yuan, Matt Rusiniak,David Acuna, Antonio Torralba,', 'confidence': 0.9695923924446106, 'text_region': [[138.0, 398.0], [574.0, 398.0], [574.0, 419.0], [138.0, 419.0]]}, {'text': 'and Sanja Fidler. Meta-sim: Learning to generate synthetic', 'confidence': 0.9897369146347046, 'text_region': [[136.0, 417.0], [575.0, 419.0], [575.0, 442.0], [136.0, 441.0]]}, {'text': 'datasets. arXiv preprint arXiv:1904.11621,2019.', 'confidence': 0.9894076585769653, 'text_region': [[138.0, 441.0], [496.0, 441.0], [496.0, 464.0], [138.0, 464.0]]}, {'text': '[13] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos', 'confidence': 0.9828172326087952, 'text_region': [[98.0, 465.0], [575.0, 465.0], [575.0, 487.0], [98.0, 487.0]]}, {'text': 'Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwa-', 'confidence': 0.9887598752975464, 'text_region': [[138.0, 487.0], [574.0, 487.0], [574.0, 510.0], [138.0, 510.0]]}, {'text': 'mura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chan-', 'confidence': 0.9918403625488281, 'text_region': [[136.0, 507.0], [572.0, 507.0], [572.0, 530.0], [136.0, 530.0]]}, {'text': 'drasekhar,Shijian Lu,et al. Icdar 2015 competition on robust', 'confidence': 0.976348876953125, 'text_region': [[136.0, 530.0], [574.0, 530.0], [574.0, 551.0], [136.0, 551.0]]}, {'text': 'reading.In 2015 13th International Conference on Docu-', 'confidence': 0.9690995812416077, 'text_region': [[136.0, 553.0], [574.0, 551.0], [574.0, 574.0], [136.0, 576.0]]}, {'text': 'ment Analysis and Recognition (ICDAR), pages 1156-1160.', 'confidence': 0.9925683736801147, 'text_region': [[138.0, 576.0], [574.0, 576.0], [574.0, 599.0], [138.0, 599.0]]}, {'text': 'IEEE,2015.', 'confidence': 0.9620897173881531, 'text_region': [[138.0, 597.0], [228.0, 597.0], [228.0, 615.0], [138.0, 615.0]]}, {'text': '[14] Dimosthenis Karatzas, Faisal Shafait, Seichi Uchida,', 'confidence': 0.9907695651054382, 'text_region': [[96.0, 620.0], [574.0, 620.0], [574.0, 644.0], [96.0, 644.0]]}, {'text': 'Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles', 'confidence': 0.9879017472267151, 'text_region': [[136.0, 642.0], [575.0, 642.0], [575.0, 665.0], [136.0, 665.0]]}, {'text': 'Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Al-', 'confidence': 0.9686670899391174, 'text_region': [[136.0, 665.0], [572.0, 665.0], [572.0, 686.0], [136.0, 686.0]]}, {'text': 'mazan, and Lluis Pere de las Heras. Icdar 2013 robust read-', 'confidence': 0.9708651304244995, 'text_region': [[136.0, 686.0], [574.0, 685.0], [574.0, 708.0], [136.0, 710.0]]}, {'text': 'ing competition. In 2013 12th International Conference on', 'confidence': 0.9852476716041565, 'text_region': [[136.0, 706.0], [577.0, 706.0], [577.0, 734.0], [136.0, 734.0]]}, {'text': 'Document Analysis and Recognition (ICDAR), pages 1484-', 'confidence': 0.9721303582191467, 'text_region': [[138.0, 731.0], [574.0, 731.0], [574.0, 754.0], [138.0, 754.0]]}, {'text': '1493. IEEE, 2013.', 'confidence': 0.9595561027526855, 'text_region': [[138.0, 752.0], [274.0, 752.0], [274.0, 774.0], [138.0, 774.0]]}, {'text': '[15] Hui Li, Peng Wang, Chunhua Shen, and Guyu Zhang. Show,', 'confidence': 0.9917055368423462, 'text_region': [[98.0, 777.0], [575.0, 777.0], [575.0, 800.0], [98.0, 800.0]]}, {'text': 'attend and read: A simple and strong baseline for irregular', 'confidence': 0.9791684150695801, 'text_region': [[136.0, 799.0], [575.0, 799.0], [575.0, 822.0], [136.0, 822.0]]}, {'text': 'text recognition. AAAI, 2019.', 'confidence': 0.9704684615135193, 'text_region': [[136.0, 820.0], [354.0, 818.0], [354.0, 841.0], [136.0, 843.0]]}, {'text': '[16] Minghui Liao, Boyu Song, Shangbang Long, Minghang He,', 'confidence': 0.9855786561965942, 'text_region': [[98.0, 845.0], [574.0, 845.0], [574.0, 868.0], [98.0, 868.0]]}, {'text': 'Cong Yao, and Xiang Bai. Synthtext3d: synthesizing scene', 'confidence': 0.9901584386825562, 'text_region': [[133.0, 863.0], [577.0, 865.0], [577.0, 893.0], [133.0, 891.0]]}, {'text': 'text images from 3d virtual worlds.Science China Informa-', 'confidence': 0.9762476682662964, 'text_region': [[138.0, 889.0], [574.0, 889.0], [574.0, 912.0], [138.0, 912.0]]}, {'text': 'tion Sciences,63(2):120105,2020.', 'confidence': 0.9857491850852966, 'text_region': [[136.0, 911.0], [391.0, 911.0], [391.0, 932.0], [136.0, 932.0]]}, {'text': '[17] Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman,', 'confidence': 0.9823232293128967, 'text_region': [[98.0, 934.0], [575.0, 934.0], [575.0, 957.0], [98.0, 957.0]]}, {'text': 'and Simon Lucey. St-gan: Spatial transformer generative', 'confidence': 0.9811557531356812, 'text_region': [[138.0, 957.0], [575.0, 957.0], [575.0, 980.0], [138.0, 980.0]]}, {'text': 'adversarial networks for image compositing.', 'confidence': 0.992427408695221, 'text_region': [[136.0, 977.0], [476.0, 978.0], [476.0, 1002.0], [136.0, 1000.0]]}, {'text': '.In Proceed-', 'confidence': 0.9429090023040771, 'text_region': [[467.0, 980.0], [574.0, 980.0], [574.0, 998.0], [467.0, 998.0]]}, {'text': 'ingsoftheIEEEConferenceonComputerVisionandPattern', 'confidence': 0.9956490397453308, 'text_region': [[138.0, 1002.0], [575.0, 1002.0], [575.0, 1023.0], [138.0, 1023.0]]}, {'text': 'Recognition,pages 9455-9464, 2018.', 'confidence': 0.9805091619491577, 'text_region': [[138.0, 1023.0], [411.0, 1023.0], [411.0, 1046.0], [138.0, 1046.0]]}, {'text': '[18] Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and', 'confidence': 0.9778456091880798, 'text_region': [[96.0, 1046.0], [575.0, 1046.0], [575.0, 1069.0], [96.0, 1069.0]]}, {'text': 'Junjie Yan. Fots: Fast oriented text spotting with a unified', 'confidence': 0.9913924336433411, 'text_region': [[136.0, 1069.0], [577.0, 1069.0], [577.0, 1092.0], [136.0, 1092.0]]}, {'text': 'network.Proc.CVPR,2018.', 'confidence': 0.9982926845550537, 'text_region': [[138.0, 1091.0], [348.0, 1091.0], [348.0, 1112.0], [138.0, 1112.0]]}, {'text': '[19]Yuliang Liu and Lianwen Jin. Deep matching prior network:', 'confidence': 0.9887316823005676, 'text_region': [[98.0, 1115.0], [574.0, 1115.0], [574.0, 1137.0], [98.0, 1137.0]]}, {'text': 'Toward tighter multi-oriented text detection.In Proc.CVPR,', 'confidence': 0.9662885069847107, 'text_region': [[136.0, 1137.0], [575.0, 1137.0], [575.0, 1158.0], [136.0, 1158.0]]}, {'text': '2017.', 'confidence': 0.9990676045417786, 'text_region': [[138.0, 1158.0], [181.0, 1158.0], [181.0, 1178.0], [138.0, 1178.0]]}, {'text': '[20] Shangbang Long, Yushuo Guan, Bingxuan Wang, Kaigui', 'confidence': 0.9821223616600037, 'text_region': [[98.0, 1183.0], [575.0, 1183.0], [575.0, 1206.0], [98.0, 1206.0]]}, {'text': 'Bian, and Cong Yao. Alchemy: Techniques for rectifica-', 'confidence': 0.9973732829093933, 'text_region': [[136.0, 1204.0], [574.0, 1204.0], [574.0, 1228.0], [136.0, 1228.0]]}, {'text': 'tion based irregular scene text recognition. arXiv preprint', 'confidence': 0.9612612724304199, 'text_region': [[136.0, 1224.0], [575.0, 1226.0], [575.0, 1251.0], [136.0, 1249.0]]}, {'text': 'arXiv:1908.11834,2019.', 'confidence': 0.99625164270401, 'text_region': [[138.0, 1249.0], [321.0, 1249.0], [321.0, 1270.0], [138.0, 1270.0]]}, {'text': '[21] Shangbang Long, Xin He, and Cong Yao. Scene text detec-', 'confidence': 0.9823036193847656, 'text_region': [[98.0, 1272.0], [574.0, 1272.0], [574.0, 1295.0], [98.0, 1295.0]]}, {'text': 'tion and recognition: The deep learning era. arXiv preprint', 'confidence': 0.9771340489387512, 'text_region': [[136.0, 1295.0], [575.0, 1295.0], [575.0, 1318.0], [136.0, 1318.0]]}, {'text': 'arXiv:1811.04256,2018.', 'confidence': 0.9959459900856018, 'text_region': [[138.0, 1317.0], [321.0, 1317.0], [321.0, 1338.0], [138.0, 1338.0]]}, {'text': '[22] Shangbang Long, Jiaqiang Ruan, Wenjie Zhang, Xin He,', 'confidence': 0.9771786332130432, 'text_region': [[98.0, 1341.0], [574.0, 1341.0], [574.0, 1363.0], [98.0, 1363.0]]}, {'text': 'Wenhao Wu, and Cong Yao. Textsnake: A flexible represen-', 'confidence': 0.9705044627189636, 'text_region': [[138.0, 1363.0], [574.0, 1363.0], [574.0, 1386.0], [138.0, 1386.0]]}, {'text': 'tation for detecting text of arbitrary shapes. In Proc. ECCV,', 'confidence': 0.9785397052764893, 'text_region': [[136.0, 1383.0], [575.0, 1384.0], [575.0, 1407.0], [136.0, 1406.0]]}, {'text': '2018.', 'confidence': 0.9987472295761108, 'text_region': [[138.0, 1407.0], [181.0, 1407.0], [181.0, 1427.0], [138.0, 1427.0]]}], 'img_idx': 9, 'score': 0.9953016638755798}\n",
      "{'type': 'reference', 'bbox': [620, 151, 1091, 1421], 'res': [{'text': '[23] Pengyuan Lyu, Zhicheng Yang, Xinhang Leng, Xiaojun Wu,', 'confidence': 0.9843140840530396, 'text_region': [[615.0, 147.0], [1091.0, 147.0], [1091.0, 170.0], [615.0, 170.0]]}, {'text': 'Ruiyu Li, and Xiaoyong Shen. 2d attentional irregular scene', 'confidence': 0.9917847514152527, 'text_region': [[655.0, 172.0], [1093.0, 172.0], [1093.0, 193.0], [655.0, 193.0]]}, {'text': 'text recognizer. arXiv preprint arXiv:1906.05708, 2019.', 'confidence': 0.984458863735199, 'text_region': [[655.0, 193.0], [1061.0, 193.0], [1061.0, 216.0], [655.0, 216.0]]}, {'text': '[24] John McCormac, Ankur Handa, Stefan Leutenegger, and', 'confidence': 0.9705728888511658, 'text_region': [[614.0, 211.0], [1094.0, 215.0], [1094.0, 243.0], [614.0, 239.0]]}, {'text': 'Andrew J. Davison.Scenenet RGB-D: 5m photorealistic', 'confidence': 0.9834209680557251, 'text_region': [[657.0, 239.0], [1093.0, 239.0], [1093.0, 261.0], [657.0, 261.0]]}, {'text': 'images of synthetic indoor trajectories with ground truth.', 'confidence': 0.990534245967865, 'text_region': [[657.0, 261.0], [1091.0, 261.0], [1091.0, 284.0], [657.0, 284.0]]}, {'text': 'CoRR, abs/1612.05079, 2016.', 'confidence': 0.9790505170822144, 'text_region': [[657.0, 282.0], [875.0, 282.0], [875.0, 304.0], [657.0, 304.0]]}, {'text': '[25]Anand Mishra,Karteek Alahari, and CV Jawahar. Scene text', 'confidence': 0.9724907279014587, 'text_region': [[615.0, 307.0], [1093.0, 307.0], [1093.0, 328.0], [615.0, 328.0]]}, {'text': 'recognition using higher order language priors.In BMVC-', 'confidence': 0.9913504123687744, 'text_region': [[655.0, 328.0], [1093.0, 327.0], [1093.0, 350.0], [655.0, 351.0]]}, {'text': 'British MachineVision Conference.BMVA,2012.', 'confidence': 0.9692268371582031, 'text_region': [[655.0, 351.0], [1019.0, 351.0], [1019.0, 373.0], [655.0, 373.0]]}, {'text': '[26]Nibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Chowd-', 'confidence': 0.9870791435241699, 'text_region': [[615.0, 375.0], [1091.0, 375.0], [1091.0, 396.0], [615.0, 396.0]]}, {'text': 'hury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Uma-', 'confidence': 0.9853522181510925, 'text_region': [[655.0, 396.0], [1091.0, 396.0], [1091.0, 419.0], [655.0, 419.0]]}, {'text': 'pada Pal, Jean-Christophe Burie, Cheng-lin Liu, et al. Ic-', 'confidence': 0.9845727682113647, 'text_region': [[655.0, 419.0], [1091.0, 419.0], [1091.0, 442.0], [655.0, 442.0]]}, {'text': 'dar2019 robust reading challenge on multi-lingual scene', 'confidence': 0.9730594754219055, 'text_region': [[654.0, 436.0], [1094.0, 437.0], [1094.0, 465.0], [654.0, 464.0]]}, {'text': 'text detection and recognition-rrc-mlt-2019. arXiv preprint', 'confidence': 0.9648934602737427, 'text_region': [[654.0, 460.0], [1094.0, 462.0], [1094.0, 485.0], [654.0, 483.0]]}, {'text': 'arXiv:1907.00945,2019.', 'confidence': 0.9977678060531616, 'text_region': [[655.0, 483.0], [840.0, 483.0], [840.0, 505.0], [655.0, 505.0]]}, {'text': '[27] Nibal Nayef, Fei Yin, Imen Bizid, Hyunsoo Choi, Yuan', 'confidence': 0.9754771590232849, 'text_region': [[615.0, 508.0], [1093.0, 508.0], [1093.0, 530.0], [615.0, 530.0]]}, {'text': 'Feng, Dimosthenis Karatzas, Zhenbo Luo, Umapada Pal,', 'confidence': 0.9905074834823608, 'text_region': [[655.0, 530.0], [1093.0, 530.0], [1093.0, 553.0], [655.0, 553.0]]}, {'text': 'Christophe Rigaud, Joseph Chazalon, et al. Icdar2017 ro-', 'confidence': 0.9852614998817444, 'text_region': [[657.0, 549.0], [1091.0, 549.0], [1091.0, 573.0], [657.0, 573.0]]}, {'text': 'bust reading challenge on multi-lingual scene text detection', 'confidence': 0.9860324859619141, 'text_region': [[655.0, 574.0], [1093.0, 574.0], [1093.0, 597.0], [655.0, 597.0]]}, {'text': 'and script identification-rrc-mlt. In Proc.ICDAR, volume 1,', 'confidence': 0.9731655716896057, 'text_region': [[655.0, 596.0], [1094.0, 596.0], [1094.0, 617.0], [655.0, 617.0]]}, {'text': 'pages 1454-1459. IEEE, 2017.', 'confidence': 0.9909593462944031, 'text_region': [[652.0, 616.0], [884.0, 612.0], [885.0, 640.0], [652.0, 644.0]]}, {'text': '[28] Jeremie Papon and Markus Schoeler. Semantic pose using', 'confidence': 0.9798126220703125, 'text_region': [[614.0, 640.0], [1093.0, 642.0], [1093.0, 665.0], [614.0, 663.0]]}, {'text': 'deep networks trained on synthetic rgb-d. In Proc. ICCV,', 'confidence': 0.9841750264167786, 'text_region': [[655.0, 663.0], [1093.0, 663.0], [1093.0, 686.0], [655.0, 686.0]]}, {'text': 'pages 774-782, 2015.', 'confidence': 0.9838753938674927, 'text_region': [[653.0, 687.0], [816.0, 683.0], [817.0, 706.0], [654.0, 710.0]]}, {'text': '[29] Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko.', 'confidence': 0.9799937009811401, 'text_region': [[615.0, 710.0], [1091.0, 710.0], [1091.0, 733.0], [615.0, 733.0]]}, {'text': 'Learning deep object detectors from 3d models. In Proc.', 'confidence': 0.9864546060562134, 'text_region': [[655.0, 731.0], [1091.0, 731.0], [1091.0, 754.0], [655.0, 754.0]]}, {'text': 'ICCV, pages 1278-1286, 2015.', 'confidence': 0.9749621152877808, 'text_region': [[655.0, 754.0], [883.0, 754.0], [883.0, 777.0], [655.0, 777.0]]}, {'text': '[30] Siyang Qin, Alessandro Bissacco, Michalis Raptis, Yasuhisa', 'confidence': 0.9662367105484009, 'text_region': [[615.0, 777.0], [1093.0, 777.0], [1093.0, 800.0], [615.0, 800.0]]}, {'text': 'Fuji, and Ying Xiao. Towards unconstrained end-to-end text', 'confidence': 0.9902117252349854, 'text_region': [[655.0, 800.0], [1094.0, 800.0], [1094.0, 822.0], [655.0, 822.0]]}, {'text': 'spotting.InProceedings of theIEEE International Confer-', 'confidence': 0.984525203704834, 'text_region': [[657.0, 822.0], [1093.0, 822.0], [1093.0, 845.0], [657.0, 845.0]]}, {'text': 'ence on Computer Vision,pages 4704-4714,2019.', 'confidence': 0.9601675868034363, 'text_region': [[654.0, 843.0], [1023.0, 841.0], [1023.0, 865.0], [654.0, 866.0]]}, {'text': '[31]Weichao Qiu and Alan Yuille.Unrealcv: Connecting com-', 'confidence': 0.9825745224952698, 'text_region': [[614.0, 866.0], [1091.0, 868.0], [1091.0, 891.0], [614.0, 889.0]]}, {'text': 'puter vision to unreal engine. In Proc. ECCV, pages 909-', 'confidence': 0.9917774200439453, 'text_region': [[655.0, 889.0], [1091.0, 889.0], [1091.0, 912.0], [655.0, 912.0]]}, {'text': '916,2016.', 'confidence': 0.9992517828941345, 'text_region': [[657.0, 912.0], [735.0, 912.0], [735.0, 931.0], [657.0, 931.0]]}, {'text': '[32] Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan', 'confidence': 0.9720805883407593, 'text_region': [[615.0, 934.0], [1094.0, 934.0], [1094.0, 957.0], [615.0, 957.0]]}, {'text': 'Tian, and Chew Lim Tan. Recognizing text with perspective', 'confidence': 0.9869009256362915, 'text_region': [[654.0, 954.0], [1093.0, 955.0], [1093.0, 983.0], [654.0, 982.0]]}, {'text': 'distortion in natural scenes. In Proc. ICCV, pages 569-576,', 'confidence': 0.9753414988517761, 'text_region': [[652.0, 975.0], [1094.0, 977.0], [1094.0, 1005.0], [652.0, 1003.0]]}, {'text': '2013.', 'confidence': 0.9975630640983582, 'text_region': [[657.0, 1002.0], [698.0, 1002.0], [698.0, 1021.0], [657.0, 1021.0]]}, {'text': '[33]Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen', 'confidence': 0.9821199774742126, 'text_region': [[615.0, 1025.0], [1093.0, 1025.0], [1093.0, 1048.0], [615.0, 1048.0]]}, {'text': 'Koltun.', 'confidence': 0.9997865557670593, 'text_region': [[657.0, 1049.0], [713.0, 1049.0], [713.0, 1068.0], [657.0, 1068.0]]}, {'text': '1.Playing for data: Ground truth from computer', 'confidence': 0.956969141960144, 'text_region': [[705.0, 1048.0], [1093.0, 1048.0], [1093.0, 1071.0], [705.0, 1071.0]]}, {'text': 'games.In European conference on computer vision,pages', 'confidence': 0.9804509878158569, 'text_region': [[655.0, 1069.0], [1094.0, 1069.0], [1094.0, 1092.0], [655.0, 1092.0]]}, {'text': '102-118. Springer, 2016.', 'confidence': 0.9807720184326172, 'text_region': [[655.0, 1091.0], [843.0, 1091.0], [843.0, 1114.0], [655.0, 1114.0]]}, {'text': '[34]Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng', 'confidence': 0.9912084937095642, 'text_region': [[614.0, 1114.0], [1093.0, 1115.0], [1093.0, 1139.0], [614.0, 1137.0]]}, {'text': 'Chan, and Chew Lim Tan. A robust arbitrary text detection', 'confidence': 0.9876783490180969, 'text_region': [[655.0, 1137.0], [1093.0, 1137.0], [1093.0, 1160.0], [655.0, 1160.0]]}, {'text': 'system for natural scene images.Expert Systems with Appli-', 'confidence': 0.9769015908241272, 'text_region': [[655.0, 1160.0], [1091.0, 1157.0], [1091.0, 1180.0], [655.0, 1183.0]]}, {'text': 'cations, 41(18):8027-8048,2014.', 'confidence': 0.9882376790046692, 'text_region': [[654.0, 1180.0], [901.0, 1178.0], [901.0, 1201.0], [654.0, 1203.0]]}, {'text': '[35] German Ros, Laura Sellart, Joanna Materzynska, David', 'confidence': 0.9903903007507324, 'text_region': [[615.0, 1204.0], [1094.0, 1204.0], [1094.0, 1228.0], [615.0, 1228.0]]}, {'text': 'Vazquez, and Antonio M Lopez. The synthia dataset: A large', 'confidence': 0.9674986004829407, 'text_region': [[655.0, 1224.0], [1093.0, 1228.0], [1093.0, 1251.0], [655.0, 1247.0]]}, {'text': 'collection of synthetic images for semantic segmentation of', 'confidence': 0.9940873384475708, 'text_region': [[657.0, 1249.0], [1094.0, 1249.0], [1094.0, 1272.0], [657.0, 1272.0]]}, {'text': 'urban scenes. In Proc. CVPR, pages 3234-3243, 2016.', 'confidence': 0.9809820652008057, 'text_region': [[657.0, 1270.0], [1053.0, 1270.0], [1053.0, 1294.0], [657.0, 1294.0]]}, {'text': '[36]Scott D Roth. Ray casting for modeling solids. Computer', 'confidence': 0.9871022701263428, 'text_region': [[615.0, 1295.0], [1094.0, 1295.0], [1094.0, 1318.0], [615.0, 1318.0]]}, {'text': 'Graphics & Image Processing,18(2):109-144,1982.', 'confidence': 0.9797967672348022, 'text_region': [[657.0, 1317.0], [1039.0, 1317.0], [1039.0, 1340.0], [657.0, 1340.0]]}, {'text': '[37] Fatemeh Sadat Saleh, Mohammad Sadegh Aliakbarian,', 'confidence': 0.9666458964347839, 'text_region': [[615.0, 1340.0], [1091.0, 1340.0], [1091.0, 1363.0], [615.0, 1363.0]]}, {'text': 'Mathieu Salzmann, Lars Petersson, and Jose M Alvarez. Ef-', 'confidence': 0.98932284116745, 'text_region': [[655.0, 1363.0], [1093.0, 1363.0], [1093.0, 1384.0], [655.0, 1384.0]]}, {'text': 'fective use of synthetic data for urban scene semantic seg-', 'confidence': 0.9836535453796387, 'text_region': [[654.0, 1381.0], [1093.0, 1383.0], [1093.0, 1411.0], [654.0, 1409.0]]}, {'text': 'mentation. In Proc.ECCV,pages 86-103,2018.', 'confidence': 0.9701323509216309, 'text_region': [[655.0, 1407.0], [1008.0, 1407.0], [1008.0, 1429.0], [655.0, 1429.0]]}], 'img_idx': 9, 'score': 0.9945760369300842}\n",
      "{'type': 'reference', 'bbox': [101, 150, 574, 1422], 'res': [{'text': '[38] Baoguang Shi, Xiang Bai, and Serge Belongie. Detecting', 'confidence': 0.9764679670333862, 'text_region': [[96.0, 147.0], [574.0, 147.0], [574.0, 170.0], [96.0, 170.0]]}, {'text': 'oriented text in natural images by linking segments. In The', 'confidence': 0.9975767135620117, 'text_region': [[138.0, 172.0], [575.0, 172.0], [575.0, 195.0], [138.0, 195.0]]}, {'text': 'IEEEConferenceonComputerVision andPatternRecogni-', 'confidence': 0.9889581799507141, 'text_region': [[136.0, 193.0], [574.0, 193.0], [574.0, 215.0], [136.0, 215.0]]}, {'text': 'tion (CVPR),2017.', 'confidence': 0.9854118227958679, 'text_region': [[138.0, 215.0], [279.0, 215.0], [279.0, 236.0], [138.0, 236.0]]}, {'text': '[39] Baoguang Shi, Mingkun Yang, XingGang Wang, Pengyuan', 'confidence': 0.9884692430496216, 'text_region': [[98.0, 239.0], [574.0, 239.0], [574.0, 262.0], [98.0, 262.0]]}, {'text': 'Lyu, Xiang Bai, and Cong Yao. Aster: An attentional scene', 'confidence': 0.9861168265342712, 'text_region': [[136.0, 261.0], [575.0, 261.0], [575.0, 284.0], [136.0, 284.0]]}, {'text': 'text recognizer with fexible rectification. IEEE transactions', 'confidence': 0.9832203388214111, 'text_region': [[138.0, 282.0], [575.0, 282.0], [575.0, 305.0], [138.0, 305.0]]}, {'text': 'on pattern analysis and machine intelligence,31(11):855-', 'confidence': 0.9758324027061462, 'text_region': [[136.0, 305.0], [575.0, 305.0], [575.0, 328.0], [136.0, 328.0]]}, {'text': '868,2018.', 'confidence': 0.9832461476325989, 'text_region': [[138.0, 328.0], [216.0, 328.0], [216.0, 346.0], [138.0, 346.0]]}, {'text': '[40] Zhuotao Tian, Michelle Shu, Pengyuan Lyu, Ruiyu Li, Chao', 'confidence': 0.9811916947364807, 'text_region': [[98.0, 350.0], [575.0, 350.0], [575.0, 373.0], [98.0, 373.0]]}, {'text': 'Zhou, Xiaoyong Shen, and Jiaya Jia. Learning shape-aware', 'confidence': 0.9907596707344055, 'text_region': [[136.0, 373.0], [575.0, 373.0], [575.0, 396.0], [136.0, 396.0]]}, {'text': 'embedding for scene text detection. In Proceedings of the', 'confidence': 0.9776432514190674, 'text_region': [[138.0, 394.0], [575.0, 394.0], [575.0, 417.0], [138.0, 417.0]]}, {'text': 'IEEE Conference on ComputerVision andPatternRecogni-', 'confidence': 0.9733934998512268, 'text_region': [[136.0, 414.0], [575.0, 416.0], [575.0, 439.0], [136.0, 437.0]]}, {'text': 'tion, pages 42344243, 2019.', 'confidence': 0.9897846579551697, 'text_region': [[136.0, 439.0], [353.0, 439.0], [353.0, 462.0], [136.0, 462.0]]}, {'text': '[41] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Woj-', 'confidence': 0.9832817912101746, 'text_region': [[98.0, 462.0], [574.0, 462.0], [574.0, 485.0], [98.0, 485.0]]}, {'text': 'ciech Zaremba, and Pieter Abbeel. Domain randomization', 'confidence': 0.9870826601982117, 'text_region': [[136.0, 485.0], [575.0, 485.0], [575.0, 507.0], [136.0, 507.0]]}, {'text': 'for transferring deep neural networks from simulation to the', 'confidence': 0.9889237284660339, 'text_region': [[138.0, 507.0], [575.0, 507.0], [575.0, 530.0], [138.0, 530.0]]}, {'text': 'real world.In 2017 IEEE/RSJ International Conference on', 'confidence': 0.9740463495254517, 'text_region': [[136.0, 526.0], [574.0, 526.0], [574.0, 549.0], [136.0, 549.0]]}, {'text': 'Intelligent Robots and Systems (IROS), pages 23-30. IEEE,', 'confidence': 0.9721683859825134, 'text_region': [[138.0, 551.0], [574.0, 551.0], [574.0, 574.0], [138.0, 574.0]]}, {'text': '2017.', 'confidence': 0.9995372891426086, 'text_region': [[138.0, 573.0], [183.0, 573.0], [183.0, 591.0], [138.0, 591.0]]}, {'text': '[42] Jonathan Tremblay, Thang To, and Stan Birchfield. Falling', 'confidence': 0.9739799499511719, 'text_region': [[96.0, 597.0], [575.0, 597.0], [575.0, 619.0], [96.0, 619.0]]}, {'text': 'things: A synthetic dataset for 3d object detection and pose', 'confidence': 0.9900513291358948, 'text_region': [[135.0, 617.0], [574.0, 617.0], [574.0, 640.0], [135.0, 640.0]]}, {'text': 'estimation.In Proc.CVPR Workshops, pages 2038-2041,', 'confidence': 0.9855911135673523, 'text_region': [[136.0, 640.0], [575.0, 640.0], [575.0, 663.0], [136.0, 663.0]]}, {'text': '2018.', 'confidence': 0.9993613958358765, 'text_region': [[138.0, 662.0], [185.0, 662.0], [185.0, 683.0], [138.0, 683.0]]}, {'text': '[43] Gul Varol, Javier Romero, Xavier Martin, Naureen Mah-', 'confidence': 0.9680819511413574, 'text_region': [[98.0, 686.0], [574.0, 686.0], [574.0, 708.0], [98.0, 708.0]]}, {'text': 'mood, Michael J Black, Ivan Laptev, and Cordelia Schmid.', 'confidence': 0.9538998007774353, 'text_region': [[135.0, 708.0], [574.0, 706.0], [574.0, 729.0], [135.0, 731.0]]}, {'text': 'Learning from synthetic humans. In Proc. CVPR, pages 109--', 'confidence': 0.9903220534324646, 'text_region': [[138.0, 731.0], [574.0, 731.0], [574.0, 754.0], [138.0, 754.0]]}, {'text': '117, 2017.', 'confidence': 0.9947212934494019, 'text_region': [[140.0, 754.0], [216.0, 754.0], [216.0, 772.0], [140.0, 772.0]]}, {'text': '[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-', 'confidence': 0.9715901017189026, 'text_region': [[96.0, 776.0], [572.0, 776.0], [572.0, 799.0], [96.0, 799.0]]}, {'text': 'reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia', 'confidence': 0.9643175601959229, 'text_region': [[136.0, 799.0], [575.0, 799.0], [575.0, 820.0], [136.0, 820.0]]}, {'text': 'Polosukhin. Attention is all you need. In Proc. NIPS, pages', 'confidence': 0.9913698434829712, 'text_region': [[135.0, 818.0], [575.0, 822.0], [575.0, 845.0], [135.0, 841.0]]}, {'text': '5998-6008, 2017.', 'confidence': 0.9856833815574646, 'text_region': [[138.0, 842.0], [271.0, 842.0], [271.0, 863.0], [138.0, 863.0]]}, {'text': '[45] Kai Wang, Boris Babenko, and Serge Belongie. End-to-end', 'confidence': 0.9545490741729736, 'text_region': [[98.0, 866.0], [577.0, 866.0], [577.0, 889.0], [98.0, 889.0]]}, {'text': 'scene text recognition.In 20ll IEEE International Confer-', 'confidence': 0.9427286982536316, 'text_region': [[138.0, 888.0], [574.0, 888.0], [574.0, 911.0], [138.0, 911.0]]}, {'text': 'ence on Computer Vision (ICCV),pages 1457-1464.IEEE,', 'confidence': 0.9788551926612854, 'text_region': [[136.0, 911.0], [574.0, 909.0], [574.0, 932.0], [136.0, 934.0]]}, {'text': '2011.', 'confidence': 0.9991924166679382, 'text_region': [[138.0, 934.0], [183.0, 934.0], [183.0, 952.0], [138.0, 952.0]]}, {'text': '[46] Tao Wang, David J Wu, Adam Coates, and Andrew Y Ng.', 'confidence': 0.9808346629142761, 'text_region': [[98.0, 957.0], [574.0, 957.0], [574.0, 980.0], [98.0, 980.0]]}, {'text': 'End-to-end text recognition with convolutional neural net-', 'confidence': 0.9918137788772583, 'text_region': [[135.0, 977.0], [572.0, 977.0], [572.0, 1000.0], [135.0, 1000.0]]}, {'text': 'works.In 2012 21st International Conference on Pattern', 'confidence': 0.9633437395095825, 'text_region': [[138.0, 1000.0], [577.0, 1000.0], [577.0, 1023.0], [138.0, 1023.0]]}, {'text': 'Recognition (ICPR),pages 3304-3308.IEEE,2012.', 'confidence': 0.9863021969795227, 'text_region': [[138.0, 1023.0], [514.0, 1023.0], [514.0, 1046.0], [138.0, 1046.0]]}, {'text': '[47] Xiaobing Wang, Yingying Jiang, Zhenbo Luo, Cheng-Lin', 'confidence': 0.982246994972229, 'text_region': [[98.0, 1044.0], [574.0, 1044.0], [574.0, 1068.0], [98.0, 1068.0]]}, {'text': 'Liu, Hyunsoo Choi, and Sungjin Kim. Arbitrary shape scene', 'confidence': 0.9870057106018066, 'text_region': [[136.0, 1069.0], [575.0, 1069.0], [575.0, 1092.0], [136.0, 1092.0]]}, {'text': 'text detection with adaptive text region representation. In', 'confidence': 0.9946965575218201, 'text_region': [[138.0, 1091.0], [577.0, 1091.0], [577.0, 1114.0], [138.0, 1114.0]]}, {'text': 'Proceedings of theIEEE Conference on Computer Vision', 'confidence': 0.9748780131340027, 'text_region': [[136.0, 1110.0], [575.0, 1110.0], [575.0, 1134.0], [136.0, 1134.0]]}, {'text': 'and Pattern Recognition,pages 6449-6458,2019.', 'confidence': 0.9763146042823792, 'text_region': [[138.0, 1135.0], [499.0, 1135.0], [499.0, 1158.0], [138.0, 1158.0]]}, {'text': '[48] Xinlong Wang, Zhipeng Man, Mingyu You, and Chunhua', 'confidence': 0.978811502456665, 'text_region': [[98.0, 1157.0], [574.0, 1157.0], [574.0, 1180.0], [98.0, 1180.0]]}, {'text': 'Shen. Adversarial generation of training examples: Appli-', 'confidence': 0.9864940047264099, 'text_region': [[135.0, 1178.0], [574.0, 1180.0], [574.0, 1205.0], [135.0, 1203.0]]}, {'text': 'cations to moving vehicle license plate recognition. arXiv', 'confidence': 0.9908623099327087, 'text_region': [[138.0, 1203.0], [575.0, 1203.0], [575.0, 1226.0], [138.0, 1226.0]]}, {'text': 'preprint arXiv:1707.03124,2017.', 'confidence': 0.9705471396446228, 'text_region': [[133.0, 1223.0], [386.0, 1219.0], [386.0, 1247.0], [133.0, 1251.0]]}, {'text': '[49]Qixiang Ye and David Doermann. Text detection and recog-', 'confidence': 0.9717721939086914, 'text_region': [[97.0, 1247.0], [574.0, 1249.0], [574.0, 1272.0], [96.0, 1270.0]]}, {'text': 'nition in imagery: A survey. IEEE transactions on pattern', 'confidence': 0.9580776691436768, 'text_region': [[136.0, 1270.0], [575.0, 1270.0], [575.0, 1294.0], [136.0, 1294.0]]}, {'text': 'analysis and machine intelligence,37(7):1480-1500,2015.', 'confidence': 0.9744076132774353, 'text_region': [[136.0, 1292.0], [569.0, 1290.0], [569.0, 1313.0], [136.0, 1315.0]]}, {'text': '[50] Fangneng Zhan, Shijian Lu, and Chuhui Xue. Verisimilar', 'confidence': 0.9829034805297852, 'text_region': [[98.0, 1317.0], [577.0, 1317.0], [577.0, 1340.0], [98.0, 1340.0]]}, {'text': 'image synthesis for accurate detection and recognition of', 'confidence': 0.9817635416984558, 'text_region': [[136.0, 1338.0], [577.0, 1338.0], [577.0, 1361.0], [136.0, 1361.0]]}, {'text': 'texts in scenes. In Proc. ECCV, 2018.', 'confidence': 0.9840373396873474, 'text_region': [[138.0, 1361.0], [414.0, 1361.0], [414.0, 1383.0], [138.0, 1383.0]]}, {'text': '[51] Fangneng Zhan, Hongyuan Zhu, and Shijian Lu. Spatial fu-', 'confidence': 0.9860042333602905, 'text_region': [[98.0, 1384.0], [574.0, 1384.0], [574.0, 1407.0], [98.0, 1407.0]]}, {'text': 'sion gan for image synthesis.In Proceedings of the IEEE', 'confidence': 0.9833036661148071, 'text_region': [[136.0, 1406.0], [575.0, 1404.0], [575.0, 1427.0], [136.0, 1429.0]]}], 'img_idx': 10, 'score': 0.9938892722129822}\n",
      "{'type': 'reference', 'bbox': [619, 151, 1090, 366], 'res': [{'text': 'Conference on ComputerVision andPattern Recognition,', 'confidence': 0.9741487503051758, 'text_region': [[657.0, 147.0], [1091.0, 147.0], [1091.0, 170.0], [657.0, 170.0]]}, {'text': 'pages 3653-3662, 2019.', 'confidence': 0.9835830926895142, 'text_region': [[652.0, 169.0], [836.0, 163.0], [837.0, 191.0], [652.0, 197.0]]}, {'text': '[52] Chengquan Zhang, Borong Liang, Zuming Huang, Mengyi', 'confidence': 0.9833617806434631, 'text_region': [[615.0, 195.0], [1093.0, 195.0], [1093.0, 218.0], [615.0, 218.0]]}, {'text': 'En, Junyu Han, Errui Ding, and Xinghao Ding. Look more', 'confidence': 0.9817457795143127, 'text_region': [[655.0, 216.0], [1093.0, 216.0], [1093.0, 239.0], [655.0, 239.0]]}, {'text': 'than once: An accurate detector for text of arbitrary shapes.', 'confidence': 0.9844732880592346, 'text_region': [[654.0, 238.0], [1091.0, 239.0], [1091.0, 262.0], [654.0, 261.0]]}, {'text': 'Proceedings of theIEEE Conference on ComputerVision', 'confidence': 0.9677108526229858, 'text_region': [[655.0, 261.0], [1093.0, 261.0], [1093.0, 284.0], [655.0, 284.0]]}, {'text': 'andPattern Recognition(CVPR),2019.', 'confidence': 0.9727039337158203, 'text_region': [[657.0, 282.0], [943.0, 282.0], [943.0, 305.0], [657.0, 305.0]]}, {'text': '[53] Xinyu Zhou, Cong Yao, He Wen,Yuzhi Wang, Shuchang', 'confidence': 0.9821243286132812, 'text_region': [[615.0, 305.0], [1093.0, 305.0], [1093.0, 328.0], [615.0, 328.0]]}, {'text': 'Zhou, Weiran He, and Jiajun Liang. EAST: An efficient and', 'confidence': 0.987571656703949, 'text_region': [[655.0, 327.0], [1093.0, 327.0], [1093.0, 350.0], [655.0, 350.0]]}, {'text': 'accurate scene text detector. In Proc. CVPR,2017.', 'confidence': 0.9690664410591125, 'text_region': [[655.0, 350.0], [1023.0, 350.0], [1023.0, 371.0], [655.0, 371.0]]}], 'img_idx': 10, 'score': 0.9927780628204346}\n",
      "{'type': 'text', 'bbox': [98, 398, 573, 912], 'res': [{'text': 'During the experiments of scene text recognition for En-', 'confidence': 0.9874241948127747, 'text_region': [[121.0, 394.0], [574.0, 394.0], [574.0, 417.0], [121.0, 417.0]]}, {'text': 'glish scripts, we notice that among the most widely used', 'confidence': 0.9836943745613098, 'text_region': [[100.0, 419.0], [575.0, 419.0], [575.0, 442.0], [100.0, 442.0]]}, {'text': 'benchmark datasets, several have incomplete annotations.', 'confidence': 0.9882206916809082, 'text_region': [[98.0, 442.0], [572.0, 442.0], [572.0, 465.0], [98.0, 465.0]]}, {'text': 'They are IIIT5K, SVT, SVTP, and CUTE-80. The annota-', 'confidence': 0.9852875471115112, 'text_region': [[98.0, 467.0], [572.0, 467.0], [572.0, 490.0], [98.0, 490.0]]}, {'text': 'tions of these datasets are case-insensitive, and ignore punc-', 'confidence': 0.9861189126968384, 'text_region': [[96.0, 492.0], [572.0, 492.0], [572.0, 515.0], [96.0, 515.0]]}, {'text': 'tuation marks.', 'confidence': 0.9446795582771301, 'text_region': [[96.0, 515.0], [215.0, 515.0], [215.0, 538.0], [96.0, 538.0]]}, {'text': 'The common practice for recent scene text recognition', 'confidence': 0.9908321499824524, 'text_region': [[120.0, 535.0], [577.0, 536.0], [577.0, 564.0], [120.0, 563.0]]}, {'text': 'research is to convert both prediction and ground-truth text', 'confidence': 0.9869472980499268, 'text_region': [[96.0, 563.0], [575.0, 563.0], [575.0, 586.0], [96.0, 586.0]]}, {'text': 'strings to lower-case and then compare them. This means', 'confidence': 0.9882103204727173, 'text_region': [[98.0, 587.0], [575.0, 587.0], [575.0, 610.0], [98.0, 610.0]]}, {'text': 'that the current evaluation is flawed. It ignores letter case', 'confidence': 0.9896240830421448, 'text_region': [[96.0, 610.0], [575.0, 610.0], [575.0, 634.0], [96.0, 634.0]]}, {'text': 'and punctuation marks which are crucial to the understand-', 'confidence': 0.9947236776351929, 'text_region': [[98.0, 634.0], [574.0, 634.0], [574.0, 657.0], [98.0, 657.0]]}, {'text': 'ing of the text contents. Besides, evaluating on a much', 'confidence': 0.9902134537696838, 'text_region': [[96.0, 655.0], [577.0, 655.0], [577.0, 683.0], [96.0, 683.0]]}, {'text': 'smaller vocabulary set results in over-optimism of the per-', 'confidence': 0.994115948677063, 'text_region': [[98.0, 681.0], [575.0, 681.0], [575.0, 705.0], [98.0, 705.0]]}, {'text': 'formance of the recognition models.', 'confidence': 0.9988977909088135, 'text_region': [[95.0, 703.0], [391.0, 701.0], [391.0, 729.0], [95.0, 731.0]]}, {'text': 'To aid further research, we use the Amazon mechan-', 'confidence': 0.9790531992912292, 'text_region': [[121.0, 729.0], [574.0, 729.0], [574.0, 752.0], [121.0, 752.0]]}, {'text': 'ical Turk (AMT） to re-annotate the aforementioned 4', 'confidence': 0.9573549628257751, 'text_region': [[98.0, 754.0], [577.0, 754.0], [577.0, 777.0], [98.0, 777.0]]}, {'text': 'datasets, which amount to 6837 word images in total.', 'confidence': 0.9816843271255493, 'text_region': [[98.0, 777.0], [574.0, 777.0], [574.0, 800.0], [98.0, 800.0]]}, {'text': 'Each word image is annotated by 3 workers, and we', 'confidence': 0.9618581533432007, 'text_region': [[98.0, 802.0], [577.0, 802.0], [577.0, 825.0], [98.0, 825.0]]}, {'text': 'manually check and correct images where the 3 an-', 'confidence': 0.9885709285736084, 'text_region': [[93.0, 822.0], [577.0, 823.0], [577.0, 851.0], [93.0, 850.0]]}, {'text': 'notations differ.', 'confidence': 0.9957495331764221, 'text_region': [[96.0, 850.0], [238.0, 850.0], [238.0, 871.0], [96.0, 871.0]]}, {'text': 'The annotated datasets are released', 'confidence': 0.9648435115814209, 'text_region': [[258.0, 850.0], [575.0, 850.0], [575.0, 873.0], [258.0, 873.0]]}, {'text': 'via GitHub at https://github.com/Jyouhou/', 'confidence': 0.9831438064575195, 'text_region': [[96.0, 874.0], [574.0, 874.0], [574.0, 898.0], [96.0, 898.0]]}, {'text': 'Case-Sensitive-Scene-Text-Recognition-Datasets.', 'confidence': 0.9996144771575928, 'text_region': [[98.0, 899.0], [662.0, 899.0], [662.0, 921.0], [98.0, 921.0]]}], 'img_idx': 11, 'score': 0.9950896501541138}\n",
      "{'type': 'text', 'bbox': [99, 1075, 574, 1331], 'res': [{'text': 'As we are encouraging case-sensitive (also with punctua-', 'confidence': 0.9798663854598999, 'text_region': [[123.0, 1072.0], [574.0, 1072.0], [574.0, 1096.0], [123.0, 1096.0]]}, {'text': 'tion marks) evaluation for scene text recognition, we would', 'confidence': 0.9839770197868347, 'text_region': [[98.0, 1097.0], [577.0, 1097.0], [577.0, 1120.0], [98.0, 1120.0]]}, {'text': 'like to provide benchmark performances on those widely', 'confidence': 0.9870197176933289, 'text_region': [[96.0, 1120.0], [574.0, 1120.0], [574.0, 1143.0], [96.0, 1143.0]]}, {'text': 'used datasets.', 'confidence': 0.9927376508712769, 'text_region': [[98.0, 1145.0], [238.0, 1145.0], [238.0, 1167.0], [98.0, 1167.0]]}, {'text': 'We evaluate two implementations of the', 'confidence': 0.9876236915588379, 'text_region': [[228.0, 1145.0], [575.0, 1145.0], [575.0, 1168.0], [228.0, 1168.0]]}, {'text': 'ASTER models, by Long et al.7 and Baek et al8 respec-', 'confidence': 0.9410130977630615, 'text_region': [[98.0, 1168.0], [575.0, 1168.0], [575.0, 1191.0], [98.0, 1191.0]]}, {'text': 'tively. Results are summarized in Tab. 7.', 'confidence': 0.9823631644248962, 'text_region': [[96.0, 1191.0], [427.0, 1191.0], [427.0, 1214.0], [96.0, 1214.0]]}, {'text': 'The two benchmark implementations perform compara-', 'confidence': 0.9914591908454895, 'text_region': [[120.0, 1213.0], [574.0, 1214.0], [574.0, 1242.0], [120.0, 1241.0]]}, {'text': \"bly, with Baek's better on straight text and Long's better at\", 'confidence': 0.9702031016349792, 'text_region': [[98.0, 1241.0], [577.0, 1241.0], [577.0, 1264.0], [98.0, 1264.0]]}, {'text': 'curved text.Compared with evaluation with lower case +', 'confidence': 0.9800593852996826, 'text_region': [[100.0, 1266.0], [574.0, 1266.0], [574.0, 1289.0], [100.0, 1289.0]]}, {'text': 'digits, the performance drops considerably for both models', 'confidence': 0.9826452732086182, 'text_region': [[98.0, 1289.0], [575.0, 1289.0], [575.0, 1312.0], [98.0, 1312.0]]}, {'text': 'when we evaluate with all symbols. These results indicate', 'confidence': 0.994337797164917, 'text_region': [[98.0, 1312.0], [575.0, 1312.0], [575.0, 1335.0], [98.0, 1335.0]]}], 'img_idx': 11, 'score': 0.9942444562911987}\n",
      "{'type': 'text', 'bbox': [98, 187, 573, 297], 'res': [{'text': 'In this work, we use a total number of 30 scene models', 'confidence': 0.9812946319580078, 'text_region': [[121.0, 185.0], [575.0, 185.0], [575.0, 208.0], [121.0, 208.0]]}, {'text': 'which are all obtained from the Internet. However, most of', 'confidence': 0.9874789714813232, 'text_region': [[100.0, 210.0], [577.0, 210.0], [577.0, 233.0], [100.0, 233.0]]}, {'text': 'these models are not free. Therefore, we are not allowed to', 'confidence': 0.9860695600509644, 'text_region': [[98.0, 234.0], [577.0, 234.0], [577.0, 256.0], [98.0, 256.0]]}, {'text': 'share the models themselves. Instead,we list the models we', 'confidence': 0.9789681434631348, 'text_region': [[98.0, 257.0], [577.0, 257.0], [577.0, 279.0], [98.0, 279.0]]}, {'text': 'use and their links in Tab. 6.', 'confidence': 0.9849197864532471, 'text_region': [[96.0, 281.0], [326.0, 279.0], [326.0, 302.0], [97.0, 304.0]]}], 'img_idx': 11, 'score': 0.9874390959739685}\n",
      "{'type': 'text', 'bbox': [616, 502, 1087, 542], 'res': [{'text': 'that it may still be a challenge to recognize a larger vocabu-', 'confidence': 0.9899294972419739, 'text_region': [[615.0, 500.0], [1089.0, 500.0], [1089.0, 523.0], [615.0, 523.0]]}, {'text': 'lary, and is worth further research.', 'confidence': 0.9900310039520264, 'text_region': [[615.0, 525.0], [891.0, 525.0], [891.0, 548.0], [615.0, 548.0]]}], 'img_idx': 11, 'score': 0.9697782397270203}\n",
      "{'type': 'text', 'bbox': [100, 976, 571, 1017], 'res': [{'text': 'We select some samples from the 4 datasets to demon-', 'confidence': 0.9846022129058838, 'text_region': [[120.0, 969.0], [574.0, 970.0], [574.0, 998.0], [120.0, 997.0]]}, {'text': 'strate the new annotations in Fig. 6.', 'confidence': 0.9772077202796936, 'text_region': [[98.0, 997.0], [387.0, 997.0], [387.0, 1020.0], [98.0, 1020.0]]}], 'img_idx': 11, 'score': 0.964680552482605}\n",
      "{'type': 'title', 'bbox': [99, 327, 571, 375], 'res': [{'text': 'B. New Annotations for Scene Text Recogni-', 'confidence': 0.9705165028572083, 'text_region': [[98.0, 327.0], [572.0, 327.0], [572.0, 350.0], [98.0, 350.0]]}, {'text': 'tion Datasets', 'confidence': 0.9746664762496948, 'text_region': [[98.0, 355.0], [234.0, 355.0], [234.0, 378.0], [98.0, 378.0]]}], 'img_idx': 11, 'score': 0.961722731590271}\n",
      "{'type': 'title', 'bbox': [100, 1037, 380, 1054], 'res': [{'text': 'B.2BenchmarkPerformances', 'confidence': 0.9986298680305481, 'text_region': [[97.0, 1035.0], [384.0, 1036.0], [384.0, 1059.0], [96.0, 1058.0]]}], 'img_idx': 11, 'score': 0.9588797092437744}\n",
      "{'type': 'title', 'bbox': [100, 939, 214, 958], 'res': [{'text': 'B.1Samples', 'confidence': 0.997761607170105, 'text_region': [[100.0, 939.0], [216.0, 939.0], [216.0, 957.0], [100.0, 957.0]]}], 'img_idx': 11, 'score': 0.9545460939407349}\n",
      "{'type': 'title', 'bbox': [99, 147, 268, 165], 'res': [{'text': 'A.SceneModels', 'confidence': 0.9976027011871338, 'text_region': [[100.0, 145.0], [271.0, 145.0], [271.0, 168.0], [100.0, 168.0]]}], 'img_idx': 11, 'score': 0.9390472769737244}\n",
      "{'type': 'figure', 'bbox': [611, 143, 1091, 419], 'res': [{'text': 'Dataset', 'confidence': 0.998992919921875, 'text_region': [[620.0, 148.0], [674.0, 148.0], [674.0, 167.0], [620.0, 167.0]]}, {'text': 'Sample Image', 'confidence': 0.9553346037864685, 'text_region': [[702.0, 147.0], [797.0, 147.0], [797.0, 170.0], [702.0, 170.0]]}, {'text': 'Original Annotation', 'confidence': 0.9837878346443176, 'text_region': [[822.0, 147.0], [956.0, 147.0], [956.0, 168.0], [822.0, 168.0]]}, {'text': 'New Annotation', 'confidence': 0.9745689034461975, 'text_region': [[983.0, 148.0], [1088.0, 148.0], [1088.0, 167.0], [983.0, 167.0]]}, {'text': 'CUTE80', 'confidence': 0.9996880888938904, 'text_region': [[620.0, 190.0], [679.0, 190.0], [679.0, 208.0], [620.0, 208.0]]}, {'text': 'Tea', 'confidence': 0.9870453476905823, 'text_region': [[711.0, 181.0], [762.0, 185.0], [761.0, 203.0], [710.0, 199.0]]}, {'text': 'TEAM', 'confidence': 0.9996492266654968, 'text_region': [[858.0, 190.0], [906.0, 190.0], [906.0, 208.0], [858.0, 208.0]]}, {'text': 'Team', 'confidence': 0.9995465278625488, 'text_region': [[1016.0, 190.0], [1054.0, 190.0], [1054.0, 210.0], [1016.0, 210.0]]}, {'text': 'IIT5K', 'confidence': 0.9228963851928711, 'text_region': [[619.0, 249.0], [669.0, 249.0], [669.0, 272.0], [619.0, 272.0]]}, {'text': '15%.', 'confidence': 0.9816680550575256, 'text_region': [[708.0, 240.0], [789.0, 235.0], [791.0, 272.0], [710.0, 276.0]]}, {'text': '15', 'confidence': 0.9995346069335938, 'text_region': [[870.0, 251.0], [893.0, 251.0], [893.0, 271.0], [870.0, 271.0]]}, {'text': '15%.', 'confidence': 0.8657354116439819, 'text_region': [[1018.0, 251.0], [1053.0, 251.0], [1053.0, 271.0], [1018.0, 271.0]]}, {'text': 'SVT', 'confidence': 0.9993128776550293, 'text_region': [[619.0, 314.0], [655.0, 314.0], [655.0, 333.0], [619.0, 333.0]]}, {'text': 'Donaldt', 'confidence': 0.9386759996414185, 'text_region': [[707.0, 306.0], [792.0, 314.0], [790.0, 342.0], [704.0, 334.0]]}, {'text': 'DONALD', 'confidence': 0.9997832179069519, 'text_region': [[843.0, 314.0], [915.0, 310.0], [917.0, 333.0], [844.0, 337.0]]}, {'text': \"Donald'\", 'confidence': 0.9229781031608582, 'text_region': [[1006.0, 312.0], [1061.0, 312.0], [1061.0, 335.0], [1006.0, 335.0]]}, {'text': 'SVTP', 'confidence': 0.9996538758277893, 'text_region': [[617.0, 373.0], [662.0, 373.0], [662.0, 398.0], [617.0, 398.0]]}, {'text': 'MARLBORO', 'confidence': 0.9994508028030396, 'text_region': [[838.0, 376.0], [926.0, 376.0], [926.0, 394.0], [838.0, 394.0]]}, {'text': 'Marlboro', 'confidence': 0.9982655644416809, 'text_region': [[1004.0, 375.0], [1068.0, 375.0], [1068.0, 398.0], [1004.0, 398.0]]}], 'img_idx': 11, 'score': 0.9573412537574768}\n",
      "{'type': 'figure_caption', 'bbox': [683, 435, 1023, 453], 'res': [{'text': 'Figure 6: Examples of the new annotations.', 'confidence': 0.975927472114563, 'text_region': [[679.0, 432.0], [1028.0, 432.0], [1028.0, 455.0], [679.0, 455.0]]}], 'img_idx': 11, 'score': 0.9343376755714417}\n",
      "{'type': 'reference', 'bbox': [101, 1352, 416, 1426], 'res': [{'text': '7https://github.com/Jyouhou/', 'confidence': 0.9987388253211975, 'text_region': [[118.0, 1346.0], [388.0, 1348.0], [387.0, 1371.0], [118.0, 1369.0]]}, {'text': 'ICDAR2019-ArT-Recognition-Alchemy', 'confidence': 0.9982599020004272, 'text_region': [[98.0, 1370.0], [421.0, 1370.0], [421.0, 1391.0], [98.0, 1391.0]]}, {'text': '$https://github.com/clovaai/', 'confidence': 0.9755629897117615, 'text_region': [[118.0, 1388.0], [394.0, 1388.0], [394.0, 1411.0], [118.0, 1411.0]]}, {'text': 'deep-text-recognition-benchmark', 'confidence': 0.999643862247467, 'text_region': [[98.0, 1407.0], [404.0, 1407.0], [404.0, 1429.0], [98.0, 1429.0]]}], 'img_idx': 11, 'score': 0.973207414150238}\n",
      "{'type': 'figure', 'bbox': [93, 250, 1113, 863], 'res': [{'text': 'Scene Name', 'confidence': 0.9486039876937866, 'text_region': [[135.0, 257.0], [216.0, 257.0], [216.0, 276.0], [135.0, 276.0]]}, {'text': 'Link', 'confidence': 0.9935569763183594, 'text_region': [[655.0, 259.0], [687.0, 259.0], [687.0, 272.0], [655.0, 272.0]]}, {'text': 'Urban City', 'confidence': 0.9871697425842285, 'text_region': [[135.0, 277.0], [211.0, 277.0], [211.0, 295.0], [135.0, 295.0]]}, {'text': 'Medieval Village', 'confidence': 0.9534748196601868, 'text_region': [[117.0, 288.0], [232.0, 294.0], [230.0, 319.0], [116.0, 313.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/urban-city', 'confidence': 0.9945129156112671, 'text_region': [[364.0, 279.0], [976.0, 279.0], [976.0, 295.0], [364.0, 295.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/medieval-village', 'confidence': 0.9946432113647461, 'text_region': [[336.0, 294.0], [1006.0, 294.0], [1006.0, 317.0], [336.0, 317.0]]}, {'text': 'Loft', 'confidence': 0.9977083802223206, 'text_region': [[156.0, 315.0], [195.0, 315.0], [195.0, 333.0], [156.0, 333.0]]}, {'text': 'https://ue4arch.com/shop/complete-projects/archviz/loft/', 'confidence': 0.9959084391593933, 'text_region': [[406.0, 315.0], [935.0, 315.0], [935.0, 337.0], [406.0, 337.0]]}, {'text': 'Desert Town', 'confidence': 0.9986522197723389, 'text_region': [[131.0, 335.0], [218.0, 335.0], [218.0, 353.0], [131.0, 353.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/desert-town', 'confidence': 0.9917306900024414, 'text_region': [[358.0, 333.0], [983.0, 333.0], [983.0, 355.0], [358.0, 355.0]]}, {'text': 'Archinterior 1', 'confidence': 0.9589323401451111, 'text_region': [[128.0, 355.0], [223.0, 355.0], [223.0, 371.0], [128.0, 371.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-US/product/archinteriors-vol-2-scene-01', 'confidence': 0.9952633380889893, 'text_region': [[278.0, 353.0], [1061.0, 353.0], [1061.0, 375.0], [278.0, 375.0]]}, {'text': 'Desert Gas Station', 'confidence': 0.9885087609291077, 'text_region': [[105.0, 371.0], [238.0, 371.0], [238.0, 393.0], [105.0, 393.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/desert-gas-station', 'confidence': 0.9934037923812866, 'text_region': [[324.0, 371.0], [1016.0, 371.0], [1016.0, 394.0], [324.0, 394.0]]}, {'text': 'ModularSchool', 'confidence': 0.998297393321991, 'text_region': [[123.0, 393.0], [230.0, 393.0], [230.0, 409.0], [123.0, 409.0]]}, {'text': 'Factory District', 'confidence': 0.984615683555603, 'text_region': [[121.0, 412.0], [228.0, 412.0], [228.0, 431.0], [121.0, 431.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/factory-district', 'confidence': 0.9959108829498291, 'text_region': [[333.0, 409.0], [1006.0, 411.0], [1006.0, 434.0], [333.0, 432.0]]}, {'text': 'Abandoned Factory', 'confidence': 0.9722890257835388, 'text_region': [[105.0, 424.0], [243.0, 429.0], [242.0, 452.0], [104.0, 447.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/modular-abandoned-factory', 'confidence': 0.9925647974014282, 'text_region': [[293.0, 429.0], [1049.0, 429.0], [1049.0, 450.0], [293.0, 450.0]]}, {'text': 'Buddhist', 'confidence': 0.9991118907928467, 'text_region': [[141.0, 447.0], [208.0, 447.0], [208.0, 470.0], [141.0, 470.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-US/product/buddhist-monastery-environment', 'confidence': 0.9941176176071167, 'text_region': [[273.0, 449.0], [1069.0, 449.0], [1069.0, 472.0], [273.0, 472.0]]}, {'text': 'Castle Fortress', 'confidence': 0.9890831112861633, 'text_region': [[128.0, 470.0], [225.0, 470.0], [225.0, 488.0], [128.0, 488.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/castle-fortress', 'confidence': 0.987976610660553, 'text_region': [[341.0, 469.0], [1001.0, 469.0], [1001.0, 490.0], [341.0, 490.0]]}, {'text': 'Desert Ruin', 'confidence': 0.9989557266235352, 'text_region': [[135.0, 488.0], [216.0, 488.0], [216.0, 507.0], [135.0, 507.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/modular-desert-ruins', 'confidence': 0.9987887740135193, 'text_region': [[316.0, 490.0], [1024.0, 490.0], [1024.0, 507.0], [316.0, 507.0]]}, {'text': 'HALArchviz', 'confidence': 0.9974594116210938, 'text_region': [[129.0, 503.0], [222.0, 507.0], [221.0, 530.0], [128.0, 526.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-US/product/hal-archviz-toolkit-v1', 'confidence': 0.9908847808837891, 'text_region': [[306.0, 507.0], [1034.0, 507.0], [1034.0, 528.0], [306.0, 528.0]]}, {'text': 'Hospital', 'confidence': 0.9463820457458496, 'text_region': [[141.0, 526.0], [208.0, 526.0], [208.0, 549.0], [141.0, 549.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/modular-sci-fi-hospital', 'confidence': 0.9919543266296387, 'text_region': [[303.0, 528.0], [1038.0, 528.0], [1038.0, 549.0], [303.0, 549.0]]}, {'text': 'HQ House', 'confidence': 0.9679856300354004, 'text_region': [[136.0, 544.0], [213.0, 544.0], [213.0, 568.0], [136.0, 568.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-US/product/hq-residential-house', 'confidence': 0.9912558794021606, 'text_region': [[314.0, 546.0], [1028.0, 546.0], [1028.0, 568.0], [314.0, 568.0]]}, {'text': 'Industrial City', 'confidence': 0.9931758642196655, 'text_region': [[128.0, 566.0], [223.0, 566.0], [223.0, 584.0], [128.0, 584.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/industrial-city', 'confidence': 0.9935266971588135, 'text_region': [[338.0, 563.0], [1003.0, 564.0], [1003.0, 587.0], [338.0, 586.0]]}, {'text': 'Archinterior2', 'confidence': 0.9966444373130798, 'text_region': [[125.0, 580.0], [225.0, 584.0], [224.0, 608.0], [124.0, 604.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-US/product/archinteriors-vol-4-scene-02', 'confidence': 0.995817244052887, 'text_region': [[279.0, 584.0], [1063.0, 584.0], [1063.0, 606.0], [279.0, 606.0]]}, {'text': 'Office', 'confidence': 0.947231113910675, 'text_region': [[150.0, 602.0], [200.0, 602.0], [200.0, 625.0], [150.0, 625.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/retro-office-environment', 'confidence': 0.9944707155227661, 'text_region': [[298.0, 602.0], [1043.0, 602.0], [1043.0, 624.0], [298.0, 624.0]]}, {'text': 'MeetingRoom', 'confidence': 0.9921948909759521, 'text_region': [[124.0, 618.0], [226.0, 622.0], [226.0, 645.0], [123.0, 642.0]]}, {'text': 'https://drive.google.com/file/d/0B_mjKk7NOcnEUWZuRDVFQ09STE0/view', 'confidence': 0.9884129166603088, 'text_region': [[364.0, 622.0], [976.0, 622.0], [976.0, 645.0], [364.0, 645.0]]}, {'text': 'OldVillage', 'confidence': 0.9764817357063293, 'text_region': [[132.0, 637.0], [218.0, 641.0], [217.0, 664.0], [131.0, 660.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/old-village', 'confidence': 0.9957981705665588, 'text_region': [[359.0, 642.0], [983.0, 642.0], [983.0, 663.0], [359.0, 663.0]]}, {'text': 'ModularBuilding', 'confidence': 0.9841516017913818, 'text_region': [[112.0, 656.0], [237.0, 662.0], [235.0, 685.0], [111.0, 679.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/modular-building-set', 'confidence': 0.9947542548179626, 'text_region': [[316.0, 663.0], [1026.0, 663.0], [1026.0, 685.0], [316.0, 685.0]]}, {'text': 'Modular Home', 'confidence': 0.9825186729431152, 'text_region': [[126.0, 681.0], [225.0, 681.0], [225.0, 700.0], [126.0, 700.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-US/product/supergenius-modular-home', 'confidence': 0.9928033351898193, 'text_region': [[298.0, 680.0], [1041.0, 680.0], [1041.0, 703.0], [298.0, 703.0]]}, {'text': 'Dungeon', 'confidence': 0.9995924830436707, 'text_region': [[141.0, 701.0], [208.0, 701.0], [208.0, 719.0], [141.0, 719.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/top-down-multistory-dungeons', 'confidence': 0.9939246773719788, 'text_region': [[278.0, 700.0], [1063.0, 700.0], [1063.0, 723.0], [278.0, 723.0]]}, {'text': 'Old Town', 'confidence': 0.9888205528259277, 'text_region': [[140.0, 716.0], [211.0, 716.0], [211.0, 739.0], [140.0, 739.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/old-town', 'confidence': 0.9922263622283936, 'text_region': [[371.0, 718.0], [970.0, 716.0], [970.0, 739.0], [371.0, 741.0]]}, {'text': 'Root Cellar', 'confidence': 0.9657257199287415, 'text_region': [[136.0, 739.0], [213.0, 739.0], [213.0, 757.0], [136.0, 757.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/root-cellar', 'confidence': 0.9953014254570007, 'text_region': [[358.0, 738.0], [985.0, 738.0], [985.0, 759.0], [358.0, 759.0]]}, {'text': 'Victorian', 'confidence': 0.9991624355316162, 'text_region': [[143.0, 759.0], [208.0, 759.0], [208.0, 777.0], [143.0, 777.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/victorian-street', 'confidence': 0.9968688488006592, 'text_region': [[336.0, 759.0], [1004.0, 759.0], [1004.0, 780.0], [336.0, 780.0]]}, {'text': 'Spaceship', 'confidence': 0.9988181591033936, 'text_region': [[141.0, 779.0], [210.0, 779.0], [210.0, 797.0], [141.0, 797.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/spaceship-interior-environment-set', 'confidence': 0.9952895045280457, 'text_region': [[253.0, 777.0], [1093.0, 777.0], [1093.0, 799.0], [253.0, 799.0]]}, {'text': 'Top-Down City', 'confidence': 0.9881168603897095, 'text_region': [[124.0, 792.0], [226.0, 796.0], [226.0, 819.0], [123.0, 815.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-US/product/top-down-city', 'confidence': 0.9909690618515015, 'text_region': [[348.0, 795.0], [991.0, 795.0], [991.0, 818.0], [348.0, 818.0]]}, {'text': 'Scene Name', 'confidence': 0.9746341705322266, 'text_region': [[131.0, 817.0], [215.0, 817.0], [215.0, 835.0], [131.0, 835.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-Us/product/urban-city', 'confidence': 0.9921140074729919, 'text_region': [[356.0, 815.0], [979.0, 813.0], [980.0, 837.0], [356.0, 838.0]]}, {'text': 'Utopian City', 'confidence': 0.9775838851928711, 'text_region': [[130.0, 835.0], [220.0, 835.0], [220.0, 858.0], [130.0, 858.0]]}, {'text': 'https://www.unrealengine.com/marketplace/en-US/product/utopian-city', 'confidence': 0.9882729649543762, 'text_region': [[354.0, 835.0], [986.0, 835.0], [986.0, 856.0], [354.0, 856.0]]}], 'img_idx': 12, 'score': 0.9709243178367615}\n",
      "{'type': 'figure_caption', 'bbox': [376, 868, 813, 885], 'res': [{'text': 'Table 6: The list of 3D scene models used in this work.', 'confidence': 0.9754427075386047, 'text_region': [[374.0, 866.0], [815.0, 866.0], [815.0, 888.0], [374.0, 888.0]]}], 'img_idx': 12, 'score': 0.8131181001663208}\n",
      "{'type': 'table', 'bbox': [178, 1121, 1005, 1256], 'res': '', 'img_idx': 12, 'score': 0.902001142501831}\n",
      "{'type': 'reference', 'bbox': [162, 1265, 1059, 1307], 'res': [{'text': 'Table 7: Results on English datasets (word level accuracy). All indicates that the evaluation considers lower case characters,', 'confidence': 0.9902255535125732, 'text_region': [[100.0, 1264.0], [1089.0, 1264.0], [1089.0, 1287.0], [100.0, 1287.0]]}, {'text': 'upper case characters, numerical digits, and punctuation marks.', 'confidence': 0.9794862270355225, 'text_region': [[96.0, 1289.0], [605.0, 1287.0], [605.0, 1310.0], [97.0, 1312.0]]}], 'img_idx': 12, 'score': 0.7773663401603699}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from paddleocr import PPStructure, save_structure_res\n",
    "\n",
    "ocr_engine = PPStructure(table=False, ocr=True, show_log=True)\n",
    "\n",
    "save_folder = \"./output\"\n",
    "img_path = \"PaddleOCR/ppstructure/docs/recovery/UnrealText.pdf\"\n",
    "result = ocr_engine(img_path)\n",
    "for index, res in enumerate(result):\n",
    "    save_structure_res(\n",
    "        res, save_folder, os.path.basename(img_path).split(\".\")[0], index\n",
    "    )\n",
    "\n",
    "for res in result:\n",
    "    for line in res:\n",
    "        line.pop(\"img\")\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'PaddleOCR/'\n",
      "/home/nguyen/workplace/signboard_ocr/PaddleOCR\n",
      "[2024/07/17 11:02:49] ppocr INFO: Architecture : \n",
      "[2024/07/17 11:02:49] ppocr INFO:     Backbone : \n",
      "[2024/07/17 11:02:49] ppocr INFO:         checkpoints : ../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy\n",
      "[2024/07/17 11:02:49] ppocr INFO:         mode : vi\n",
      "[2024/07/17 11:02:49] ppocr INFO:         name : LayoutXLMForSer\n",
      "[2024/07/17 11:02:49] ppocr INFO:         num_classes : 7\n",
      "[2024/07/17 11:02:49] ppocr INFO:         pretrained : True\n",
      "[2024/07/17 11:02:49] ppocr INFO:     Transform : None\n",
      "[2024/07/17 11:02:49] ppocr INFO:     algorithm : LayoutXLM\n",
      "[2024/07/17 11:02:49] ppocr INFO:     model_type : kie\n",
      "[2024/07/17 11:02:49] ppocr INFO: Eval : \n",
      "[2024/07/17 11:02:49] ppocr INFO:     dataset : \n",
      "[2024/07/17 11:02:49] ppocr INFO:         data_dir : train_data/XFUND/zh_val/image\n",
      "[2024/07/17 11:02:49] ppocr INFO:         label_file_list : ['train_data/XFUND/zh_val/val.json']\n",
      "[2024/07/17 11:02:49] ppocr INFO:         name : SimpleDataSet\n",
      "[2024/07/17 11:02:49] ppocr INFO:         transforms : \n",
      "[2024/07/17 11:02:49] ppocr INFO:             DecodeImage : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 channel_first : False\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 img_mode : RGB\n",
      "[2024/07/17 11:02:49] ppocr INFO:             VQATokenLabelEncode : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 algorithm : LayoutXLM\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 class_path : train_data/XFUND/class_list_xfun.txt\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 contains_re : False\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 order_method : tb-yx\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 use_textline_bbox_info : True\n",
      "[2024/07/17 11:02:49] ppocr INFO:             VQATokenPad : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 return_attention_mask : True\n",
      "[2024/07/17 11:02:49] ppocr INFO:             VQASerTokenChunk : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:02:49] ppocr INFO:             Resize : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 size : [224, 224]\n",
      "[2024/07/17 11:02:49] ppocr INFO:             NormalizeImage : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 mean : [123.675, 116.28, 103.53]\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 order : hwc\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 scale : 1\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 std : [58.395, 57.12, 57.375]\n",
      "[2024/07/17 11:02:49] ppocr INFO:             ToCHWImage : None\n",
      "[2024/07/17 11:02:49] ppocr INFO:             KeepKeys : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 keep_keys : ['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'image', 'labels']\n",
      "[2024/07/17 11:02:49] ppocr INFO:     loader : \n",
      "[2024/07/17 11:02:49] ppocr INFO:         batch_size_per_card : 8\n",
      "[2024/07/17 11:02:49] ppocr INFO:         drop_last : False\n",
      "[2024/07/17 11:02:49] ppocr INFO:         num_workers : 4\n",
      "[2024/07/17 11:02:49] ppocr INFO:         shuffle : False\n",
      "[2024/07/17 11:02:49] ppocr INFO: Global : \n",
      "[2024/07/17 11:02:49] ppocr INFO:     amp_custom_white_list : ['scale', 'concat', 'elementwise_add']\n",
      "[2024/07/17 11:02:49] ppocr INFO:     cal_metric_during_train : False\n",
      "[2024/07/17 11:02:49] ppocr INFO:     d2s_train_image_shape : [3, 224, 224]\n",
      "[2024/07/17 11:02:49] ppocr INFO:     distributed : False\n",
      "[2024/07/17 11:02:49] ppocr INFO:     epoch_num : 200\n",
      "[2024/07/17 11:02:49] ppocr INFO:     eval_batch_step : [0, 19]\n",
      "[2024/07/17 11:02:49] ppocr INFO:     infer_img : ./ppstructure/docs/kie/input/zh_val_42.jpg\n",
      "[2024/07/17 11:02:49] ppocr INFO:     kie_det_model_dir : None\n",
      "[2024/07/17 11:02:49] ppocr INFO:     kie_rec_model_dir : None\n",
      "[2024/07/17 11:02:49] ppocr INFO:     log_smooth_window : 10\n",
      "[2024/07/17 11:02:49] ppocr INFO:     print_batch_step : 10\n",
      "[2024/07/17 11:02:49] ppocr INFO:     save_epoch_step : 2000\n",
      "[2024/07/17 11:02:49] ppocr INFO:     save_inference_dir : None\n",
      "[2024/07/17 11:02:49] ppocr INFO:     save_model_dir : ./output/ser_vi_layoutxlm_xfund_zh\n",
      "[2024/07/17 11:02:49] ppocr INFO:     save_res_path : ./output/ser/xfund_zh/res\n",
      "[2024/07/17 11:02:49] ppocr INFO:     seed : 2022\n",
      "[2024/07/17 11:02:49] ppocr INFO:     use_gpu : False\n",
      "[2024/07/17 11:02:49] ppocr INFO:     use_visualdl : False\n",
      "[2024/07/17 11:02:49] ppocr INFO: Loss : \n",
      "[2024/07/17 11:02:49] ppocr INFO:     key : backbone_out\n",
      "[2024/07/17 11:02:49] ppocr INFO:     name : VQASerTokenLayoutLMLoss\n",
      "[2024/07/17 11:02:49] ppocr INFO:     num_classes : 7\n",
      "[2024/07/17 11:02:49] ppocr INFO: Metric : \n",
      "[2024/07/17 11:02:49] ppocr INFO:     main_indicator : hmean\n",
      "[2024/07/17 11:02:49] ppocr INFO:     name : VQASerTokenMetric\n",
      "[2024/07/17 11:02:49] ppocr INFO: Optimizer : \n",
      "[2024/07/17 11:02:49] ppocr INFO:     beta1 : 0.9\n",
      "[2024/07/17 11:02:49] ppocr INFO:     beta2 : 0.999\n",
      "[2024/07/17 11:02:49] ppocr INFO:     lr : \n",
      "[2024/07/17 11:02:49] ppocr INFO:         epochs : 200\n",
      "[2024/07/17 11:02:49] ppocr INFO:         learning_rate : 5e-05\n",
      "[2024/07/17 11:02:49] ppocr INFO:         name : Linear\n",
      "[2024/07/17 11:02:49] ppocr INFO:         warmup_epoch : 2\n",
      "[2024/07/17 11:02:49] ppocr INFO:     name : AdamW\n",
      "[2024/07/17 11:02:49] ppocr INFO:     regularizer : \n",
      "[2024/07/17 11:02:49] ppocr INFO:         factor : 0.0\n",
      "[2024/07/17 11:02:49] ppocr INFO:         name : L2\n",
      "[2024/07/17 11:02:49] ppocr INFO: PostProcess : \n",
      "[2024/07/17 11:02:49] ppocr INFO:     class_path : train_data/XFUND/class_list_xfun.txt\n",
      "[2024/07/17 11:02:49] ppocr INFO:     name : VQASerTokenLayoutLMPostProcess\n",
      "[2024/07/17 11:02:49] ppocr INFO: Train : \n",
      "[2024/07/17 11:02:49] ppocr INFO:     dataset : \n",
      "[2024/07/17 11:02:49] ppocr INFO:         data_dir : train_data/XFUND/zh_train/image\n",
      "[2024/07/17 11:02:49] ppocr INFO:         label_file_list : ['train_data/XFUND/zh_train/train.json']\n",
      "[2024/07/17 11:02:49] ppocr INFO:         name : SimpleDataSet\n",
      "[2024/07/17 11:02:49] ppocr INFO:         ratio_list : [1.0]\n",
      "[2024/07/17 11:02:49] ppocr INFO:         transforms : \n",
      "[2024/07/17 11:02:49] ppocr INFO:             DecodeImage : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 channel_first : False\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 img_mode : RGB\n",
      "[2024/07/17 11:02:49] ppocr INFO:             VQATokenLabelEncode : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 algorithm : LayoutXLM\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 class_path : train_data/XFUND/class_list_xfun.txt\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 contains_re : False\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 order_method : tb-yx\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 use_textline_bbox_info : True\n",
      "[2024/07/17 11:02:49] ppocr INFO:             VQATokenPad : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 return_attention_mask : True\n",
      "[2024/07/17 11:02:49] ppocr INFO:             VQASerTokenChunk : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:02:49] ppocr INFO:             Resize : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 size : [224, 224]\n",
      "[2024/07/17 11:02:49] ppocr INFO:             NormalizeImage : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 mean : [123.675, 116.28, 103.53]\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 order : hwc\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 scale : 1\n",
      "[2024/07/17 11:02:49] ppocr INFO:                 std : [58.395, 57.12, 57.375]\n",
      "[2024/07/17 11:02:49] ppocr INFO:             ToCHWImage : None\n",
      "[2024/07/17 11:02:49] ppocr INFO:             KeepKeys : \n",
      "[2024/07/17 11:02:49] ppocr INFO:                 keep_keys : ['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'image', 'labels']\n",
      "[2024/07/17 11:02:49] ppocr INFO:     loader : \n",
      "[2024/07/17 11:02:49] ppocr INFO:         batch_size_per_card : 8\n",
      "[2024/07/17 11:02:49] ppocr INFO:         drop_last : False\n",
      "[2024/07/17 11:02:49] ppocr INFO:         num_workers : 4\n",
      "[2024/07/17 11:02:49] ppocr INFO:         shuffle : True\n",
      "[2024/07/17 11:02:49] ppocr INFO: profiler_options : None\n",
      "[2024/07/17 11:02:49] ppocr INFO: train with paddle 3.0.0-beta1 and device Place(cpu)\n",
      "\u001b[32m[2024-07-17 11:02:51,097] [    INFO]\u001b[0m - Loading configuration file ../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy/model_config.json\u001b[0m\n",
      "\u001b[33m[2024-07-17 11:02:51,098] [ WARNING]\u001b[0m - You are using a model of type layoutlmv2 to instantiate a model of type layoutxlm. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:02:51,098] [    INFO]\u001b[0m - Loading weights file ../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:02:53,693] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:03:05,084] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing LayoutXLMForTokenClassification.\n",
      "\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:03:05,084] [    INFO]\u001b[0m - All the weights of LayoutXLMForTokenClassification were initialized from the model checkpoint at ../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LayoutXLMForTokenClassification for predictions without further training.\u001b[0m\n",
      "[2024/07/17 11:03:05] ppocr INFO: resume from ../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy\n",
      "(…)s/layoutxlm_base/sentencepiece.bpe.model: 100%|█| 5.07M/5.07M [00:04<00:00, 1\n",
      "\u001b[32m[2024-07-17 11:03:12,383] [    INFO]\u001b[0m - tokenizer config file saved in /home/nguyen/.paddlenlp/models/layoutxlm-base-uncased/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:03:12,383] [    INFO]\u001b[0m - Special tokens file saved in /home/nguyen/.paddlenlp/models/layoutxlm-base-uncased/special_tokens_map.json\u001b[0m\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "[2024/07/17 11:04:26] ppocr INFO: process: [0/1], save result to ./output/ser/xfund_zh/res/zh_val_42_ser.jpg\n"
     ]
    }
   ],
   "source": [
    "%cd PaddleOCR/\n",
    "!python3 tools/infer_kie_token_ser.py \\\n",
    "    -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n",
    "    -o Architecture.Backbone.checkpoints=../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy \\\n",
    "    Global.infer_img=./ppstructure/docs/kie/input/zh_val_42.jpg \\\n",
    "    Global.use_gpu=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nguyen/workplace/signboard_ocr/PaddleOCR\n",
      "[2024/07/17 11:56:07] ppocr INFO: ********** re config **********\n",
      "[2024/07/17 11:56:07] ppocr INFO: Architecture : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     Backbone : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         checkpoints : ../paddleocr_data/pretrained_model/re_vi_layoutxlm_xfund_pretrained/best_accuracy\n",
      "[2024/07/17 11:56:07] ppocr INFO:         mode : vi\n",
      "[2024/07/17 11:56:07] ppocr INFO:         name : LayoutXLMForRe\n",
      "[2024/07/17 11:56:07] ppocr INFO:         pretrained : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:     Transform : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:     algorithm : LayoutXLM\n",
      "[2024/07/17 11:56:07] ppocr INFO:     model_type : kie\n",
      "[2024/07/17 11:56:07] ppocr INFO: Eval : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     dataset : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         data_dir : train_data/XFUND/zh_val/image\n",
      "[2024/07/17 11:56:07] ppocr INFO:         label_file_list : ['train_data/XFUND/zh_val/val.json']\n",
      "[2024/07/17 11:56:07] ppocr INFO:         name : SimpleDataSet\n",
      "[2024/07/17 11:56:07] ppocr INFO:         transforms : \n",
      "[2024/07/17 11:56:07] ppocr INFO:             DecodeImage : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 channel_first : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 img_mode : RGB\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQATokenLabelEncode : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 algorithm : LayoutXLM\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 class_path : train_data/XFUND/class_list_xfun.txt\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 contains_re : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 order_method : tb-yx\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 use_textline_bbox_info : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQATokenPad : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 return_attention_mask : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQAReTokenRelation : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQAReTokenChunk : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:56:07] ppocr INFO:             TensorizeEntitiesRelations : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:             Resize : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 size : [224, 224]\n",
      "[2024/07/17 11:56:07] ppocr INFO:             NormalizeImage : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 mean : [123.675, 116.28, 103.53]\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 order : hwc\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 scale : 1\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 std : [58.395, 57.12, 57.375]\n",
      "[2024/07/17 11:56:07] ppocr INFO:             ToCHWImage : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:             KeepKeys : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 keep_keys : ['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'entities', 'relations']\n",
      "[2024/07/17 11:56:07] ppocr INFO:     loader : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         batch_size_per_card : 8\n",
      "[2024/07/17 11:56:07] ppocr INFO:         drop_last : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:         num_workers : 8\n",
      "[2024/07/17 11:56:07] ppocr INFO:         shuffle : False\n",
      "[2024/07/17 11:56:07] ppocr INFO: Global : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     cal_metric_during_train : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:     epoch_num : 130\n",
      "[2024/07/17 11:56:07] ppocr INFO:     eval_batch_step : [0, 19]\n",
      "[2024/07/17 11:56:07] ppocr INFO:     infer_img : train_data/XFUND/zh_val/image/zh_val_42.jpg\n",
      "[2024/07/17 11:56:07] ppocr INFO:     kie_det_model_dir : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:     kie_rec_model_dir : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:     log_smooth_window : 10\n",
      "[2024/07/17 11:56:07] ppocr INFO:     print_batch_step : 10\n",
      "[2024/07/17 11:56:07] ppocr INFO:     save_epoch_step : 2000\n",
      "[2024/07/17 11:56:07] ppocr INFO:     save_inference_dir : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:     save_model_dir : ./output/re_vi_layoutxlm_xfund_zh\n",
      "[2024/07/17 11:56:07] ppocr INFO:     save_res_path : ./output/re/xfund_zh/with_gt\n",
      "[2024/07/17 11:56:07] ppocr INFO:     seed : 2022\n",
      "[2024/07/17 11:56:07] ppocr INFO:     use_gpu : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:     use_visualdl : False\n",
      "[2024/07/17 11:56:07] ppocr INFO: Loss : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     key : loss\n",
      "[2024/07/17 11:56:07] ppocr INFO:     name : LossFromOutput\n",
      "[2024/07/17 11:56:07] ppocr INFO:     reduction : mean\n",
      "[2024/07/17 11:56:07] ppocr INFO: Metric : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     main_indicator : hmean\n",
      "[2024/07/17 11:56:07] ppocr INFO:     name : VQAReTokenMetric\n",
      "[2024/07/17 11:56:07] ppocr INFO: Optimizer : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     beta1 : 0.9\n",
      "[2024/07/17 11:56:07] ppocr INFO:     beta2 : 0.999\n",
      "[2024/07/17 11:56:07] ppocr INFO:     clip_norm : 10\n",
      "[2024/07/17 11:56:07] ppocr INFO:     lr : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         learning_rate : 5e-05\n",
      "[2024/07/17 11:56:07] ppocr INFO:         warmup_epoch : 10\n",
      "[2024/07/17 11:56:07] ppocr INFO:     name : AdamW\n",
      "[2024/07/17 11:56:07] ppocr INFO:     regularizer : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         factor : 0.0\n",
      "[2024/07/17 11:56:07] ppocr INFO:         name : L2\n",
      "[2024/07/17 11:56:07] ppocr INFO: PostProcess : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     name : VQAReTokenLayoutLMPostProcess\n",
      "[2024/07/17 11:56:07] ppocr INFO: Train : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     dataset : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         data_dir : train_data/XFUND/zh_train/image\n",
      "[2024/07/17 11:56:07] ppocr INFO:         label_file_list : ['train_data/XFUND/zh_train/train.json']\n",
      "[2024/07/17 11:56:07] ppocr INFO:         name : SimpleDataSet\n",
      "[2024/07/17 11:56:07] ppocr INFO:         ratio_list : [1.0]\n",
      "[2024/07/17 11:56:07] ppocr INFO:         transforms : \n",
      "[2024/07/17 11:56:07] ppocr INFO:             DecodeImage : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 channel_first : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 img_mode : RGB\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQATokenLabelEncode : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 algorithm : LayoutXLM\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 class_path : train_data/XFUND/class_list_xfun.txt\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 contains_re : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 order_method : tb-yx\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 use_textline_bbox_info : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQATokenPad : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 return_attention_mask : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQAReTokenRelation : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQAReTokenChunk : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:56:07] ppocr INFO:             TensorizeEntitiesRelations : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:             Resize : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 size : [224, 224]\n",
      "[2024/07/17 11:56:07] ppocr INFO:             NormalizeImage : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 mean : [123.675, 116.28, 103.53]\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 order : hwc\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 scale : 1\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 std : [58.395, 57.12, 57.375]\n",
      "[2024/07/17 11:56:07] ppocr INFO:             ToCHWImage : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:             KeepKeys : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 keep_keys : ['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'entities', 'relations']\n",
      "[2024/07/17 11:56:07] ppocr INFO:     loader : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         batch_size_per_card : 2\n",
      "[2024/07/17 11:56:07] ppocr INFO:         drop_last : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:         num_workers : 4\n",
      "[2024/07/17 11:56:07] ppocr INFO:         shuffle : True\n",
      "[2024/07/17 11:56:07] ppocr INFO: \n",
      "\n",
      "[2024/07/17 11:56:07] ppocr INFO: ********** ser config **********\n",
      "[2024/07/17 11:56:07] ppocr INFO: Architecture : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     Backbone : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         checkpoints : ../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy\n",
      "[2024/07/17 11:56:07] ppocr INFO:         mode : vi\n",
      "[2024/07/17 11:56:07] ppocr INFO:         name : LayoutXLMForSer\n",
      "[2024/07/17 11:56:07] ppocr INFO:         num_classes : 7\n",
      "[2024/07/17 11:56:07] ppocr INFO:         pretrained : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:     Transform : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:     algorithm : LayoutXLM\n",
      "[2024/07/17 11:56:07] ppocr INFO:     model_type : kie\n",
      "[2024/07/17 11:56:07] ppocr INFO: Eval : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     dataset : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         data_dir : train_data/XFUND/zh_val/image\n",
      "[2024/07/17 11:56:07] ppocr INFO:         label_file_list : ['train_data/XFUND/zh_val/val.json']\n",
      "[2024/07/17 11:56:07] ppocr INFO:         name : SimpleDataSet\n",
      "[2024/07/17 11:56:07] ppocr INFO:         transforms : \n",
      "[2024/07/17 11:56:07] ppocr INFO:             DecodeImage : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 channel_first : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 img_mode : RGB\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQATokenLabelEncode : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 algorithm : LayoutXLM\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 class_path : train_data/XFUND/class_list_xfun.txt\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 contains_re : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 order_method : tb-yx\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 use_textline_bbox_info : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQATokenPad : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 return_attention_mask : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQASerTokenChunk : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:56:07] ppocr INFO:             Resize : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 size : [224, 224]\n",
      "[2024/07/17 11:56:07] ppocr INFO:             NormalizeImage : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 mean : [123.675, 116.28, 103.53]\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 order : hwc\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 scale : 1\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 std : [58.395, 57.12, 57.375]\n",
      "[2024/07/17 11:56:07] ppocr INFO:             ToCHWImage : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:             KeepKeys : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 keep_keys : ['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'image', 'labels']\n",
      "[2024/07/17 11:56:07] ppocr INFO:     loader : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         batch_size_per_card : 8\n",
      "[2024/07/17 11:56:07] ppocr INFO:         drop_last : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:         num_workers : 4\n",
      "[2024/07/17 11:56:07] ppocr INFO:         shuffle : False\n",
      "[2024/07/17 11:56:07] ppocr INFO: Global : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     amp_custom_white_list : ['scale', 'concat', 'elementwise_add']\n",
      "[2024/07/17 11:56:07] ppocr INFO:     cal_metric_during_train : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:     d2s_train_image_shape : [3, 224, 224]\n",
      "[2024/07/17 11:56:07] ppocr INFO:     epoch_num : 200\n",
      "[2024/07/17 11:56:07] ppocr INFO:     eval_batch_step : [0, 19]\n",
      "[2024/07/17 11:56:07] ppocr INFO:     infer_img : ppstructure/docs/kie/input/zh_val_42.jpg\n",
      "[2024/07/17 11:56:07] ppocr INFO:     kie_det_model_dir : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:     kie_rec_model_dir : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:     log_smooth_window : 10\n",
      "[2024/07/17 11:56:07] ppocr INFO:     print_batch_step : 10\n",
      "[2024/07/17 11:56:07] ppocr INFO:     save_epoch_step : 2000\n",
      "[2024/07/17 11:56:07] ppocr INFO:     save_inference_dir : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:     save_model_dir : ./output/ser_vi_layoutxlm_xfund_zh\n",
      "[2024/07/17 11:56:07] ppocr INFO:     save_res_path : ./output/ser/xfund_zh/res\n",
      "[2024/07/17 11:56:07] ppocr INFO:     seed : 2022\n",
      "[2024/07/17 11:56:07] ppocr INFO:     use_gpu : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:     use_visualdl : False\n",
      "[2024/07/17 11:56:07] ppocr INFO: Loss : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     key : backbone_out\n",
      "[2024/07/17 11:56:07] ppocr INFO:     name : VQASerTokenLayoutLMLoss\n",
      "[2024/07/17 11:56:07] ppocr INFO:     num_classes : 7\n",
      "[2024/07/17 11:56:07] ppocr INFO: Metric : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     main_indicator : hmean\n",
      "[2024/07/17 11:56:07] ppocr INFO:     name : VQASerTokenMetric\n",
      "[2024/07/17 11:56:07] ppocr INFO: Optimizer : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     beta1 : 0.9\n",
      "[2024/07/17 11:56:07] ppocr INFO:     beta2 : 0.999\n",
      "[2024/07/17 11:56:07] ppocr INFO:     lr : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         epochs : 200\n",
      "[2024/07/17 11:56:07] ppocr INFO:         learning_rate : 5e-05\n",
      "[2024/07/17 11:56:07] ppocr INFO:         name : Linear\n",
      "[2024/07/17 11:56:07] ppocr INFO:         warmup_epoch : 2\n",
      "[2024/07/17 11:56:07] ppocr INFO:     name : AdamW\n",
      "[2024/07/17 11:56:07] ppocr INFO:     regularizer : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         factor : 0.0\n",
      "[2024/07/17 11:56:07] ppocr INFO:         name : L2\n",
      "[2024/07/17 11:56:07] ppocr INFO: PostProcess : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     class_path : train_data/XFUND/class_list_xfun.txt\n",
      "[2024/07/17 11:56:07] ppocr INFO:     name : VQASerTokenLayoutLMPostProcess\n",
      "[2024/07/17 11:56:07] ppocr INFO: Train : \n",
      "[2024/07/17 11:56:07] ppocr INFO:     dataset : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         data_dir : train_data/XFUND/zh_train/image\n",
      "[2024/07/17 11:56:07] ppocr INFO:         label_file_list : ['train_data/XFUND/zh_train/train.json']\n",
      "[2024/07/17 11:56:07] ppocr INFO:         name : SimpleDataSet\n",
      "[2024/07/17 11:56:07] ppocr INFO:         ratio_list : [1.0]\n",
      "[2024/07/17 11:56:07] ppocr INFO:         transforms : \n",
      "[2024/07/17 11:56:07] ppocr INFO:             DecodeImage : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 channel_first : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 img_mode : RGB\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQATokenLabelEncode : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 algorithm : LayoutXLM\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 class_path : train_data/XFUND/class_list_xfun.txt\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 contains_re : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 order_method : tb-yx\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 use_textline_bbox_info : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQATokenPad : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 return_attention_mask : True\n",
      "[2024/07/17 11:56:07] ppocr INFO:             VQASerTokenChunk : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 max_seq_len : 512\n",
      "[2024/07/17 11:56:07] ppocr INFO:             Resize : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 size : [224, 224]\n",
      "[2024/07/17 11:56:07] ppocr INFO:             NormalizeImage : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 mean : [123.675, 116.28, 103.53]\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 order : hwc\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 scale : 1\n",
      "[2024/07/17 11:56:07] ppocr INFO:                 std : [58.395, 57.12, 57.375]\n",
      "[2024/07/17 11:56:07] ppocr INFO:             ToCHWImage : None\n",
      "[2024/07/17 11:56:07] ppocr INFO:             KeepKeys : \n",
      "[2024/07/17 11:56:07] ppocr INFO:                 keep_keys : ['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'image', 'labels']\n",
      "[2024/07/17 11:56:07] ppocr INFO:     loader : \n",
      "[2024/07/17 11:56:07] ppocr INFO:         batch_size_per_card : 8\n",
      "[2024/07/17 11:56:07] ppocr INFO:         drop_last : False\n",
      "[2024/07/17 11:56:07] ppocr INFO:         num_workers : 4\n",
      "[2024/07/17 11:56:07] ppocr INFO:         shuffle : True\n",
      "[2024/07/17 11:56:07] ppocr INFO: train with paddle 3.0.0-beta1 and device Place(cpu)\n",
      "\u001b[32m[2024-07-17 11:56:10,698] [    INFO]\u001b[0m - Loading configuration file ../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy/model_config.json\u001b[0m\n",
      "\u001b[33m[2024-07-17 11:56:10,698] [ WARNING]\u001b[0m - You are using a model of type layoutlmv2 to instantiate a model of type layoutxlm. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:56:10,699] [    INFO]\u001b[0m - Loading weights file ../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:56:13,191] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:56:25,259] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing LayoutXLMForTokenClassification.\n",
      "\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:56:25,259] [    INFO]\u001b[0m - All the weights of LayoutXLMForTokenClassification were initialized from the model checkpoint at ../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LayoutXLMForTokenClassification for predictions without further training.\u001b[0m\n",
      "[2024/07/17 11:56:25] ppocr INFO: resume from ../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy\n",
      "\u001b[32m[2024-07-17 11:56:26,638] [    INFO]\u001b[0m - tokenizer config file saved in /home/nguyen/.paddlenlp/models/layoutxlm-base-uncased/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:56:26,638] [    INFO]\u001b[0m - Special tokens file saved in /home/nguyen/.paddlenlp/models/layoutxlm-base-uncased/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:56:26,651] [    INFO]\u001b[0m - Loading configuration file ../paddleocr_data/pretrained_model/re_vi_layoutxlm_xfund_pretrained/best_accuracy/model_config.json\u001b[0m\n",
      "\u001b[33m[2024-07-17 11:56:26,653] [ WARNING]\u001b[0m - You are using a model of type layoutlmv2 to instantiate a model of type layoutxlm. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:56:26,654] [    INFO]\u001b[0m - Loading weights file ../paddleocr_data/pretrained_model/re_vi_layoutxlm_xfund_pretrained/best_accuracy/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:56:29,172] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:56:44,658] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing LayoutXLMForRelationExtraction.\n",
      "\u001b[0m\n",
      "\u001b[32m[2024-07-17 11:56:44,658] [    INFO]\u001b[0m - All the weights of LayoutXLMForRelationExtraction were initialized from the model checkpoint at ../paddleocr_data/pretrained_model/re_vi_layoutxlm_xfund_pretrained/best_accuracy.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LayoutXLMForRelationExtraction for predictions without further training.\u001b[0m\n",
      "[2024/07/17 11:56:44] ppocr INFO: resume from ../paddleocr_data/pretrained_model/re_vi_layoutxlm_xfund_pretrained/best_accuracy\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "[2024/07/17 11:58:12] ppocr INFO: process: [0/1], save result to ./output/re/xfund_zh/with_gt/zh_val_42_ser_re.jpg\n",
      "/home/nguyen/workplace/signboard_ocr\n"
     ]
    }
   ],
   "source": [
    "# predict using SER and RE trained model at the same time\n",
    "%cd /home/nguyen/workplace/signboard_ocr/PaddleOCR\n",
    "!python3 tools/infer_kie_token_ser_re.py \\\n",
    "    -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml \\\n",
    "    -o Architecture.Backbone.checkpoints=../paddleocr_data/pretrained_model/re_vi_layoutxlm_xfund_pretrained/best_accuracy \\\n",
    "    Global.infer_img=train_data/XFUND/zh_val/image/zh_val_42.jpg \\\n",
    "    Global.use_gpu=False\\\n",
    "    -c_ser configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n",
    "    -o_ser Architecture.Backbone.checkpoints=../paddleocr_data/pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
